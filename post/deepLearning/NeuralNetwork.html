<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="deepLearning,">





  <link rel="alternate" href="/atom.xml" title="My Little World" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="矩阵维度为方便重复计算，减少for循环的使用，在神经网络的计算过程中，尽可能的将数据转成向量进行计算利用向量的广播能力进行快速计算，神经网络多层传递过程中，矩阵的维度一般遵循以下关系 如果前一层（输入）维度为（m,1），中间层维度是（n, 1）, 后一层（输出）维度是（p， 1）那么 中间层w的维度就是（n, m）， b 的维度就是（n，1）, b 的维度始终和中间层一致输出层W的维度（p,n），">
<meta name="keywords" content="deepLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络的相关推导公式">
<meta property="og:url" content="http://yoohannah.github.io/post/deepLearning/NeuralNetwork.html">
<meta property="og:site_name" content="My Little World">
<meta property="og:description" content="矩阵维度为方便重复计算，减少for循环的使用，在神经网络的计算过程中，尽可能的将数据转成向量进行计算利用向量的广播能力进行快速计算，神经网络多层传递过程中，矩阵的维度一般遵循以下关系 如果前一层（输入）维度为（m,1），中间层维度是（n, 1）, 后一层（输出）维度是（p， 1）那么 中间层w的维度就是（n, m）， b 的维度就是（n，1）, b 的维度始终和中间层一致输出层W的维度（p,n），">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/11.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/28.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/12.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/13.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/14.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/15.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/16.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/29.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/17.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/18.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/19.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/20.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/21.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/22.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/24.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/23.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/25.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/26.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/27.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/30.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/31.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/32.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/33.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/34.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/LogReg_kiank.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/35.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/37.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/38.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/39.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/40.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/41.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/LModel.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/forward.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/cost.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/backforward.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/dao1.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/dao2.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/dao3.png">
<meta property="og:updated_time" content="2025-02-07T00:07:52.258Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络的相关推导公式">
<meta name="twitter:description" content="矩阵维度为方便重复计算，减少for循环的使用，在神经网络的计算过程中，尽可能的将数据转成向量进行计算利用向量的广播能力进行快速计算，神经网络多层传递过程中，矩阵的维度一般遵循以下关系 如果前一层（输入）维度为（m,1），中间层维度是（n, 1）, 后一层（输出）维度是（p， 1）那么 中间层w的维度就是（n, m）， b 的维度就是（n，1）, b 的维度始终和中间层一致输出层W的维度（p,n），">
<meta name="twitter:image" content="http://yoohannah.github.io/image/deepLearning/11.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoohannah.github.io/post/deepLearning/NeuralNetwork.html">





  <title> 神经网络的相关推导公式 | My Little World </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">My Little World</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">learn and share</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/deepLearning/NeuralNetwork.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                神经网络的相关推导公式
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-12-01T21:24:37+08:00">
                2024-12-01
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="矩阵维度"><a href="#矩阵维度" class="headerlink" title="矩阵维度"></a>矩阵维度</h1><p>为方便重复计算，减少for循环的使用，在神经网络的计算过程中，尽可能的将数据转成向量进行计算<br>利用向量的广播能力进行快速计算，神经网络多层传递过程中，矩阵的维度一般遵循以下关系</p>
<p>如果前一层（输入）维度为（m,1），中间层维度是（n, 1）, 后一层（输出）维度是（p， 1）<br>那么 中间层w的维度就是（n, m）， b 的维度就是（n，1）, b 的维度始终和中间层一致<br>输出层W的维度（p,n），b 的维度就是(p,1)</p>
<p><img src="/image/deepLearning/11.png" alt></p>
<h3 id="参数的矩阵维度关系"><a href="#参数的矩阵维度关系" class="headerlink" title="参数的矩阵维度关系"></a>参数的矩阵维度关系</h3><p><img src="/image/deepLearning/28.png" alt></p>
<h1 id="向前传播计算过程"><a href="#向前传播计算过程" class="headerlink" title="向前传播计算过程"></a>向前传播计算过程</h1><h2 id="一个神经元的计算"><a href="#一个神经元的计算" class="headerlink" title="一个神经元的计算"></a>一个神经元的计算</h2><p>每个神经元的计算包括两部分，先计算z,在用激活函数计算a,<br>同一层不同神经元计算的区别就在于使用不同的参数<br><img src="/image/deepLearning/12.png" alt><br>如果将参数w和b整理成向量，对当前样本数据进行一次性向量计算<br>就可以直接得到当前层的直接产出向量<br><img src="/image/deepLearning/13.png" alt></p>
<h2 id="一层神经元的计算"><a href="#一层神经元的计算" class="headerlink" title="一层神经元的计算"></a>一层神经元的计算</h2><p>同样，每一层都可以用相同的计算式表示<br><img src="/image/deepLearning/14.png" alt></p>
<h2 id="一组样本数据的计算"><a href="#一组样本数据的计算" class="headerlink" title="一组样本数据的计算"></a>一组样本数据的计算</h2><p>通过for 循环进行每一层的计算可得到所有样本数据的预测数据y^<br><img src="/image/deepLearning/15.png" alt><br>但是通过将输入层维度(m,1) 的向量增加为（m,x）的向量，可以实现一次计算x个样本的效果，从而去掉for循环<br>如果中间层有n个神经元，输出得到的结果就是(n,x)的矩阵<br>第n - 1行 上的x个数，每个数代表每个样本数据在中间层第n-1个神经元的计算后的值<br>第x - 1列 上的n个数，每个数代表第x-i个样本数据在中间层计算后的每个神经元的值<br><img src="/image/deepLearning/16.png" alt><br>最终经过两层神经元处理后变成，结果变成(1,x)的向量，每个值代表每个样本经过神经网络计算后的预测值</p>
<h3 id="输入输出值矩阵维度之间的关系"><a href="#输入输出值矩阵维度之间的关系" class="headerlink" title="输入输出值矩阵维度之间的关系"></a>输入输出值矩阵维度之间的关系</h3><p><img src="/image/deepLearning/29.png" alt></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="/image/deepLearning/17.png" alt></p>
<h1 id="其他激活函数"><a href="#其他激活函数" class="headerlink" title="其他激活函数"></a>其他激活函数</h1><p>除了sigmoid 激活函数外，常见的激活函数还有Tanh, ReLu,和leaky ReLu,<br>后三者更常见，且使用更广泛，sigmoid基本只用于二分类场景<br><img src="/image/deepLearning/18.png" alt></p>
<h2 id="为什么不使用线性函数作为激活函数"><a href="#为什么不使用线性函数作为激活函数" class="headerlink" title="为什么不使用线性函数作为激活函数"></a>为什么不使用线性函数作为激活函数</h2><p>因为如果使用线性函数作为激活函数，无论神经网络有多少层，都相当于只进行了一次线性函数计算，隐藏层作用消失<br><img src="/image/deepLearning/19.png" alt></p>
<h2 id="不同激活函数的导数"><a href="#不同激活函数的导数" class="headerlink" title="不同激活函数的导数"></a>不同激活函数的导数</h2><p><img src="/image/deepLearning/20.png" alt><br><img src="/image/deepLearning/21.png" alt><br><img src="/image/deepLearning/22.png" alt></p>
<h1 id="向后传播过程"><a href="#向后传播过程" class="headerlink" title="向后传播过程"></a>向后传播过程</h1><p>回顾一下梯度下降的计算过程<br><img src="/image/deepLearning/24.png" alt><br>对于单个神经元的向后传播过程，就是计算单个神经元参数偏导数的过程<br><img src="/image/deepLearning/23.png" alt></p>
<p>对于多层的神经网络进行带入<br><img src="/image/deepLearning/25.png" alt></p>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>同向前传播一样，通过引入向量矩阵，减少for循环<br><img src="/image/deepLearning/26.png" alt></p>
<h1 id="为啥不能初始化参数为0？"><a href="#为啥不能初始化参数为0？" class="headerlink" title="为啥不能初始化参数为0？"></a>为啥不能初始化参数为0？</h1><p>如果初始化参数为0或相同值，那么所有节点计算的值都相同，会产生对称性，对后续计算的影响也相同，同样会导致隐藏层节点结算无效<br>解决办法就是随机初始化参数<br><img src="/image/deepLearning/27.png" alt><br>一般一开始会将参数随机成比较小的值，如果一开始是比较大的值，z 的值就会比较大，<br>当激活函数是sigmoid 或者tanh 这样的激活函数时，<br>计算结果所在的位置就会在梯度比较平缓的地方导致，激活函数处于比较饱和的状态，梯度下降比较慢，影响学习速度</p>
<h1 id="深度网络"><a href="#深度网络" class="headerlink" title="深度网络"></a>深度网络</h1><h2 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h2><p><img src="/image/deepLearning/30.png" alt></p>
<h2 id="向后传播"><a href="#向后传播" class="headerlink" title="向后传播"></a>向后传播</h2><p><img src="/image/deepLearning/31.png" alt></p>
<h2 id="一些超参数"><a href="#一些超参数" class="headerlink" title="一些超参数"></a>一些超参数</h2><p><img src="/image/deepLearning/32.png" alt></p>
<h2 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h2><p><img src="/image/deepLearning/33.png" alt></p>
<h3 id="为什么要使用深层网络"><a href="#为什么要使用深层网络" class="headerlink" title="为什么要使用深层网络"></a>为什么要使用深层网络</h3><p>使用小的（单层神经元数据量少的）的但是有多层的深层网络，往往会比使用浅层(layer数少)网络计算步骤更简洁<br>比如下面的电路与或非门计算过程<br>如果像左侧使用深层网络，每一次层神经元都少一半<br>如果使用右侧单层神经网络，这一层上的神经元会以2的指数方式计算<br>总体算下来，深层网络需要处理的神经元会少很多</p>
<p><img src="/image/deepLearning/34.png" alt></p>
<h1 id="实验练习"><a href="#实验练习" class="headerlink" title="实验练习"></a>实验练习</h1><h2 id="逻辑回归全过程"><a href="#逻辑回归全过程" class="headerlink" title="逻辑回归全过程"></a>逻辑回归全过程</h2><p><a href="https://github.com/GeeeekExplorer/AndrewNg-Deep-Learning/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%20Solution.ipynb" target="_blank" rel="noopener">gitbub[ipynb]链接</a><br><a href="https://colab.research.google.com/github/GeeeekExplorer/AndrewNg-Deep-Learning/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%20Solution.ipynb#scrollTo=OEOpKgClZLco" target="_blank" rel="noopener">实验</a></p>
<h2 id="思路梳理"><a href="#思路梳理" class="headerlink" title="思路梳理"></a>思路梳理</h2><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>实验中要实现对一张图片是否是猫的判断，<br>首先要对图片进行处理，将图片转换成向量，<br>一个像素点由RGB三个数据组成, 现在如果横竖都取图片的64个像素点<br>一张64X64的图片就有64X64=4096个 [r,g,b] 这样的数据，<br>一张图片的数据表示就是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [[196 192 190], [193 186 182],...中间还有61组, [188 179 174]], 每一行 有64个</span><br><span class="line">    [[196 192 190], [193 186 182],..., [188 179 174]],</span><br><span class="line">    ... 中间有60行</span><br><span class="line">    [[196 192 190], [193 186 182],..., [188 179 174]]</span><br><span class="line">    [[196 192 190], [193 186 182],..., [88 79 74]]</span><br><span class="line">] 一共64行</span><br></pre></td></tr></table></figure>
<p>现在把所有数据摊平再转置，就可转成一个[64X64X3=12288, 1]的向量,<br>也就是m个测试数据组成的矩阵中的一列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[196,], [192,],..., [88,], [79,],[74,]]</span><br></pre></td></tr></table></figure>
<p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[0], -1).T</span><br></pre></td></tr></table></figure>
<p>现在我们有209个训练数据的训练集train_set_x_orig的维度是(209, 64, 64, 3)<br>a 就是209<br>现将要将训练集数据一次性转成209列的向量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m_train = train_set_x_orig.shape[0] // 209</span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T</span><br></pre></td></tr></table></figure></p>
<p>train_set_x_flatten 现在的维度就是(12288, 209)<br>每一列是一张图片的像素数据</p>
<h3 id="数据中心标准化"><a href="#数据中心标准化" class="headerlink" title="数据中心标准化"></a>数据中心标准化</h3><p>基于现在处理的是图片的像素数据，所以所有的数据肯定都在0~255之间</p>
<blockquote>
<p>One common preprocessing step in machine learning is to center and standardize your dataset,<br>meaning that you substract the mean of the whole numpy array from each example,<br>and then divide each example by the standard deviation of the whole numpy array.<br>But for picture datasets,<br>it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p>
</blockquote>
<p>一个常见的预处理步骤是尽可能将数据聚拢到坐标系0附近，常用的方法是对数据进行标准化，<br>也就是将数据减去均值，然后将数据除以标准差<br>但是对于图片数据集来说，<br>除以255（像素通道的最大值），会更简单，而且效果也差不多<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten / 255.</span><br></pre></td></tr></table></figure></p>
<h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><blockquote>
<p>What you need to remember:<br>Common steps for pre-processing a new dataset are:<br>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)<br>Reshape the datasets such that each example is now a vector of size (num_px <em> num_px </em> 3, 1)<br>“Standardize” the data</p>
</blockquote>
<p>常见的数据预处理步骤：</p>
<ol>
<li>确定问题的维度和形状（m_train, m_test, num_px, …）</li>
<li>将数据集重新组织成每个示例都是大小为（num_px <em> num_px </em> 3, 1）的向量</li>
<li>标准化数据</li>
</ol>
<h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><blockquote>
<p>The main steps for building a Neural Network are:</p>
</blockquote>
<blockquote>
<p>Define the model structure (such as number of input features)<br>Initialize the model’s parameters<br>Loop:<br>Calculate current loss (forward propagation)<br>Calculate current gradient (backward propagation)<br>Update parameters (gradient descent)<br>You often build 1-3 separately and integrate them into one function we call model().</p>
</blockquote>
<p>构建一个神经网络模型的主要步骤：</p>
<ol>
<li>定义模型结构（例如输入特征的数量,这里是一张图片的12288个rgb数据）</li>
<li>初始化模型的参数</li>
<li>循环：<br> 计算当前损失（前向传播）<br> 计算当前梯度（反向传播）<br> 更新参数（梯度下降）<br>通常会将1-3分别构建, 然后将它们集成到一个函数中，我们称之为model()。<br><img src="/image/deepLearning/LogReg_kiank.png" alt></li>
</ol>
<h3 id="𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现"><a href="#𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现" class="headerlink" title="𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现"></a>𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现</h3><p>𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑤𝑇𝑥+𝑏)=1/(1+𝑒−(𝑤𝑇𝑥+𝑏))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">    Compute the sigmoid of z</span><br><span class="line">    Arguments:</span><br><span class="line">    z -- A scalar or numpy array of any size.</span><br><span class="line">    Return:</span><br><span class="line">    s -- sigmoid(z)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def sigmoid(z):</span><br><span class="line">    s = 1/(1+np.exp(-z))</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure></p>
<h3 id="初始化模型的参数"><a href="#初始化模型的参数" class="headerlink" title="初始化模型的参数"></a>初始化模型的参数</h3><p>用0来初始化参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span><br><span class="line">Argument:</span><br><span class="line">dim -- num_px * num_px * 3</span><br><span class="line">Returns:</span><br><span class="line">w -- initialized vector of shape (dim, 1)</span><br><span class="line">b -- initialized scalar (corresponds to the bias)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def initialize_with_zeros(dim):</span><br><span class="line">    w = np.zeros((dim, 1))</span><br><span class="line">    b = 0</span><br><span class="line">    return w, b</span><br><span class="line"></span><br><span class="line">## 测试</span><br><span class="line">dim = 2</span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"></span><br><span class="line">==&gt;</span><br><span class="line">w = [[0.]</span><br><span class="line"> [0.]]</span><br><span class="line">b = 0.0</span><br></pre></td></tr></table></figure></p>
<h3 id="前向向后传播实现"><a href="#前向向后传播实现" class="headerlink" title="前向向后传播实现"></a>前向向后传播实现</h3><p>根据公式进行代码实现<br><img src="/image/deepLearning/35.png" alt><br>最终得到每轮训练的损失函数和梯度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: propagate</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Implement the cost function and its gradient for the propagation explained above</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span><br><span class="line">b -- bias, a scalar</span><br><span class="line">X -- data of size (num_px * num_px * 3, number of examples)</span><br><span class="line">Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span><br><span class="line"></span><br><span class="line">Return:</span><br><span class="line">cost -- negative log-likelihood cost for logistic regression</span><br><span class="line">dw -- gradient of the loss with respect to w, thus same shape as w</span><br><span class="line">db -- gradient of the loss with respect to b, thus same shape as b</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def propagate(w, b, X, Y):</span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    </span><br><span class="line">    # FORWARD PROPAGATION (FROM X TO COST)</span><br><span class="line">    A = sigmoid(w.T @ X + b)                                    # compute activation 得到 (m,1) 的矩阵A,m 是训练集样本数</span><br><span class="line">    cost = -np.mean(Y * np.log(A) + (1 - Y) * np.log(1 - A))    # compute cost, 在某些 NumPy 的特定版本或上下文中, np.mean 的输出可能是形状为 (1,) 的数组，而不是一个纯标量</span><br><span class="line">    </span><br><span class="line">    # BACKWARD PROPAGATION (TO FIND GRAD)</span><br><span class="line">    dw = X @ (A - Y).T / m</span><br><span class="line">    db = np.mean(A - Y)</span><br><span class="line"></span><br><span class="line">    assert(dw.shape == w.shape)</span><br><span class="line">    assert(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost) # 移除多余的单一维度，确保 cost 是标量</span><br><span class="line">    assert(cost.shape == ()) # 这里明确要求 cost 的形状是 ()，即零维标量。如果 cost 是 (1,)，那么会触发断言错误。</span><br><span class="line">    </span><br><span class="line">    grads = &#123;&quot;dw&quot;: dw,</span><br><span class="line">             &quot;db&quot;: db&#125;</span><br><span class="line">    </span><br><span class="line">    return grads, cost</span><br></pre></td></tr></table></figure>
<h3 id="梯度下降实现"><a href="#梯度下降实现" class="headerlink" title="梯度下降实现"></a>梯度下降实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">This function optimizes w and b by running a gradient descent algorithm</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span><br><span class="line">b -- bias, a scalar</span><br><span class="line">X -- data of shape (num_px * num_px * 3, number of examples)</span><br><span class="line">Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span><br><span class="line">num_iterations -- number of iterations of the optimization loop</span><br><span class="line">learning_rate -- learning rate of the gradient descent update rule</span><br><span class="line">print_cost -- True to print the loss every 100 steps</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">params -- dictionary containing the weights w and bias b</span><br><span class="line">grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span><br><span class="line">costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span><br><span class="line"></span><br><span class="line">1) Calculate the cost and the gradient for the current parameters. Use propagate().</span><br><span class="line">2) Update the parameters using gradient descent rule for w and b.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):</span><br><span class="line">     costs = [] // 收集每轮计算的损失函数值</span><br><span class="line">    </span><br><span class="line">    for i in range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        # Cost and gradient calculation (≈ 1-4 lines of code)</span><br><span class="line">        # 第一轮用初始化w和b计算的损失函数和梯度</span><br><span class="line">        # 后面用更新后的w和b计算的损失函数和梯度</span><br><span class="line">        grads, cost = propagate(w, b, X, Y) </span><br><span class="line">        </span><br><span class="line">        # 解构梯度</span><br><span class="line">        dw = grads[&quot;dw&quot;]</span><br><span class="line">        db = grads[&quot;db&quot;]</span><br><span class="line">        </span><br><span class="line">        # 梯度下降更新参数</span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        # Record the costs</span><br><span class="line">        # 每100轮记录一次损失函数值</span><br><span class="line">        if i % 100 == 0: </span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        # 如果需要每100轮打印下损失函数就再打印下</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line">    </span><br><span class="line">    # num_iterations轮 训练结束后返回最终更新到的参数，梯度，和损失函数集合(可以用于绘制学习曲线)</span><br><span class="line">    params = &#123;&quot;w&quot;: w,</span><br><span class="line">              &quot;b&quot;: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;&quot;dw&quot;: dw,</span><br><span class="line">             &quot;db&quot;: db&#125;</span><br><span class="line">    </span><br><span class="line">    return params, grads, costs</span><br></pre></td></tr></table></figure>
<h3 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h3><p>根据公式实现预测函数</p>
<p>  𝑌̂ =𝐴=𝜎(𝑤𝑇𝑋+𝑏)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span><br><span class="line">b -- bias, a scalar</span><br><span class="line">X -- data of size (num_px * num_px * 3, number of examples)</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># X 是摊平后数据 (12288, m), X.shape[0] 是影响因素个数 12288 个RGB值, X.shape[1] 是训练集样本数</span><br><span class="line">def predict(w, b, X):</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    Y_prediction = np.zeros((1,m))</span><br><span class="line">    w = w.reshape(X.shape[0], 1)</span><br><span class="line">    </span><br><span class="line">    # Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture</span><br><span class="line">    A = sigmoid(w.T @ X + b)</span><br><span class="line">    </span><br><span class="line">    for i in range(A.shape[1]):</span><br><span class="line">        # Convert probabilities A[0,i] to actual predictions p[0,i] </span><br><span class="line">        # 大于0.5 预测为1 是猫, 小于0.5 预测为0, 不是猫</span><br><span class="line">        Y_prediction[0, i] = A[0, i] &gt; 0.5</span><br><span class="line">    </span><br><span class="line">    assert(Y_prediction.shape == (1, m))</span><br><span class="line">    </span><br><span class="line">    return Y_prediction</span><br></pre></td></tr></table></figure>
<h3 id="组装模型"><a href="#组装模型" class="headerlink" title="组装模型"></a>组装模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Builds the logistic regression model by calling the function you&apos;ve implemented previously</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span><br><span class="line">Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span><br><span class="line">X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span><br><span class="line">Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span><br><span class="line">num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span><br><span class="line">learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span><br><span class="line">print_cost -- Set to true to print the cost every 100 iterations</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">d -- dictionary containing information about the model.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):</span><br><span class="line"></span><br><span class="line">    # initialize parameters with zeros (≈ 1 line of code)</span><br><span class="line">    # 初始化模型的参数</span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[0])</span><br><span class="line"></span><br><span class="line">    # Gradient descent (≈ 1 line of code)</span><br><span class="line">    # 根据训练数据采用梯度下降方法更新参数</span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    # Retrieve parameters w and b from dictionary &quot;parameters&quot;</span><br><span class="line">    w = parameters[&quot;w&quot;]</span><br><span class="line">    b = parameters[&quot;b&quot;]</span><br><span class="line">    </span><br><span class="line">    # Predict test/train set examples (≈ 2 lines of code)</span><br><span class="line">    # 用训练好的参数预测测试集和训练集的结果</span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    # Print train/test Errors</span><br><span class="line">    # 打印训练集和测试集的准确率</span><br><span class="line">    print(&quot;train accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))</span><br><span class="line">    print(&quot;test accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))</span><br><span class="line"></span><br><span class="line">    # 返回模型训练的损失函数值(学习曲线)，训练测试数据集的预测结果(判断模型是否拟合)，模型的参数, 学习率，迭代次数等信息</span><br><span class="line">    d = &#123;&quot;costs&quot;: costs,</span><br><span class="line">         &quot;Y_prediction_test&quot;: Y_prediction_test, </span><br><span class="line">         &quot;Y_prediction_train&quot; : Y_prediction_train, </span><br><span class="line">         &quot;w&quot; : w, </span><br><span class="line">         &quot;b&quot; : b,</span><br><span class="line">         &quot;learning_rate&quot; : learning_rate,</span><br><span class="line">         &quot;num_iterations&quot;: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    return d</span><br><span class="line"></span><br><span class="line"># 调用模型</span><br><span class="line"># 注意入参train_set_x, train_set_y, test_set_x, test_set_y, 是经过预处理的数据集</span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)</span><br></pre></td></tr></table></figure>
<h3 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h3><ol>
<li>预测结果分析</li>
</ol>
<p>除了函数本身里面的准确率计算，可以初步判断模型是否过拟合训练数据，还可以单独拿出一个测试数据，和 预测数据进行结果比较，进行验证</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = 14</span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))</span><br><span class="line">print (&quot;y = &quot; + str(test_set_y[0,index]) + &quot;, you predicted that it is a \&quot;&quot; + classes[int(d[&quot;Y_prediction_test&quot;][0,index])].decode(&quot;utf-8&quot;) +  &quot;\&quot; picture.&quot;)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>学习曲线分析</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Plot learning curve (with costs)</span><br><span class="line">costs = np.squeeze(d[&apos;costs&apos;])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(&apos;cost&apos;)</span><br><span class="line">plt.xlabel(&apos;iterations (per hundreds)&apos;)</span><br><span class="line">plt.title(&quot;Learning rate =&quot; + str(d[&quot;learning_rate&quot;]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>学习率分析 or 超参数分析</li>
</ol>
<p>增加训练次数，观察学习曲线变化同理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [0.01, 0.001, 0.0001]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line">for i in learning_rates:</span><br><span class="line">    print (&quot;learning rate is: &quot; + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)</span><br><span class="line">    print (&apos;\n&apos; + &quot;-------------------------------------------------------&quot; + &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">for i in learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][&quot;costs&quot;]), label = str(models[str(i)][&quot;learning_rate&quot;]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(&apos;cost&apos;)</span><br><span class="line">plt.xlabel(&apos;iterations (hundreds)&apos;)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=&apos;upper center&apos;, shadow=True)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(&apos;0.90&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="应用训练结果进行预测"><a href="#应用训练结果进行预测" class="headerlink" title="应用训练结果进行预测"></a>应用训练结果进行预测</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">my_image = &quot;my_image2.jpg&quot;   # change this to the name of your image file </span><br><span class="line"></span><br><span class="line">fname = &quot;images/&quot; + my_image</span><br><span class="line">image = np.array(plt.imread(fname))</span><br><span class="line">image = image/255.</span><br><span class="line">my_image = np.array(Image.fromarray(np.uint8(image)).resize((num_px,num_px))).reshape((1, num_px*num_px*3)).T // 摊平数据(12288, 1)</span><br><span class="line">#my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T</span><br><span class="line">my_predicted_image = predict(d[&quot;w&quot;], d[&quot;b&quot;], my_image)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(&quot;y = &quot; + str(np.squeeze(my_predicted_image)) + &quot;, your algorithm predicts a \&quot;&quot; + classes[int(np.squeeze(my_predicted_image)),].decode(&quot;utf-8&quot;) +  &quot;\&quot; picture.&quot;)</span><br></pre></td></tr></table></figure>
<p><a href="https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">二分类问题的实践</a><br><a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit框架</a></p>
<h2 id="使用隐藏层实现对非线性数据的分类"><a href="#使用隐藏层实现对非线性数据的分类" class="headerlink" title="使用隐藏层实现对非线性数据的分类"></a>使用隐藏层实现对非线性数据的分类</h2><p>题目<br>我们现在有一堆数据分布成花朵的形状，非线性，如下，<br><img src="/image/deepLearning/37.png" alt><br>可以看到上面的数据有的是红色，有的是蓝色，假设红色代表支持特朗普，蓝色代表支持拜登<br>我们希望用一个神经网络来对这些数据进行分类，<br>分类结果就是输入数据可以直接得到数据是红色还是蓝色的标签<br>解决思路就想办法对红色和蓝色的数据集中地区进行分块划分，<br>如果我们还是用sigmoid 函数，那么就会变成线性的，不会得到正确的区块划分<br><img src="/image/deepLearning/38.png" alt><br>我们希望用一个非线性函数把数据进行精确度更好的划分</p>
<blockquote>
<p>The general methodology to build a Neural Network is to: </p>
<ol>
<li>Define the neural network structure ( # of input units, # of hidden units, etc). </li>
<li>Initialize the model’s parameters </li>
<li>Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)</li>
</ol>
<p>You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model().<br>Once you’ve built nn_model() and learnt the right parameters, you can make predictions on new data.</p>
</blockquote>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/image/deepLearning/39.png" alt></p>
<h3 id="涉及方程"><a href="#涉及方程" class="headerlink" title="涉及方程"></a>涉及方程</h3><p>向前传播<br><img src="/image/deepLearning/40.png" alt><br>反向传播<br><img src="/image/deepLearning/41.png" alt></p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>重点关注模型组装好后，如何使用，如何预测，精确度计算，超参数如何训练，<a href="https://yoohannah.github.io/post/machineLearning/MachineLearningDevelopmentProcess.html">迁移学习</a>怎么做</p>
<p><a href="https://github.com/YooHannah/algorithm/blob/master/deeplearning/NeuralNetworkModel/index.py" target="_blank" rel="noopener">非线性逻辑回归实现代码</a></p>
<h2 id="L层神经网络实现"><a href="#L层神经网络实现" class="headerlink" title="L层神经网络实现"></a>L层神经网络实现</h2><p>整体架构<br><img src="/image/deepLearning/LModel.png" alt></p>
<ol>
<li>向前传播的时候，前L-1层都是先线性然后用relu 函数激活，最后一层是线性然后用sigmoid 函数激活<blockquote>
<p>The model’s structure is: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.<br><img src="/image/deepLearning/forward.png" alt></p>
</blockquote>
</li>
<li>计算损失函数<br><img src="/image/deepLearning/cost.png" alt></li>
<li>反向传播的时候，与向前传播相反，除第一层是线性然后用sigmoid 函数激活，后面l-1层是线性然后用relu 函数激活，前面的每一层都是线性然后用relu 函数激活<blockquote>
<p>The model’s structure is: SIGMOID -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; … -&gt; SIGMOID.<br>for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps.<br><img src="/image/deepLearning/backforward.png" alt><br>使用链式法则, 对下面的线性函数求导dw, db, dA<br><img src="/image/deepLearning/dao1.png" alt><br>得到反向传播计算公式<br><img src="/image/deepLearning/dao2.png" alt></p>
</blockquote>
</li>
</ol>
<p>注意输出层的sigmoid 函数求导<br><img src="/image/deepLearning/dao3.png" alt></p>
<p><a href="https://github.com/YooHannah/algorithm/blob/master/deeplearning/DeepNeuralNetwork/index.py" target="_blank" rel="noopener">L层神经网络实现代码</a></p>
<h2 id="运行异常"><a href="#运行异常" class="headerlink" title="运行异常"></a>运行异常</h2><ol>
<li>参数初始化问题</li>
</ol>
<table>
<thead>
<tr>
<th>语句</th>
<th>初始化方式</th>
<th>优缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>np.random.randn(layer_dims[l], layer_dims[l - 1])</code></td>
<td>标准正态分布初始化</td>
<td>简单，但可能导致梯度爆炸或梯度消失，尤其是在深层网络中。</td>
</tr>
<tr>
<td><code>np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])</code></td>
<td>Xavier初始化的变体，<code>np.sqrt(layer_dims[l-1])</code>是上一层神经元个数</td>
<td>提供更稳定的梯度和激活值，适合对称激活函数（如Sigmoid、Tanh）。减少梯度爆炸或梯度消失问题。</td>
</tr>
</tbody>
</table>
<p>如果使用 ReLU 或 Leaky ReLU 作为激活函数，可以采用 He 初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters[&apos;W&apos; + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])</span><br></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deepLearning/" rel="tag"># deepLearning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/post/deepLearning/basicKnowledge.html" rel="next" title="一些基础知识">
                <i class="fa fa-chevron-left"></i> 一些基础知识
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/post/nodejs/streamExamp.html" rel="prev" title="一个实现流通信的案例">
                一个实现流通信的案例 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/psb.jpg" alt="YooHannah">
          <p class="site-author-name" itemprop="name">YooHannah</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">263</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#矩阵维度"><span class="nav-number">1.</span> <span class="nav-text">矩阵维度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#参数的矩阵维度关系"><span class="nav-number">1.0.1.</span> <span class="nav-text">参数的矩阵维度关系</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#向前传播计算过程"><span class="nav-number">2.</span> <span class="nav-text">向前传播计算过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一个神经元的计算"><span class="nav-number">2.1.</span> <span class="nav-text">一个神经元的计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一层神经元的计算"><span class="nav-number">2.2.</span> <span class="nav-text">一层神经元的计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一组样本数据的计算"><span class="nav-number">2.3.</span> <span class="nav-text">一组样本数据的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#输入输出值矩阵维度之间的关系"><span class="nav-number">2.3.1.</span> <span class="nav-text">输入输出值矩阵维度之间的关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">2.4.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#其他激活函数"><span class="nav-number">3.</span> <span class="nav-text">其他激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么不使用线性函数作为激活函数"><span class="nav-number">3.1.</span> <span class="nav-text">为什么不使用线性函数作为激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不同激活函数的导数"><span class="nav-number">3.2.</span> <span class="nav-text">不同激活函数的导数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#向后传播过程"><span class="nav-number">4.</span> <span class="nav-text">向后传播过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#小结-1"><span class="nav-number">4.1.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为啥不能初始化参数为0？"><span class="nav-number">5.</span> <span class="nav-text">为啥不能初始化参数为0？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度网络"><span class="nav-number">6.</span> <span class="nav-text">深度网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#向前传播"><span class="nav-number">6.1.</span> <span class="nav-text">向前传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#向后传播"><span class="nav-number">6.2.</span> <span class="nav-text">向后传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一些超参数"><span class="nav-number">6.3.</span> <span class="nav-text">一些超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结-2"><span class="nav-number">6.4.</span> <span class="nav-text">小结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要使用深层网络"><span class="nav-number">6.4.1.</span> <span class="nav-text">为什么要使用深层网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实验练习"><span class="nav-number">7.</span> <span class="nav-text">实验练习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归全过程"><span class="nav-number">7.1.</span> <span class="nav-text">逻辑回归全过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#思路梳理"><span class="nav-number">7.2.</span> <span class="nav-text">思路梳理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据处理"><span class="nav-number">7.3.</span> <span class="nav-text">数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#向量化"><span class="nav-number">7.3.1.</span> <span class="nav-text">向量化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据中心标准化"><span class="nav-number">7.3.2.</span> <span class="nav-text">数据中心标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小结-3"><span class="nav-number">7.3.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#构建模型"><span class="nav-number">7.4.</span> <span class="nav-text">构建模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现"><span class="nav-number">7.4.1.</span> <span class="nav-text">𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化模型的参数"><span class="nav-number">7.4.2.</span> <span class="nav-text">初始化模型的参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向向后传播实现"><span class="nav-number">7.4.3.</span> <span class="nav-text">前向向后传播实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降实现"><span class="nav-number">7.4.4.</span> <span class="nav-text">梯度下降实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测函数"><span class="nav-number">7.4.5.</span> <span class="nav-text">预测函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#组装模型"><span class="nav-number">7.4.6.</span> <span class="nav-text">组装模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型分析"><span class="nav-number">7.4.7.</span> <span class="nav-text">模型分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#应用训练结果进行预测"><span class="nav-number">7.4.8.</span> <span class="nav-text">应用训练结果进行预测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用隐藏层实现对非线性数据的分类"><span class="nav-number">7.5.</span> <span class="nav-text">使用隐藏层实现对非线性数据的分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型结构"><span class="nav-number">7.5.1.</span> <span class="nav-text">模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#涉及方程"><span class="nav-number">7.5.2.</span> <span class="nav-text">涉及方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实现"><span class="nav-number">7.5.3.</span> <span class="nav-text">实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L层神经网络实现"><span class="nav-number">7.6.</span> <span class="nav-text">L层神经网络实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#运行异常"><span class="nav-number">7.7.</span> <span class="nav-text">运行异常</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YooHannah</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/treedocument/treedocument.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  

  


</body>
</html>
