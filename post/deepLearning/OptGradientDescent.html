<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="deepLearning,">





  <link rel="alternate" href="/atom.xml" title="My Little World" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨å•è½®è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å†ä¸€æ¬¡ä»è®¡ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®ï¼Œå°†æ•´ä¸ªè®­ç»ƒæ•°æ®åˆ†æ‰¹è¿›è¡Œè®­ç»ƒï¼Œæ¯æ‰¹ï¼ˆepochï¼‰è®­ç»ƒçš„æ ·æœ¬æ•°ç§°ä¸ºbatch size batch size æœ€å¤§ç­‰äºæ•´ä¸ªè®­ç»ƒæ ·æœ¬æ•°mæ—¶ï¼Œç›¸å½“äºè¿›è¡Œä¸€æ¬¡æ‰¹é‡è¿ç®—ï¼Œå°±æ˜¯æ ‡å‡†çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•,éœ€è¦è®¡ç®—å®ŒåŸºäºæ•´ä¸ªè®­ç»ƒæ ·æœ¬å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼ŒèŠ±è´¹æ—¶é—´è¾ƒé•¿ï¼Œç„¶åæ‰èƒ½è¿›è¡Œæ¢¯åº¦ä¸‹é™è®¡ç®— 12345678910111213# (Batch) Gradien">
<meta name="keywords" content="deepLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="ä¸€äº›ä¼˜åŒ–æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•">
<meta property="og:url" content="http://yoohannah.github.io/post/deepLearning/OptGradientDescent.html">
<meta property="og:site_name" content="My Little World">
<meta property="og:description" content="å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨å•è½®è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å†ä¸€æ¬¡ä»è®¡ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®ï¼Œå°†æ•´ä¸ªè®­ç»ƒæ•°æ®åˆ†æ‰¹è¿›è¡Œè®­ç»ƒï¼Œæ¯æ‰¹ï¼ˆepochï¼‰è®­ç»ƒçš„æ ·æœ¬æ•°ç§°ä¸ºbatch size batch size æœ€å¤§ç­‰äºæ•´ä¸ªè®­ç»ƒæ ·æœ¬æ•°mæ—¶ï¼Œç›¸å½“äºè¿›è¡Œä¸€æ¬¡æ‰¹é‡è¿ç®—ï¼Œå°±æ˜¯æ ‡å‡†çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•,éœ€è¦è®¡ç®—å®ŒåŸºäºæ•´ä¸ªè®­ç»ƒæ ·æœ¬å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼ŒèŠ±è´¹æ—¶é—´è¾ƒé•¿ï¼Œç„¶åæ‰èƒ½è¿›è¡Œæ¢¯åº¦ä¸‹é™è®¡ç®— 12345678910111213# (Batch) Gradien">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/65.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/66.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/67.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/68.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/70.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/69.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/71.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/72.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/74.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/73.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/75.png">
<meta property="og:image" content="http://yoohannah.github.io/image/deepLearning/76.png">
<meta property="og:updated_time" content="2025-02-07T00:07:52.259Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ä¸€äº›ä¼˜åŒ–æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•">
<meta name="twitter:description" content="å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•åœ¨å•è½®è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å†ä¸€æ¬¡ä»è®¡ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®ï¼Œå°†æ•´ä¸ªè®­ç»ƒæ•°æ®åˆ†æ‰¹è¿›è¡Œè®­ç»ƒï¼Œæ¯æ‰¹ï¼ˆepochï¼‰è®­ç»ƒçš„æ ·æœ¬æ•°ç§°ä¸ºbatch size batch size æœ€å¤§ç­‰äºæ•´ä¸ªè®­ç»ƒæ ·æœ¬æ•°mæ—¶ï¼Œç›¸å½“äºè¿›è¡Œä¸€æ¬¡æ‰¹é‡è¿ç®—ï¼Œå°±æ˜¯æ ‡å‡†çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•,éœ€è¦è®¡ç®—å®ŒåŸºäºæ•´ä¸ªè®­ç»ƒæ ·æœ¬å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼ŒèŠ±è´¹æ—¶é—´è¾ƒé•¿ï¼Œç„¶åæ‰èƒ½è¿›è¡Œæ¢¯åº¦ä¸‹é™è®¡ç®— 12345678910111213# (Batch) Gradien">
<meta name="twitter:image" content="http://yoohannah.github.io/image/deepLearning/65.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'åšä¸»'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoohannah.github.io/post/deepLearning/OptGradientDescent.html">





  <title> ä¸€äº›ä¼˜åŒ–æ¢¯åº¦ä¸‹é™çš„æ–¹æ³• | My Little World </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">My Little World</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">learn and share</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            é¦–é¡µ
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            åˆ†ç±»
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            æ ‡ç­¾
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            å½’æ¡£
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            å…³äº
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/deepLearning/OptGradientDescent.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                ä¸€äº›ä¼˜åŒ–æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-01-31T14:57:37+08:00">
                2025-01-31
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•"><a href="#å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•" class="headerlink" title="å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•"></a>å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•</h1><p>åœ¨å•è½®è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸å†ä¸€æ¬¡ä»è®¡ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®ï¼Œå°†æ•´ä¸ªè®­ç»ƒæ•°æ®åˆ†æ‰¹è¿›è¡Œè®­ç»ƒï¼Œæ¯æ‰¹ï¼ˆepochï¼‰è®­ç»ƒçš„æ ·æœ¬æ•°ç§°ä¸ºbatch size</p>
<p>batch size æœ€å¤§ç­‰äºæ•´ä¸ªè®­ç»ƒæ ·æœ¬æ•°mæ—¶ï¼Œç›¸å½“äºè¿›è¡Œä¸€æ¬¡æ‰¹é‡è¿ç®—ï¼Œå°±æ˜¯æ ‡å‡†çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•,<br>éœ€è¦è®¡ç®—å®ŒåŸºäºæ•´ä¸ªè®­ç»ƒæ ·æœ¬å‚æ•°å’ŒæŸå¤±å‡½æ•°ï¼ŒèŠ±è´¹æ—¶é—´è¾ƒé•¿ï¼Œç„¶åæ‰èƒ½è¿›è¡Œæ¢¯åº¦ä¸‹é™è®¡ç®—</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># (Batch) Gradient Descent:</span><br><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line">for i in range(0, num_iterations):</span><br><span class="line">    # Forward propagation</span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    # Compute cost.</span><br><span class="line">    cost += compute_cost(a, Y)</span><br><span class="line">    # Backward propagation.</span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    # Update parameters.</span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
<p>batch size æœ€å°ç­‰äº1æ—¶ï¼Œå°±æ˜¯ä¸€ä¸ªæ‰¹æ¬¡å¤„ç†ä¸€ä¸ªæ•°æ®, é€Ÿåº¦å¿«ï¼Œä½†æ˜¯æ²¡æ³•åˆ©ç”¨å‘é‡åŠ é€Ÿæ¢¯åº¦ä¸‹é™ï¼Œä¹Ÿç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•Stochastic Gradient Descent:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Stochastic Gradient Descent:</span><br><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line">for i in range(0, num_iterations):</span><br><span class="line">    for j in range(0, m):</span><br><span class="line">        # Forward propagation</span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters) # X[:,j] ä»måˆ—ä¸­ï¼Œæ¯æ¬¡å–ä¸€åˆ—ï¼Œå°±æ˜¯ä¸€ä¸ªæ ·æœ¬</span><br><span class="line">        # Compute cost</span><br><span class="line">        cost += compute_cost(a, Y[:,j])</span><br><span class="line">        # Backward propagation</span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        # Update parameters.</span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure></p>
<p>å½“batch size ä»‹äº1å’Œmä¹‹é—´æ—¶ï¼Œå°±æ˜¯å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•Mini-batch Gradient Descent<br>å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å¤„ç†æ•°æ®åˆ†ä¸¤æ­¥ï¼Œ<br>ä¸€æ­¥æ˜¯æ··æ´—æ•°æ®ï¼Œå°†æ•°æ®é›†é¡ºåºæ‰“ä¹±ï¼Œä½†ä¿è¯X(i) å’Œ Yï¼ˆiï¼‰æ˜¯ä¸€ä¸€å¯¹åº”çš„<br>ç¬¬äºŒæ­¥æ˜¯å°†æ•°æ®åˆ†æˆå¤šä¸ªbatchï¼Œæ¯ä¸ªbatchåŒ…å«batch size ä¸ªæ ·æœ¬ï¼Œéœ€è¦æ³¨æ„å¦‚æœm ä¸èƒ½æ•´é™¤batch sizeï¼Œæœ€åä¸€ä¸ªbatch æ˜¯ä¸è¶³batch size ä¸ªæ ·æœ¬ï¼Œéœ€è¦å•ç‹¬å¤„ç†</p>
<blockquote>
<p>Shuffling and Partitioning are the two steps required to build mini-batches -<br>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: random_mini_batches</span><br><span class="line"></span><br><span class="line">def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a list of random minibatches from (X, Y)</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (input size, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span><br><span class="line">    mini_batch_size -- size of the mini-batches, integer</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            # To make your &quot;random&quot; minibatches the same as ours</span><br><span class="line">    m = X.shape[1]                  # number of training examples</span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    # Step 1: Shuffle (X, Y)</span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((1,m))</span><br><span class="line"></span><br><span class="line">    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning</span><br><span class="line">    for k in range(0, num_complete_minibatches):</span><br><span class="line">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    # Handling the end case (last mini-batch &lt; mini_batch_size)</span><br><span class="line">    if m % mini_batch_size != 0:</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size :]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size :]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    return mini_batches</span><br></pre></td></tr></table></figure>
<p>è¿­ä»£è¿‡ç¨‹å¤„ç†çš„æ•°æ®é›†å°±æ˜¯ä¸Šé¢åˆ†æ‰¹å¥½çš„mini_batches</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mini_batches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0)</span><br><span class="line">t =  math.floor(m/mini_batch_size)</span><br><span class="line">if m % mini_batch_size != 0</span><br><span class="line">  t+=1</span><br><span class="line">for i in range(0, num_iterations):</span><br><span class="line">    for j in range(0, t):</span><br><span class="line">        # Forward propagation</span><br><span class="line">        a, caches = forward_propagation(mini_batches[j][0], parameters) # å–ä¸€æ‰¹è®¡ç®—ä¸€æ‰¹ï¼Œä¸ç”¨æ•´ä¸ªè®¡ç®—å®Œå†æ›´æ–°æ¢¯åº¦</span><br><span class="line">        # Compute cost</span><br><span class="line">        cost += compute_cost(a, mini_batches[j][1])</span><br><span class="line">        # Backward propagation</span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        # Update parameters.</span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step. - You have to tune a learning rate hyperparameter  ğ›¼ . - With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</p>
</blockquote>
<p><img src="/image/deepLearning/65.png" alt><br><img src="/image/deepLearning/66.png" alt><br><img src="/image/deepLearning/67.png" alt><br><img src="/image/deepLearning/68.png" alt></p>
<h1 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h1><h2 id="æŒ‡æ•°åŠ æƒå¹³å‡"><a href="#æŒ‡æ•°åŠ æƒå¹³å‡" class="headerlink" title="æŒ‡æ•°åŠ æƒå¹³å‡"></a>æŒ‡æ•°åŠ æƒå¹³å‡</h2><p>æŒ‡æ•°åŠ æƒå¹³å‡ï¼ˆExponential Weighted Moving Average, EWMAï¼‰æ˜¯ä¸€ç§ç”¨äºå¹³æ»‘æ—¶é—´åºåˆ—æ•°æ®çš„æŠ€æœ¯ã€‚<br>å®ƒé€šè¿‡å¯¹æ•°æ®ç‚¹èµ‹äºˆä¸åŒçš„æƒé‡æ¥è®¡ç®—å¹³å‡å€¼ï¼Œè¾ƒæ–°çš„æ•°æ®ç‚¹æƒé‡è¾ƒå¤§ï¼Œè¾ƒæ—§çš„æ•°æ®ç‚¹æƒé‡è¾ƒå°ã€‚<br>è¿™æ ·å¯ä»¥æ›´æ•æ„Ÿåœ°åæ˜ æœ€æ–°æ•°æ®çš„å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™å†å²æ•°æ®çš„è¶‹åŠ¿ã€‚<br>æŒ‡æ•°åŠ æƒå¹³å‡çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š</p>
<p>St = Î² <em> St-1 + ï¼ˆ1-Î²ï¼‰ </em> Xt<br>å…¶ä¸­ï¼š<br> Stæ˜¯æ—¶é—´ t æ—¶åˆ»çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼ã€‚<br> Xtæ˜¯æ—¶é—´ t æ—¶åˆ»çš„å®é™…æ•°æ®å€¼ã€‚<br> Î² æ˜¯å¹³æ»‘å› å­ï¼Œå–å€¼èŒƒå›´åœ¨ 0 åˆ° 1 ä¹‹é—´ã€‚è¾ƒå¤§çš„  å€¼ä½¿å¾— EWMA å¯¹æœ€æ–°æ•°æ®æ›´æ•æ„Ÿï¼Œè¾ƒå°çš„  å€¼åˆ™ä½¿å¾— EWMA æ›´å¹³æ»‘ã€‚<br> St-1æ˜¯æ—¶é—´ t-1 æ—¶åˆ»çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼ã€‚<br>è§£é‡Š<br>åˆå§‹å€¼ï¼šé€šå¸¸ï¼Œåˆå§‹çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼ S0  å¯ä»¥è®¾ç½®ä¸ºç¬¬ä¸€ä¸ªæ•°æ®ç‚¹ X0 ã€‚<br>é€’å½’è®¡ç®—ï¼šæ¯ä¸ªæ–°çš„æ•°æ®ç‚¹éƒ½ä¼šæ›´æ–° EWMAï¼Œæ–°çš„ EWMA æ˜¯å½“å‰æ•°æ®ç‚¹å’Œå‰ä¸€ä¸ª EWMA çš„åŠ æƒå’Œã€‚<br>å¹³æ»‘å› å­ ï¼šå†³å®šäº†æ–°æ•°æ®ç‚¹å’Œå†å²æ•°æ®å¯¹å½“å‰ EWMA çš„å½±å“ç¨‹åº¦ã€‚è¾ƒå¤§çš„  å€¼ä¼šä½¿å¾— EWMA å¯¹æ–°æ•°æ®ç‚¹å˜åŒ–æ›´æ•æ„Ÿï¼Œè¾ƒå°çš„  å€¼ä¼šä½¿å¾— EWMA æ›´å¹³æ»‘ï¼Œå—å†å²æ•°æ®å½±å“æ›´å¤§ã€‚</p>
<h3 id="åå·®ä¿®æ­£"><a href="#åå·®ä¿®æ­£" class="headerlink" title="åå·®ä¿®æ­£"></a>åå·®ä¿®æ­£</h3><p>åœ¨è®¡ç®— EWMA æ—¶ï¼Œåˆå§‹å€¼çš„é€‰æ‹©å¯¹åç»­è®¡ç®—çš„å½±å“è¾ƒå¤§ã€‚ç‰¹åˆ«æ˜¯åœ¨æ•°æ®åºåˆ—çš„åˆå§‹é˜¶æ®µï¼Œ<br>ç”±äºç¼ºä¹è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œè®¡ç®—çš„å¹³å‡å€¼å¯èƒ½ä¼šåç¦»çœŸå®å€¼ã€‚<br>å› æ­¤ï¼Œéœ€è¦å¯¹åˆå§‹é˜¶æ®µçš„è®¡ç®—ç»“æœè¿›è¡Œä¿®æ­£ï¼Œä»¥å‡å°è¿™ç§åå·®ã€‚<br>ä¸ºäº†è¿›è¡Œåå·®ä¿®æ­£ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼ï¼š<br>St^ = St / (1 - Î²^t)<br>å…¶ä¸­ï¼š<br>St^æ˜¯ç»è¿‡åå·®ä¿®æ­£çš„æ—¶é—´ t æ—¶åˆ»çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼ã€‚<br>Stæ˜¯æ—¶é—´ t æ—¶åˆ»çš„æœªä¿®æ­£çš„æŒ‡æ•°åŠ æƒå¹³å‡å€¼ã€‚<br>tæ˜¯æ—¶é—´æ­¥æ•°ï¼Œä»1 å¼€å§‹ï¼Œå®ƒæ˜¯Î²^t æ˜¯æŒ‡Î²çš„ t æ¬¡æ–¹ã€‚<br>é€šè¿‡å¯¹åˆå§‹å€¼è¿›è¡Œä¿®æ­£ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ EWMA åœ¨åˆå§‹é˜¶æ®µæ›´æ¥è¿‘çœŸå®å€¼ï¼Œä»è€Œå‡å°‘åå·®ã€‚<br>ä¿®æ­£åçš„ EWMA å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å‡†ç¡®åœ°åæ˜ æ•°æ®çš„è¶‹åŠ¿å’Œå˜åŒ–ã€‚</p>
<h2 id="momentum-1"><a href="#momentum-1" class="headerlink" title="momentum"></a>momentum</h2><p>momentum æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºåŠ é€Ÿæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ã€‚å®ƒé€šè¿‡å¼•å…¥åŠ¨é‡ï¼ˆmomentumï¼‰çš„æ¦‚å¿µæ¥åŠ é€Ÿå‚æ•°çš„æ›´æ–°ã€‚<br>åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œmomentum ä¼šè€ƒè™‘ä¸Šä¸€æ¬¡è¿­ä»£çš„æ¢¯åº¦æ–¹å‘ï¼Œå¹¶æ ¹æ®åŠ¨é‡çš„å¤§å°æ¥è°ƒæ•´å½“å‰çš„æ¢¯åº¦æ–¹å‘ã€‚<br>è¿™æ ·å¯ä»¥åœ¨æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­ï¼Œæ›´åŠ å¹³æ»‘åœ°æ›´æ–°å‚æ•°ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›ã€‚<br>momentum çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š<br>VdW = Î² <em> VdW + (1 - Î²) </em> dW<br>Vdb = Î² <em> Vdb + (1 - Î²) </em> db<br>W = W - Î± <em> VdW<br>b = b - Î± </em> Vdb<br>å…¶ä¸­ï¼š<br>VdWæ˜¯æƒé‡å‚æ•°Wçš„åŠ¨é‡ã€‚<br>Vdbæ˜¯åç½®å‚æ•°bçš„åŠ¨é‡ã€‚<br>dWæ˜¯æƒé‡å‚æ•°Wçš„æ¢¯åº¦ã€‚<br>dbæ˜¯åç½®å‚æ•°bçš„æ¢¯åº¦ã€‚<br>Î±æ˜¯å­¦ä¹ ç‡ã€‚<br>Î²æ˜¯åŠ¨é‡å› å­ï¼Œé€šå¸¸å–å€¼åœ¨0.9åˆ°0.99ä¹‹é—´ã€‚</p>
<p>ç”±äºå°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ‰€é‡‡ç”¨çš„è·¯å¾„å°†â€œæŒ¯è¡â€è‡³æ”¶æ•›ï¼Œåˆ©ç”¨momentumå¯ä»¥å‡å°‘è¿™äº›æŒ¯è¡ã€‚<br>ï¼ˆå°†VdWï¼Œå¸¦å…¥Wæ›´æ–°å¼å­è®¡ç®—ï¼Œä¼šå‘ç° W ä¸‹é™çš„æ¯”ä¹‹å‰è¦æ…¢ä¸€äº›ï¼Œè´Ÿè´Ÿå¾—æ­£ï¼Œä¼šåŠ å›æ¥-Î² <em> VdW + Î² </em> dWï¼‰<br><img src="/image/deepLearning/70.png" alt><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: initialize_velocity</span><br><span class="line"></span><br><span class="line">def initialize_velocity(parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initializes the velocity as a python dictionary with:</span><br><span class="line">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span><br><span class="line">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters.</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    v -- python dictionary containing the current velocity.</span><br><span class="line">                    v[&apos;dW&apos; + str(l)] = velocity of dWl</span><br><span class="line">                    v[&apos;db&apos; + str(l)] = velocity of dbl</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize velocity</span><br><span class="line">    for l in range(L):</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line">        </span><br><span class="line">    return v</span><br></pre></td></tr></table></figure></p>
<p><img src="/image/deepLearning/69.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># GRADED FUNCTION: update_parameters_with_momentum</span><br><span class="line"></span><br><span class="line">def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Update parameters using Momentum</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters:</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    grads -- python dictionary containing your gradients for each parameters:</span><br><span class="line">                    grads[&apos;dW&apos; + str(l)] = dWl</span><br><span class="line">                    grads[&apos;db&apos; + str(l)] = dbl</span><br><span class="line">    v -- python dictionary containing the current velocity:</span><br><span class="line">                    v[&apos;dW&apos; + str(l)] = ...</span><br><span class="line">                    v[&apos;db&apos; + str(l)] = ...</span><br><span class="line">    beta -- the momentum hyperparameter, scalar</span><br><span class="line">    learning_rate -- the learning rate, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    v -- python dictionary containing your updated velocities</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    </span><br><span class="line">    # Momentum update for each parameter</span><br><span class="line">    for l in range(L):</span><br><span class="line">        </span><br><span class="line">        # compute velocities</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = beta * v[&quot;dW&quot; + str(l+1)] + (1 - beta) * grads[&quot;dW&quot; + str(l+1)]</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = beta * v[&quot;db&quot; + str(l+1)] + (1 - beta) * grads[&quot;db&quot; + str(l+1)]</span><br><span class="line">        # update parameters</span><br><span class="line">        parameters[&quot;W&quot; + str(l+1)] -= learning_rate * v[&quot;dW&quot; + str(l+1)]</span><br><span class="line">        parameters[&quot;b&quot; + str(l+1)] -= learning_rate * v[&quot;db&quot; + str(l+1)]</span><br><span class="line">        </span><br><span class="line">    return parameters, v</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that:</p>
</blockquote>
<blockquote>
<p>The velocity is initialized with zeros. So the algorithm will take a few iterations to â€œbuild upâ€ velocity and start to take bigger steps.<br>If  ğ›½=0 , then this just becomes standard gradient descent without momentum.<br>How do you choose  ğ›½ ?</p>
</blockquote>
<blockquote>
<p>The larger the momentum  ğ›½  is, the smoother the update because the more we take the past gradients into account. But if  ğ›½  is too big, it could also smooth out the updates too much.<br>Common values for  ğ›½  range from 0.8 to 0.999. If you donâ€™t feel inclined to tune this,  ğ›½=0.9  is often a reasonable default.<br>Tuning the optimal  ğ›½  for your model might need trying several values to see what works best in term of reducing the value of the cost function  ğ½ .</p>
</blockquote>
<blockquote>
<p>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. - You have to tune a momentum hyperparameter  ğ›½  and a learning rate  ğ›¼ .</p>
</blockquote>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>Root Mean Square Propagation<br>ç±»ä¼¼æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œåœ¨æ›´æ”¹æ›´æ–°æ¢¯åº¦çš„é€»è¾‘ï¼Œä¸å†ç›´æ¥å‡å»å­¦ä¹ ç‡ä¹˜ä»¥æ¢¯åº¦ï¼Œè€Œæ˜¯å‡å»å­¦ä¹ ç‡ä¹˜ä»¥ä¼˜åŒ–å¤„ç†åçš„æ¢¯åº¦å€¼ï¼Œè¯¦è§å¦‚ä¸‹å…¬å¼<br><img src="/image/deepLearning/71.png" alt></p>
<h2 id="Adam-ä¼˜åŒ–ç®—æ³•"><a href="#Adam-ä¼˜åŒ–ç®—æ³•" class="headerlink" title="Adam ä¼˜åŒ–ç®—æ³•"></a>Adam ä¼˜åŒ–ç®—æ³•</h2><p>ç»“åˆæŒ‡æ•°å¹³å‡å’ŒRMSpropä¸¤ç§ç®—æ³•æ›´æ–°æ¢¯åº¦<br><img src="/image/deepLearning/72.png" alt><br><img src="/image/deepLearning/74.png" alt><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: initialize_adam</span><br><span class="line"></span><br><span class="line">def initialize_adam(parameters) :</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Initializes v and s as two python dictionaries with:</span><br><span class="line">                - keys: &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span><br><span class="line">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters.</span><br><span class="line">                    parameters[&quot;W&quot; + str(l)] = Wl</span><br><span class="line">                    parameters[&quot;b&quot; + str(l)] = bl</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span><br><span class="line">                    v[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">                    v[&quot;db&quot; + str(l)] = ...</span><br><span class="line">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span><br><span class="line">                    s[&quot;dW&quot; + str(l)] = ...</span><br><span class="line">                    s[&quot;db&quot; + str(l)] = ...</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2 # number of layers in the neural networks</span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Initialize v, s. Input: &quot;parameters&quot;. Outputs: &quot;v, s&quot;.</span><br><span class="line">    for l in range(L):</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line">        s[&quot;dW&quot; + str(l+1)] = np.zeros_like(parameters[&quot;W&quot; + str(l+1)])</span><br><span class="line">        s[&quot;db&quot; + str(l+1)] = np.zeros_like(parameters[&quot;b&quot; + str(l+1)])</span><br><span class="line">    </span><br><span class="line">    return v, s</span><br><span class="line"></span><br><span class="line"># GRADED FUNCTION: update_parameters_with_adam</span><br><span class="line"></span><br><span class="line">def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,</span><br><span class="line">                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Update parameters using Adam</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters:</span><br><span class="line">                    parameters[&apos;W&apos; + str(l)] = Wl</span><br><span class="line">                    parameters[&apos;b&apos; + str(l)] = bl</span><br><span class="line">    grads -- python dictionary containing your gradients for each parameters:</span><br><span class="line">                    grads[&apos;dW&apos; + str(l)] = dWl</span><br><span class="line">                    grads[&apos;db&apos; + str(l)] = dbl</span><br><span class="line">    v -- Adam variable, moving average of the first gradient, python dictionary</span><br><span class="line">    s -- Adam variable, moving average of the squared gradient, python dictionary</span><br><span class="line">    learning_rate -- the learning rate, scalar.</span><br><span class="line">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span><br><span class="line">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span><br><span class="line">    epsilon -- hyperparameter preventing division by zero in Adam updates</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    v -- Adam variable, moving average of the first gradient, python dictionary</span><br><span class="line">    s -- Adam variable, moving average of the squared gradient, python dictionary</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // 2                 # number of layers in the neural networks</span><br><span class="line">    v_corrected = &#123;&#125;                         # Initializing first moment estimate, python dictionary</span><br><span class="line">    s_corrected = &#123;&#125;                         # Initializing second moment estimate, python dictionary</span><br><span class="line">    </span><br><span class="line">    # Perform Adam update on all parameters</span><br><span class="line">    for l in range(L):</span><br><span class="line">        # Moving average of the gradients. Inputs: &quot;v, grads, beta1&quot;. Output: &quot;v&quot;.</span><br><span class="line">        v[&quot;dW&quot; + str(l+1)] = beta1 * v[&quot;dW&quot; + str(l+1)] + (1 - beta1) * grads[&quot;dW&quot; + str(l+1)]</span><br><span class="line">        v[&quot;db&quot; + str(l+1)] = beta1 * v[&quot;db&quot; + str(l+1)] + (1 - beta1) * grads[&quot;db&quot; + str(l+1)]</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected first moment estimate. Inputs: &quot;v, beta1, t&quot;. Output: &quot;v_corrected&quot;.</span><br><span class="line">        v_corrected[&quot;dW&quot; + str(l+1)] = v[&quot;dW&quot; + str(l+1)] / (1 - beta1 ** t)</span><br><span class="line">        v_corrected[&quot;db&quot; + str(l+1)] = v[&quot;db&quot; + str(l+1)] / (1 - beta1 ** t)</span><br><span class="line"></span><br><span class="line">        # Moving average of the squared gradients. Inputs: &quot;s, grads, beta2&quot;. Output: &quot;s&quot;.</span><br><span class="line">        s[&quot;dW&quot; + str(l+1)] = beta2 * s[&quot;dW&quot; + str(l+1)] + (1 - beta2) * grads[&quot;dW&quot; + str(l+1)] ** 2</span><br><span class="line">        s[&quot;db&quot; + str(l+1)] = beta2 * s[&quot;db&quot; + str(l+1)] + (1 - beta2) * grads[&quot;db&quot; + str(l+1)] ** 2</span><br><span class="line"></span><br><span class="line">        # Compute bias-corrected second raw moment estimate. Inputs: &quot;s, beta2, t&quot;. Output: &quot;s_corrected&quot;.</span><br><span class="line">        s_corrected[&quot;dW&quot; + str(l+1)] = s[&quot;dW&quot; + str(l+1)] / (1 - beta2 ** t)</span><br><span class="line">        s_corrected[&quot;db&quot; + str(l+1)] = s[&quot;db&quot; + str(l+1)] / (1 - beta2 ** t)</span><br><span class="line"></span><br><span class="line">        # Update parameters. Inputs: &quot;parameters, learning_rate, v_corrected, s_corrected, epsilon&quot;. Output: &quot;parameters&quot;.</span><br><span class="line">        parameters[&quot;W&quot; + str(l+1)] -= learning_rate * v_corrected[&quot;dW&quot; + str(l+1)] / (np.sqrt(s_corrected[&quot;dW&quot; + str(l+1)]) + epsilon)</span><br><span class="line">        parameters[&quot;b&quot; + str(l+1)] -= learning_rate * v_corrected[&quot;db&quot; + str(l+1)] / (np.sqrt(s_corrected[&quot;db&quot; + str(l+1)]) + epsilon)</span><br><span class="line"></span><br><span class="line">    return parameters, v, s</span><br></pre></td></tr></table></figure></p>
<p><img src="/image/deepLearning/73.png" alt><br><a href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener">Adam ç®—æ³•åŸç†</a></p>
<h1 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h1><p><a href="https://github.com/YooHannah/algorithm/blob/master/deeplearning/OptGradientDescent/index.py" target="_blank" rel="noopener">ä¸Šé¢ä¸‰ç§ç®—æ³•ç»ƒä¹ </a></p>
<p>å°ç»“</p>
<blockquote>
<p>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</p>
</blockquote>
<blockquote>
<p>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, youâ€™ve seen that Adam converges a lot faster.</p>
</blockquote>
<p>å®éªŒæ•ˆæœAdamç®—æ³•é€Ÿåº¦å¿«ï¼Œå‡†ç¡®ç‡é«˜</p>
<blockquote>
<p>Some advantages of Adam include:</p>
</blockquote>
<blockquote>
<p>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum)<br>Usually works well even with little tuning of hyperparameters (except  ğ›¼ )</p>
</blockquote>
<h1 id="å­¦ä¹ ç‡è¡°å‡"><a href="#å­¦ä¹ ç‡è¡°å‡" class="headerlink" title="å­¦ä¹ ç‡è¡°å‡"></a>å­¦ä¹ ç‡è¡°å‡</h1><p>åœ¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œéšç€è®¡ç®—æ‰¹æ¬¡åç§»ï¼Œé€æ¸å‡å°å­¦ä¹ ç‡çš„å€¼ï¼Œå¯ä»¥é™ä½æ¢¯åº¦éœ‡è¡å¹…åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›é€Ÿåº¦<br>ä¸€äº›å­¦ä¹ ç‡è¡°å‡ç®—æ³•å¦‚ä¸‹<br><img src="/image/deepLearning/75.png" alt><br><img src="/image/deepLearning/76.png" alt></p>
<h1 id="ä¸¤ä¸ªç»éªŒæ³•åˆ™"><a href="#ä¸¤ä¸ªç»éªŒæ³•åˆ™" class="headerlink" title="ä¸¤ä¸ªç»éªŒæ³•åˆ™"></a>ä¸¤ä¸ªç»éªŒæ³•åˆ™</h1><p>Unlikely to get stuck in a bad local optima ä¸€èˆ¬ä¸å­˜åœ¨å±€éƒ¨æœ€ä¼˜è§£ï¼ŒæŸå¤±å‡½æ•°ä¸å‚æ•°å…³ç³»å¾€å¾€æˆé©¬éè£…<br>Plateaus can make learning slow å¹³ç¼“çš„åœ°æ–¹å¾€å¾€ä¼šé€ æˆå­¦ä¹ é€Ÿåº¦ä¸‹é™ï¼Œéœ€è¦èŠ±è´¹æ›´å¤šæ—¶é—´æ‰¾åˆ°æ›´ä¼˜è§£</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deepLearning/" rel="tag"># deepLearning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/post/deepLearning/OptNeuralNetwork.html" rel="next" title="ä¸€äº›ä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œã€è®­ç»ƒè¿‡ç¨‹ã€‘çš„æ–¹æ³•">
                <i class="fa fa-chevron-left"></i> ä¸€äº›ä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œã€è®­ç»ƒè¿‡ç¨‹ã€‘çš„æ–¹æ³•
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/post/deepLearning/HyperparameterTuning.html" rel="prev" title="è¶…å‚æ•°è°ƒä¼˜">
                è¶…å‚æ•°è°ƒä¼˜ <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            æ–‡ç« ç›®å½•
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            ç«™ç‚¹æ¦‚è§ˆ
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/psb.jpg" alt="YooHannah">
          <p class="site-author-name" itemprop="name">YooHannah</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">262</span>
              <span class="site-state-item-name">æ—¥å¿—</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">åˆ†ç±»</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">æ ‡ç­¾</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•"><span class="nav-number">1.</span> <span class="nav-text">å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#momentum"><span class="nav-number">2.</span> <span class="nav-text">momentum</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#æŒ‡æ•°åŠ æƒå¹³å‡"><span class="nav-number">2.1.</span> <span class="nav-text">æŒ‡æ•°åŠ æƒå¹³å‡</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#åå·®ä¿®æ­£"><span class="nav-number">2.1.1.</span> <span class="nav-text">åå·®ä¿®æ­£</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#momentum-1"><span class="nav-number">2.2.</span> <span class="nav-text">momentum</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adam"><span class="nav-number">3.</span> <span class="nav-text">Adam</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSprop"><span class="nav-number">3.1.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam-ä¼˜åŒ–ç®—æ³•"><span class="nav-number">3.2.</span> <span class="nav-text">Adam ä¼˜åŒ–ç®—æ³•</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#å®éªŒ"><span class="nav-number">4.</span> <span class="nav-text">å®éªŒ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#å­¦ä¹ ç‡è¡°å‡"><span class="nav-number">5.</span> <span class="nav-text">å­¦ä¹ ç‡è¡°å‡</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ä¸¤ä¸ªç»éªŒæ³•åˆ™"><span class="nav-number">6.</span> <span class="nav-text">ä¸¤ä¸ªç»éªŒæ³•åˆ™</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YooHannah</span>
</div>


<div class="powered-by">
  ç”± <a class="theme-link" href="https://hexo.io">Hexo</a> å¼ºåŠ›é©±åŠ¨
</div>

<div class="theme-info">
  ä¸»é¢˜ -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/treedocument/treedocument.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  

  


</body>
</html>
