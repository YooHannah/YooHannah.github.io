<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="My Little World" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta property="og:type" content="website">
<meta property="og:title" content="My Little World">
<meta property="og:url" content="http://yoohannah.github.io/page/2/index.html">
<meta property="og:site_name" content="My Little World">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="My Little World">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoohannah.github.io/page/2/">





  <title> My Little World </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">My Little World</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">learn and share</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/nodejs/streamExamp.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/nodejs/streamExamp.html" itemprop="url">
                  一个实现流通信的案例
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-01-26T16:17:37+08:00">
                2025-01-26
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>前端包括node层和纯前端层，需要请求第三方http接口在页面实现chatGTP的打字机效果</p>
<h1 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h1><ol>
<li>在node层调用第三方http接口，避免跨域问题</li>
<li>由于第三方接口为流式接口，从node层发出请求再转发到前端也需要进行流式通信</li>
<li>前端层对返回的流式数据进行处理后更新数据呈现在页面上</li>
</ol>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ol>
<li>前端层使用fetch进行请求，使用ReadableStream进行流式处理</li>
<li>node层使用axios进行请求，使用stream进行流式处理  </li>
</ol>
<h2 id="node层实现"><a href="#node层实现" class="headerlink" title="node层实现"></a>node层实现</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> axios <span class="keyword">from</span> <span class="string">'axios'</span>;</span><br><span class="line"><span class="keyword">import</span> &#123; PassThrough &#125; <span class="keyword">from</span> <span class="string">'stream'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">faqStream</span>(<span class="params">body?: any</span>): <span class="title">Promise</span>&lt;<span class="title">any</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">const</span> ctx = useContext&lt;HttpContext&gt;();</span><br><span class="line">  ctx.set(&#123;</span><br><span class="line">    Connection: <span class="string">'keep-alive'</span>,</span><br><span class="line">    <span class="string">'Cache-Control'</span>: <span class="string">'no-cache'</span>,</span><br><span class="line">    <span class="string">'Content-Type'</span>: <span class="string">'application/octet-stream'</span> <span class="comment">// 表示返回数据是个 stream</span></span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">const</span> stream = <span class="keyword">new</span> PassThrough();</span><br><span class="line">  ctx.body = stream;</span><br><span class="line">  <span class="comment">// 发起第三方请求</span></span><br><span class="line">  <span class="keyword">const</span> headers = &#123;</span><br><span class="line">    <span class="string">'Content-Type'</span>: <span class="string">'application/json'</span></span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">const</span> url = <span class="string">'http://vvvv.xxxx.net/aiBot/oncall_response_stream'</span>;</span><br><span class="line">  axios</span><br><span class="line">    .post(url, ctx.request.body, &#123; <span class="attr">headers</span>: headers, <span class="attr">responseType</span>: <span class="string">'stream'</span> &#125;)</span><br><span class="line">    .then(<span class="function">(<span class="params">response</span>) =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (response.status !== <span class="number">200</span>) &#123;</span><br><span class="line">        <span class="built_in">console</span>.error(<span class="string">'Error status:'</span>, response.status);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      response.data.on(<span class="string">'data'</span>, (chunk) =&gt; &#123;</span><br><span class="line">        chunk</span><br><span class="line">          .toString()</span><br><span class="line">          .split(<span class="string">'\n\n'</span>)</span><br><span class="line">          .filter(<span class="function">(<span class="params">item</span>) =&gt;</span> item)</span><br><span class="line">          .forEach(<span class="function">(<span class="params">chunkStr</span>) =&gt;</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> chunkJson = &#123;&#125;;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              chunkJson = <span class="built_in">JSON</span>.parse(chunkStr);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (error) &#123;</span><br><span class="line">              <span class="built_in">console</span>.error(<span class="string">'Error parse:'</span>, error);</span><br><span class="line">              <span class="built_in">console</span>.error(<span class="string">'Error chunkStr:'</span>, chunkStr);</span><br><span class="line">              <span class="built_in">console</span>.error(<span class="string">'Error origin chunk:'</span>, chunk.toString());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (chunkJson?.data?.chunk) &#123;</span><br><span class="line">              <span class="comment">// 拿到有效数据后，传给前端</span></span><br><span class="line">              stream.write(chunkJson.data.chunk);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;);</span><br><span class="line">      &#125;);</span><br><span class="line">      response.data.on(<span class="string">'end'</span>, () =&gt; &#123;</span><br><span class="line">        <span class="comment">// 第三方请求流结束后，关闭向前端写的流</span></span><br><span class="line">        stream.end();</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;)</span><br><span class="line">    .catch(<span class="function">(<span class="params">error</span>) =&gt;</span> &#123;</span><br><span class="line">      <span class="built_in">console</span>.error(<span class="string">'Error all:'</span>, error);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="前端层实现"><a href="#前端层实现" class="headerlink" title="前端层实现"></a>前端层实现</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> &#123; useState, useCallback, useRef &#125; <span class="keyword">from</span> <span class="string">'react'</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> [listLoading, setListLoading] = useState(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">const</span> [hasReport, setHasReport] = useState(<span class="number">-1</span>);</span><br><span class="line">  <span class="keyword">const</span> [AiRemoteData, setAIRemoteData] = useState(<span class="string">''</span>);</span><br><span class="line">  <span class="keyword">const</span> AiRequestController = useRef();</span><br><span class="line">  <span class="keyword">const</span> &#123; addThrottle &#125; = useThrottleFunc();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 拉取Ai 回答</span></span><br><span class="line">  <span class="keyword">const</span> getAIRemoteData = useCallback(</span><br><span class="line">    addThrottle(</span><br><span class="line">      <span class="keyword">async</span> () =&gt; &#123;</span><br><span class="line">        <span class="keyword">const</span> &#123; keyWord &#125; = query;</span><br><span class="line">        <span class="keyword">if</span> (!keyWord) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          setListLoading(<span class="function">(<span class="params">val</span>) =&gt;</span> val + <span class="number">1</span>);</span><br><span class="line">          <span class="keyword">if</span> (AiRequestController.current) &#123;</span><br><span class="line">            <span class="comment">// 如果当前请求存在，则取消当前请求</span></span><br><span class="line">            AiRequestController.current.abort();</span><br><span class="line">          &#125;</span><br><span class="line">          AiRequestController.current = <span class="keyword">new</span> AbortController();</span><br><span class="line">          <span class="keyword">const</span> jwtToken = <span class="keyword">await</span> getJwt();</span><br><span class="line">          <span class="comment">// 1. 创建一个新的请求</span></span><br><span class="line">          <span class="keyword">const</span> response = <span class="keyword">await</span> fetch(</span><br><span class="line">            <span class="string">`https://hahahaha.net/api/diagnosisBasic/faqStream`</span>,</span><br><span class="line">            &#123;</span><br><span class="line">              method: <span class="string">'POST'</span>,</span><br><span class="line">              body: <span class="built_in">JSON</span>.stringify(&#123;</span><br><span class="line">                user_id: userInfo.id,</span><br><span class="line">                query: keyWord,</span><br><span class="line">                class: ''</span><br><span class="line">              &#125;),</span><br><span class="line">              headers: &#123;</span><br><span class="line">                <span class="string">'x-jwt-token'</span>: jwtToken</span><br><span class="line">              &#125;,</span><br><span class="line">              signal: AiRequestController.current.signal</span><br><span class="line">            &#125;</span><br><span class="line">          );</span><br><span class="line">          <span class="keyword">const</span> reader = response.body.getReader(); <span class="comment">// 获取reader</span></span><br><span class="line">          <span class="keyword">const</span> decoder = <span class="keyword">new</span> TextDecoder(); <span class="comment">// 文本解码器</span></span><br><span class="line">          <span class="keyword">let</span> answer = <span class="string">''</span>; <span class="comment">// 存储答案</span></span><br><span class="line">          <span class="comment">// 2. 循环取值</span></span><br><span class="line">          <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 取值, value 是后端返回流信息, done 表示后端结束流的输出</span></span><br><span class="line">            <span class="keyword">const</span> &#123; value, done &#125; = <span class="keyword">await</span> reader.read();</span><br><span class="line">            <span class="keyword">if</span> (done) &#123;</span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 对 value 进行解码</span></span><br><span class="line">            <span class="keyword">const</span> val = decoder.decode(value);</span><br><span class="line">            <span class="keyword">if</span> (!answer) &#123;</span><br><span class="line">              setListLoading(<span class="function">(<span class="params">count</span>) =&gt;</span> count - <span class="number">1</span>);</span><br><span class="line">              setHasReport(<span class="number">-1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            answer += val;</span><br><span class="line">            setAIRemoteData(answer);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          setAIRemoteData(<span class="string">''</span>);</span><br><span class="line">          <span class="built_in">console</span>.error(<span class="string">'数据解析出错'</span>);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          setListLoading(<span class="function">(<span class="params">val</span>) =&gt;</span> val - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="number">500</span>,</span><br><span class="line">      <span class="string">'getAIRemoteData'</span></span><br><span class="line">    ),</span><br><span class="line">    [query.keyWord]</span><br><span class="line">  );</span><br></pre></td></tr></table></figure>
<h3 id="番外-全局节流函数"><a href="#番外-全局节流函数" class="headerlink" title="番外: 全局节流函数"></a>番外: 全局节流函数</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> throttleTimerList: &#123; [key: string]: Timeout | <span class="literal">null</span> &#125; = &#123;&#125;;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> useThrottleFunc = <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">const</span> timer = useRef();</span><br><span class="line">  <span class="keyword">const</span> addThrottle = (</span><br><span class="line">    fn: <span class="function">(<span class="params">params?: LooseObject | <span class="literal">undefined</span></span>) =&gt;</span> <span class="keyword">void</span>,</span><br><span class="line">    waitTime?: number,</span><br><span class="line">    timerKey?: string</span><br><span class="line">  ) =&gt; &#123;</span><br><span class="line">    <span class="keyword">const</span> timerFlag = timerKey || <span class="string">'getRemoteData'</span>;</span><br><span class="line">    throttleTimerList[timerFlag] = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="function">(<span class="params">params?: LooseObject | <span class="literal">undefined</span></span>) =&gt;</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (throttleTimerList[timerFlag]) &#123;</span><br><span class="line">        clearTimeout(throttleTimerList[timerFlag]);</span><br><span class="line">      &#125;</span><br><span class="line">      throttleTimerList[timerFlag] = setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        fn(params);</span><br><span class="line">        clearTimeout(throttleTimerList[timerFlag]);</span><br><span class="line">        throttleTimerList[timerFlag] = <span class="literal">null</span>;</span><br><span class="line">      &#125;, waitTime || <span class="number">500</span>);</span><br><span class="line">    &#125;;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  useEffect(</span><br><span class="line">    () =&gt; <span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="built_in">Object</span>.keys(throttleTimerList).forEach(<span class="function">(<span class="params">key</span>) =&gt;</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (throttleTimerList[key]) &#123;</span><br><span class="line">          clearTimeout(throttleTimerList[key]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">delete</span> throttleTimerList[key];</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;,</span><br><span class="line">    []</span><br><span class="line">  );</span><br><span class="line">  <span class="keyword">return</span> &#123;</span><br><span class="line">    addThrottle,</span><br><span class="line">    throttleTimer: timer</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="相关知识链接"><a href="#相关知识链接" class="headerlink" title="相关知识链接"></a>相关知识链接</h1><p><a href="https://nodejs.cn/api/webstreams.html#%E7%A4%BA%E4%BE%8B-readablestream" target="_blank" rel="noopener">NODE-Stream</a><br><a href="https://nodejs.cn/api/webstreams.html#%E7%A4%BA%E4%BE%8B-readablestream" target="_blank" rel="noopener">NODE-ReadableStream</a><br><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/ReadableStream" target="_blank" rel="noopener">ReadableStream</a><br><a href="https://juejin.cn/post/7211401380770349115" target="_blank" rel="noopener">application/octet-stream vs text/event-stream</a><br><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/AbortController" target="_blank" rel="noopener">AbortController</a><br><a href="https://juejin.cn/post/7329884027186724901" target="_blank" rel="noopener">fetch获取流式数据相关问题</a><br><a href="https://juejin.cn/post/7212270321622286394#heading-7" target="_blank" rel="noopener">在 Koa 中基于 gpt-3.5 模型实现一个最基本的流式问答 DEMO</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/deepLearning/NeuralNetwork.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/deepLearning/NeuralNetwork.html" itemprop="url">
                  神经网络的相关推导公式
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-12-01T21:24:37+08:00">
                2024-12-01
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="矩阵维度"><a href="#矩阵维度" class="headerlink" title="矩阵维度"></a>矩阵维度</h1><p>为方便重复计算，减少for循环的使用，在神经网络的计算过程中，尽可能的将数据转成向量进行计算<br>利用向量的广播能力进行快速计算，神经网络多层传递过程中，矩阵的维度一般遵循以下关系</p>
<p>如果前一层（输入）维度为（m,1），中间层维度是（n, 1）, 后一层（输出）维度是（p， 1）<br>那么 中间层w的维度就是（n, m）， b 的维度就是（n，1）, b 的维度始终和中间层一致<br>输出层W的维度（p,n），b 的维度就是(p,1)</p>
<p><img src="/image/deepLearning/11.png" alt></p>
<h3 id="参数的矩阵维度关系"><a href="#参数的矩阵维度关系" class="headerlink" title="参数的矩阵维度关系"></a>参数的矩阵维度关系</h3><p><img src="/image/deepLearning/28.png" alt></p>
<h1 id="向前传播计算过程"><a href="#向前传播计算过程" class="headerlink" title="向前传播计算过程"></a>向前传播计算过程</h1><h2 id="一个神经元的计算"><a href="#一个神经元的计算" class="headerlink" title="一个神经元的计算"></a>一个神经元的计算</h2><p>每个神经元的计算包括两部分，先计算z,在用激活函数计算a,<br>同一层不同神经元计算的区别就在于使用不同的参数<br><img src="/image/deepLearning/12.png" alt><br>如果将参数w和b整理成向量，对当前样本数据进行一次性向量计算<br>就可以直接得到当前层的直接产出向量<br><img src="/image/deepLearning/13.png" alt></p>
<h2 id="一层神经元的计算"><a href="#一层神经元的计算" class="headerlink" title="一层神经元的计算"></a>一层神经元的计算</h2><p>同样，每一层都可以用相同的计算式表示<br><img src="/image/deepLearning/14.png" alt></p>
<h2 id="一组样本数据的计算"><a href="#一组样本数据的计算" class="headerlink" title="一组样本数据的计算"></a>一组样本数据的计算</h2><p>通过for 循环进行每一层的计算可得到所有样本数据的预测数据y^<br><img src="/image/deepLearning/15.png" alt><br>但是通过将输入层维度(m,1) 的向量增加为（m,x）的向量，可以实现一次计算x个样本的效果，从而去掉for循环<br>如果中间层有n个神经元，输出得到的结果就是(n,x)的矩阵<br>第n - 1行 上的x个数，每个数代表每个样本数据在中间层第n-1个神经元的计算后的值<br>第x - 1列 上的n个数，每个数代表第x-i个样本数据在中间层计算后的每个神经元的值<br><img src="/image/deepLearning/16.png" alt><br>最终经过两层神经元处理后变成，结果变成(1,x)的向量，每个值代表每个样本经过神经网络计算后的预测值</p>
<h3 id="输入输出值矩阵维度之间的关系"><a href="#输入输出值矩阵维度之间的关系" class="headerlink" title="输入输出值矩阵维度之间的关系"></a>输入输出值矩阵维度之间的关系</h3><p><img src="/image/deepLearning/29.png" alt></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><img src="/image/deepLearning/17.png" alt></p>
<h1 id="其他激活函数"><a href="#其他激活函数" class="headerlink" title="其他激活函数"></a>其他激活函数</h1><p>除了sigmoid 激活函数外，常见的激活函数还有Tanh, ReLu,和leaky ReLu,<br>后三者更常见，且使用更广泛，sigmoid基本只用于二分类场景<br><img src="/image/deepLearning/18.png" alt></p>
<h2 id="为什么不使用线性函数作为激活函数"><a href="#为什么不使用线性函数作为激活函数" class="headerlink" title="为什么不使用线性函数作为激活函数"></a>为什么不使用线性函数作为激活函数</h2><p>因为如果使用线性函数作为激活函数，无论神经网络有多少层，都相当于只进行了一次线性函数计算，隐藏层作用消失<br><img src="/image/deepLearning/19.png" alt></p>
<h2 id="不同激活函数的导数"><a href="#不同激活函数的导数" class="headerlink" title="不同激活函数的导数"></a>不同激活函数的导数</h2><p><img src="/image/deepLearning/20.png" alt><br><img src="/image/deepLearning/21.png" alt><br><img src="/image/deepLearning/22.png" alt></p>
<h1 id="向后传播过程"><a href="#向后传播过程" class="headerlink" title="向后传播过程"></a>向后传播过程</h1><p>回顾一下梯度下降的计算过程<br><img src="/image/deepLearning/24.png" alt><br>对于单个神经元的向后传播过程，就是计算单个神经元参数偏导数的过程<br><img src="/image/deepLearning/23.png" alt></p>
<p>对于多层的神经网络进行带入<br><img src="/image/deepLearning/25.png" alt></p>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><p>同向前传播一样，通过引入向量矩阵，减少for循环<br><img src="/image/deepLearning/26.png" alt></p>
<h1 id="为啥不能初始化参数为0？"><a href="#为啥不能初始化参数为0？" class="headerlink" title="为啥不能初始化参数为0？"></a>为啥不能初始化参数为0？</h1><p>如果初始化参数为0或相同值，那么所有节点计算的值都相同，会产生对称性，对后续计算的影响也相同，同样会导致隐藏层节点结算无效<br>解决办法就是随机初始化参数<br><img src="/image/deepLearning/27.png" alt><br>一般一开始会将参数随机成比较小的值，如果一开始是比较大的值，z 的值就会比较大，<br>当激活函数是sigmoid 或者tanh 这样的激活函数时，<br>计算结果所在的位置就会在梯度比较平缓的地方导致，激活函数处于比较饱和的状态，梯度下降比较慢，影响学习速度</p>
<h1 id="深度网络"><a href="#深度网络" class="headerlink" title="深度网络"></a>深度网络</h1><h2 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h2><p><img src="/image/deepLearning/30.png" alt></p>
<h2 id="向后传播"><a href="#向后传播" class="headerlink" title="向后传播"></a>向后传播</h2><p><img src="/image/deepLearning/31.png" alt></p>
<h2 id="一些超参数"><a href="#一些超参数" class="headerlink" title="一些超参数"></a>一些超参数</h2><p><img src="/image/deepLearning/32.png" alt></p>
<h2 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h2><p><img src="/image/deepLearning/33.png" alt></p>
<h3 id="为什么要使用深层网络"><a href="#为什么要使用深层网络" class="headerlink" title="为什么要使用深层网络"></a>为什么要使用深层网络</h3><p>使用小的（单层神经元数据量少的）的但是有多层的深层网络，往往会比使用浅层(layer数少)网络计算步骤更简洁<br>比如下面的电路与或非门计算过程<br>如果像左侧使用深层网络，每一次层神经元都少一半<br>如果使用右侧单层神经网络，这一层上的神经元会以2的指数方式计算<br>总体算下来，深层网络需要处理的神经元会少很多</p>
<p><img src="/image/deepLearning/34.png" alt></p>
<h1 id="实验练习"><a href="#实验练习" class="headerlink" title="实验练习"></a>实验练习</h1><h2 id="逻辑回归全过程"><a href="#逻辑回归全过程" class="headerlink" title="逻辑回归全过程"></a>逻辑回归全过程</h2><p><a href="https://github.com/GeeeekExplorer/AndrewNg-Deep-Learning/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%20Solution.ipynb" target="_blank" rel="noopener">gitbub[ipynb]链接</a><br><a href="https://colab.research.google.com/github/GeeeekExplorer/AndrewNg-Deep-Learning/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset%20Solution.ipynb#scrollTo=OEOpKgClZLco" target="_blank" rel="noopener">实验</a></p>
<h2 id="思路梳理"><a href="#思路梳理" class="headerlink" title="思路梳理"></a>思路梳理</h2><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>实验中要实现对一张图片是否是猫的判断，<br>首先要对图片进行处理，将图片转换成向量，<br>一个像素点由RGB三个数据组成, 现在如果横竖都取图片的64个像素点<br>一张64X64的图片就有64X64=4096个 [r,g,b] 这样的数据，<br>一张图片的数据表示就是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [[196 192 190], [193 186 182],...中间还有61组, [188 179 174]], 每一行 有64个</span><br><span class="line">    [[196 192 190], [193 186 182],..., [188 179 174]],</span><br><span class="line">    ... 中间有60行</span><br><span class="line">    [[196 192 190], [193 186 182],..., [188 179 174]]</span><br><span class="line">    [[196 192 190], [193 186 182],..., [88 79 74]]</span><br><span class="line">] 一共64行</span><br></pre></td></tr></table></figure>
<p>现在把所有数据摊平再转置，就可转成一个[64X64X3=12288, 1]的向量,<br>也就是m个测试数据组成的矩阵中的一列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[196,], [192,],..., [88,], [79,],[74,]]</span><br></pre></td></tr></table></figure>
<p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[0], -1).T</span><br></pre></td></tr></table></figure>
<p>现在我们有209个训练数据的训练集train_set_x_orig的维度是(209, 64, 64, 3)<br>a 就是209<br>现将要将训练集数据一次性转成209列的向量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m_train = train_set_x_orig.shape[0] // 209</span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T</span><br></pre></td></tr></table></figure></p>
<p>train_set_x_flatten 现在的维度就是(12288, 209)<br>每一列是一张图片的像素数据</p>
<h3 id="数据中心标准化"><a href="#数据中心标准化" class="headerlink" title="数据中心标准化"></a>数据中心标准化</h3><p>基于现在处理的是图片的像素数据，所以所有的数据肯定都在0~255之间</p>
<blockquote>
<p>One common preprocessing step in machine learning is to center and standardize your dataset,<br>meaning that you substract the mean of the whole numpy array from each example,<br>and then divide each example by the standard deviation of the whole numpy array.<br>But for picture datasets,<br>it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p>
</blockquote>
<p>一个常见的预处理步骤是尽可能将数据聚拢到坐标系0附近，常用的方法是对数据进行标准化，<br>也就是将数据减去均值，然后将数据除以标准差<br>但是对于图片数据集来说，<br>除以255（像素通道的最大值），会更简单，而且效果也差不多<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten / 255.</span><br></pre></td></tr></table></figure></p>
<h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><blockquote>
<p>What you need to remember:<br>Common steps for pre-processing a new dataset are:<br>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)<br>Reshape the datasets such that each example is now a vector of size (num_px <em> num_px </em> 3, 1)<br>“Standardize” the data</p>
</blockquote>
<p>常见的数据预处理步骤：</p>
<ol>
<li>确定问题的维度和形状（m_train, m_test, num_px, …）</li>
<li>将数据集重新组织成每个示例都是大小为（num_px <em> num_px </em> 3, 1）的向量</li>
<li>标准化数据</li>
</ol>
<h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><blockquote>
<p>The main steps for building a Neural Network are:</p>
</blockquote>
<blockquote>
<p>Define the model structure (such as number of input features)<br>Initialize the model’s parameters<br>Loop:<br>Calculate current loss (forward propagation)<br>Calculate current gradient (backward propagation)<br>Update parameters (gradient descent)<br>You often build 1-3 separately and integrate them into one function we call model().</p>
</blockquote>
<p>构建一个神经网络模型的主要步骤：</p>
<ol>
<li>定义模型结构（例如输入特征的数量,这里是一张图片的12288个rgb数据）</li>
<li>初始化模型的参数</li>
<li>循环：<br> 计算当前损失（前向传播）<br> 计算当前梯度（反向传播）<br> 更新参数（梯度下降）<br>通常会将1-3分别构建, 然后将它们集成到一个函数中，我们称之为model()。<br><img src="/image/deepLearning/LogReg_kiank.png" alt></li>
</ol>
<h3 id="𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现"><a href="#𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现" class="headerlink" title="𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现"></a>𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数实现</h3><p>𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑤𝑇𝑥+𝑏)=1/(1+𝑒−(𝑤𝑇𝑥+𝑏))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">    Compute the sigmoid of z</span><br><span class="line">    Arguments:</span><br><span class="line">    z -- A scalar or numpy array of any size.</span><br><span class="line">    Return:</span><br><span class="line">    s -- sigmoid(z)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def sigmoid(z):</span><br><span class="line">    s = 1/(1+np.exp(-z))</span><br><span class="line">    return s</span><br></pre></td></tr></table></figure></p>
<h3 id="初始化模型的参数"><a href="#初始化模型的参数" class="headerlink" title="初始化模型的参数"></a>初始化模型的参数</h3><p>用0来初始化参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span><br><span class="line">Argument:</span><br><span class="line">dim -- num_px * num_px * 3</span><br><span class="line">Returns:</span><br><span class="line">w -- initialized vector of shape (dim, 1)</span><br><span class="line">b -- initialized scalar (corresponds to the bias)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def initialize_with_zeros(dim):</span><br><span class="line">    w = np.zeros((dim, 1))</span><br><span class="line">    b = 0</span><br><span class="line">    return w, b</span><br><span class="line"></span><br><span class="line">## 测试</span><br><span class="line">dim = 2</span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"></span><br><span class="line">==&gt;</span><br><span class="line">w = [[0.]</span><br><span class="line"> [0.]]</span><br><span class="line">b = 0.0</span><br></pre></td></tr></table></figure></p>
<h3 id="前向向后传播实现"><a href="#前向向后传播实现" class="headerlink" title="前向向后传播实现"></a>前向向后传播实现</h3><p>根据公式进行代码实现<br><img src="/image/deepLearning/35.png" alt><br>最终得到每轮训练的损失函数和梯度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: propagate</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Implement the cost function and its gradient for the propagation explained above</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span><br><span class="line">b -- bias, a scalar</span><br><span class="line">X -- data of size (num_px * num_px * 3, number of examples)</span><br><span class="line">Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span><br><span class="line"></span><br><span class="line">Return:</span><br><span class="line">cost -- negative log-likelihood cost for logistic regression</span><br><span class="line">dw -- gradient of the loss with respect to w, thus same shape as w</span><br><span class="line">db -- gradient of the loss with respect to b, thus same shape as b</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def propagate(w, b, X, Y):</span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    </span><br><span class="line">    # FORWARD PROPAGATION (FROM X TO COST)</span><br><span class="line">    A = sigmoid(w.T @ X + b)                                    # compute activation 得到 (m,1) 的矩阵A,m 是训练集样本数</span><br><span class="line">    cost = -np.mean(Y * np.log(A) + (1 - Y) * np.log(1 - A))    # compute cost, 在某些 NumPy 的特定版本或上下文中, np.mean 的输出可能是形状为 (1,) 的数组，而不是一个纯标量</span><br><span class="line">    </span><br><span class="line">    # BACKWARD PROPAGATION (TO FIND GRAD)</span><br><span class="line">    dw = X @ (A - Y).T / m</span><br><span class="line">    db = np.mean(A - Y)</span><br><span class="line"></span><br><span class="line">    assert(dw.shape == w.shape)</span><br><span class="line">    assert(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost) # 移除多余的单一维度，确保 cost 是标量</span><br><span class="line">    assert(cost.shape == ()) # 这里明确要求 cost 的形状是 ()，即零维标量。如果 cost 是 (1,)，那么会触发断言错误。</span><br><span class="line">    </span><br><span class="line">    grads = &#123;&quot;dw&quot;: dw,</span><br><span class="line">             &quot;db&quot;: db&#125;</span><br><span class="line">    </span><br><span class="line">    return grads, cost</span><br></pre></td></tr></table></figure>
<h3 id="梯度下降实现"><a href="#梯度下降实现" class="headerlink" title="梯度下降实现"></a>梯度下降实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">This function optimizes w and b by running a gradient descent algorithm</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span><br><span class="line">b -- bias, a scalar</span><br><span class="line">X -- data of shape (num_px * num_px * 3, number of examples)</span><br><span class="line">Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span><br><span class="line">num_iterations -- number of iterations of the optimization loop</span><br><span class="line">learning_rate -- learning rate of the gradient descent update rule</span><br><span class="line">print_cost -- True to print the loss every 100 steps</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">params -- dictionary containing the weights w and bias b</span><br><span class="line">grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span><br><span class="line">costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span><br><span class="line"></span><br><span class="line">1) Calculate the cost and the gradient for the current parameters. Use propagate().</span><br><span class="line">2) Update the parameters using gradient descent rule for w and b.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):</span><br><span class="line">     costs = [] // 收集每轮计算的损失函数值</span><br><span class="line">    </span><br><span class="line">    for i in range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        # Cost and gradient calculation (≈ 1-4 lines of code)</span><br><span class="line">        # 第一轮用初始化w和b计算的损失函数和梯度</span><br><span class="line">        # 后面用更新后的w和b计算的损失函数和梯度</span><br><span class="line">        grads, cost = propagate(w, b, X, Y) </span><br><span class="line">        </span><br><span class="line">        # 解构梯度</span><br><span class="line">        dw = grads[&quot;dw&quot;]</span><br><span class="line">        db = grads[&quot;db&quot;]</span><br><span class="line">        </span><br><span class="line">        # 梯度下降更新参数</span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        # Record the costs</span><br><span class="line">        # 每100轮记录一次损失函数值</span><br><span class="line">        if i % 100 == 0: </span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        # 如果需要每100轮打印下损失函数就再打印下</span><br><span class="line">        if print_cost and i % 100 == 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line">    </span><br><span class="line">    # num_iterations轮 训练结束后返回最终更新到的参数，梯度，和损失函数集合(可以用于绘制学习曲线)</span><br><span class="line">    params = &#123;&quot;w&quot;: w,</span><br><span class="line">              &quot;b&quot;: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;&quot;dw&quot;: dw,</span><br><span class="line">             &quot;db&quot;: db&#125;</span><br><span class="line">    </span><br><span class="line">    return params, grads, costs</span><br></pre></td></tr></table></figure>
<h3 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h3><p>根据公式实现预测函数</p>
<p>  𝑌̂ =𝐴=𝜎(𝑤𝑇𝑋+𝑏)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span><br><span class="line">b -- bias, a scalar</span><br><span class="line">X -- data of size (num_px * num_px * 3, number of examples)</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># X 是摊平后数据 (12288, m), X.shape[0] 是影响因素个数 12288 个RGB值, X.shape[1] 是训练集样本数</span><br><span class="line">def predict(w, b, X):</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    Y_prediction = np.zeros((1,m))</span><br><span class="line">    w = w.reshape(X.shape[0], 1)</span><br><span class="line">    </span><br><span class="line">    # Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture</span><br><span class="line">    A = sigmoid(w.T @ X + b)</span><br><span class="line">    </span><br><span class="line">    for i in range(A.shape[1]):</span><br><span class="line">        # Convert probabilities A[0,i] to actual predictions p[0,i] </span><br><span class="line">        # 大于0.5 预测为1 是猫, 小于0.5 预测为0, 不是猫</span><br><span class="line">        Y_prediction[0, i] = A[0, i] &gt; 0.5</span><br><span class="line">    </span><br><span class="line">    assert(Y_prediction.shape == (1, m))</span><br><span class="line">    </span><br><span class="line">    return Y_prediction</span><br></pre></td></tr></table></figure>
<h3 id="组装模型"><a href="#组装模型" class="headerlink" title="组装模型"></a>组装模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Builds the logistic regression model by calling the function you&apos;ve implemented previously</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line">X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span><br><span class="line">Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span><br><span class="line">X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span><br><span class="line">Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span><br><span class="line">num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span><br><span class="line">learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span><br><span class="line">print_cost -- Set to true to print the cost every 100 iterations</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">d -- dictionary containing information about the model.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):</span><br><span class="line"></span><br><span class="line">    # initialize parameters with zeros (≈ 1 line of code)</span><br><span class="line">    # 初始化模型的参数</span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[0])</span><br><span class="line"></span><br><span class="line">    # Gradient descent (≈ 1 line of code)</span><br><span class="line">    # 根据训练数据采用梯度下降方法更新参数</span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    # Retrieve parameters w and b from dictionary &quot;parameters&quot;</span><br><span class="line">    w = parameters[&quot;w&quot;]</span><br><span class="line">    b = parameters[&quot;b&quot;]</span><br><span class="line">    </span><br><span class="line">    # Predict test/train set examples (≈ 2 lines of code)</span><br><span class="line">    # 用训练好的参数预测测试集和训练集的结果</span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    # Print train/test Errors</span><br><span class="line">    # 打印训练集和测试集的准确率</span><br><span class="line">    print(&quot;train accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))</span><br><span class="line">    print(&quot;test accuracy: &#123;&#125; %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))</span><br><span class="line"></span><br><span class="line">    # 返回模型训练的损失函数值(学习曲线)，训练测试数据集的预测结果(判断模型是否拟合)，模型的参数, 学习率，迭代次数等信息</span><br><span class="line">    d = &#123;&quot;costs&quot;: costs,</span><br><span class="line">         &quot;Y_prediction_test&quot;: Y_prediction_test, </span><br><span class="line">         &quot;Y_prediction_train&quot; : Y_prediction_train, </span><br><span class="line">         &quot;w&quot; : w, </span><br><span class="line">         &quot;b&quot; : b,</span><br><span class="line">         &quot;learning_rate&quot; : learning_rate,</span><br><span class="line">         &quot;num_iterations&quot;: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    return d</span><br><span class="line"></span><br><span class="line"># 调用模型</span><br><span class="line"># 注意入参train_set_x, train_set_y, test_set_x, test_set_y, 是经过预处理的数据集</span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)</span><br></pre></td></tr></table></figure>
<h3 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h3><ol>
<li>预测结果分析</li>
</ol>
<p>除了函数本身里面的准确率计算，可以初步判断模型是否过拟合训练数据，还可以单独拿出一个测试数据，和 预测数据进行结果比较，进行验证</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = 14</span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))</span><br><span class="line">print (&quot;y = &quot; + str(test_set_y[0,index]) + &quot;, you predicted that it is a \&quot;&quot; + classes[int(d[&quot;Y_prediction_test&quot;][0,index])].decode(&quot;utf-8&quot;) +  &quot;\&quot; picture.&quot;)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>学习曲线分析</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Plot learning curve (with costs)</span><br><span class="line">costs = np.squeeze(d[&apos;costs&apos;])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(&apos;cost&apos;)</span><br><span class="line">plt.xlabel(&apos;iterations (per hundreds)&apos;)</span><br><span class="line">plt.title(&quot;Learning rate =&quot; + str(d[&quot;learning_rate&quot;]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>学习率分析 or 超参数分析</li>
</ol>
<p>增加训练次数，观察学习曲线变化同理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [0.01, 0.001, 0.0001]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line">for i in learning_rates:</span><br><span class="line">    print (&quot;learning rate is: &quot; + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)</span><br><span class="line">    print (&apos;\n&apos; + &quot;-------------------------------------------------------&quot; + &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">for i in learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][&quot;costs&quot;]), label = str(models[str(i)][&quot;learning_rate&quot;]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(&apos;cost&apos;)</span><br><span class="line">plt.xlabel(&apos;iterations (hundreds)&apos;)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=&apos;upper center&apos;, shadow=True)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(&apos;0.90&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="应用训练结果进行预测"><a href="#应用训练结果进行预测" class="headerlink" title="应用训练结果进行预测"></a>应用训练结果进行预测</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">my_image = &quot;my_image2.jpg&quot;   # change this to the name of your image file </span><br><span class="line"></span><br><span class="line">fname = &quot;images/&quot; + my_image</span><br><span class="line">image = np.array(plt.imread(fname))</span><br><span class="line">image = image/255.</span><br><span class="line">my_image = np.array(Image.fromarray(np.uint8(image)).resize((num_px,num_px))).reshape((1, num_px*num_px*3)).T // 摊平数据(12288, 1)</span><br><span class="line">#my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T</span><br><span class="line">my_predicted_image = predict(d[&quot;w&quot;], d[&quot;b&quot;], my_image)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(&quot;y = &quot; + str(np.squeeze(my_predicted_image)) + &quot;, your algorithm predicts a \&quot;&quot; + classes[int(np.squeeze(my_predicted_image)),].decode(&quot;utf-8&quot;) +  &quot;\&quot; picture.&quot;)</span><br></pre></td></tr></table></figure>
<p><a href="https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">二分类问题的实践</a><br><a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit框架</a></p>
<h2 id="使用隐藏层实现对非线性数据的分类"><a href="#使用隐藏层实现对非线性数据的分类" class="headerlink" title="使用隐藏层实现对非线性数据的分类"></a>使用隐藏层实现对非线性数据的分类</h2><p>题目<br>我们现在有一堆数据分布成花朵的形状，非线性，如下，<br><img src="/image/deepLearning/37.png" alt><br>可以看到上面的数据有的是红色，有的是蓝色，假设红色代表支持特朗普，蓝色代表支持拜登<br>我们希望用一个神经网络来对这些数据进行分类，<br>分类结果就是输入数据可以直接得到数据是红色还是蓝色的标签<br>解决思路就想办法对红色和蓝色的数据集中地区进行分块划分，<br>如果我们还是用sigmoid 函数，那么就会变成线性的，不会得到正确的区块划分<br><img src="/image/deepLearning/38.png" alt><br>我们希望用一个非线性函数把数据进行精确度更好的划分</p>
<blockquote>
<p>The general methodology to build a Neural Network is to: </p>
<ol>
<li>Define the neural network structure ( # of input units, # of hidden units, etc). </li>
<li>Initialize the model’s parameters </li>
<li>Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent)</li>
</ol>
<p>You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model().<br>Once you’ve built nn_model() and learnt the right parameters, you can make predictions on new data.</p>
</blockquote>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="/image/deepLearning/39.png" alt></p>
<h3 id="涉及方程"><a href="#涉及方程" class="headerlink" title="涉及方程"></a>涉及方程</h3><p>向前传播<br><img src="/image/deepLearning/40.png" alt><br>反向传播<br><img src="/image/deepLearning/41.png" alt></p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>重点关注模型组装好后，如何使用，如何预测，精确度计算，超参数如何训练，<a href="https://yoohannah.github.io/post/machineLearning/MachineLearningDevelopmentProcess.html">迁移学习</a>怎么做</p>
<p><a href="https://github.com/YooHannah/algorithm/blob/master/deeplearning/NeuralNetworkModel/index.py" target="_blank" rel="noopener">非线性逻辑回归实现代码</a></p>
<h2 id="L层神经网络实现"><a href="#L层神经网络实现" class="headerlink" title="L层神经网络实现"></a>L层神经网络实现</h2><p>整体架构<br><img src="/image/deepLearning/LModel.png" alt></p>
<ol>
<li>向前传播的时候，前L-1层都是先线性然后用relu 函数激活，最后一层是线性然后用sigmoid 函数激活<blockquote>
<p>The model’s structure is: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.<br><img src="/image/deepLearning/forward.png" alt></p>
</blockquote>
</li>
<li>计算损失函数<br><img src="/image/deepLearning/cost.png" alt></li>
<li>反向传播的时候，与向前传播相反，除第一层是线性然后用sigmoid 函数激活，后面l-1层是线性然后用relu 函数激活，前面的每一层都是线性然后用relu 函数激活<blockquote>
<p>The model’s structure is: SIGMOID -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; … -&gt; SIGMOID.<br>for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps.<br><img src="/image/deepLearning/backforward.png" alt><br>使用链式法则, 对下面的线性函数求导dw, db, dA<br><img src="/image/deepLearning/dao1.png" alt><br>得到反向传播计算公式<br><img src="/image/deepLearning/dao2.png" alt></p>
</blockquote>
</li>
</ol>
<p>注意输出层的sigmoid 函数求导<br><img src="/image/deepLearning/dao3.png" alt></p>
<p><a href="https://github.com/YooHannah/algorithm/blob/master/deeplearning/DeepNeuralNetwork/index.py" target="_blank" rel="noopener">L层神经网络实现代码</a></p>
<h2 id="运行异常"><a href="#运行异常" class="headerlink" title="运行异常"></a>运行异常</h2><ol>
<li>参数初始化问题</li>
</ol>
<table>
<thead>
<tr>
<th>语句</th>
<th>初始化方式</th>
<th>优缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>np.random.randn(layer_dims[l], layer_dims[l - 1])</code></td>
<td>标准正态分布初始化</td>
<td>简单，但可能导致梯度爆炸或梯度消失，尤其是在深层网络中。</td>
</tr>
<tr>
<td><code>np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])</code></td>
<td>Xavier初始化的变体，<code>np.sqrt(layer_dims[l-1])</code>是上一层神经元个数</td>
<td>提供更稳定的梯度和激活值，适合对称激活函数（如Sigmoid、Tanh）。减少梯度爆炸或梯度消失问题。</td>
</tr>
</tbody>
</table>
<p>如果使用 ReLU 或 Leaky ReLU 作为激活函数，可以采用 He 初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters[&apos;W&apos; + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/deepLearning/basicKnowledge.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/deepLearning/basicKnowledge.html" itemprop="url">
                  一些基础知识
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-11-27T17:10:37+08:00">
                2024-11-27
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="深度学习为什么会崛起"><a href="#深度学习为什么会崛起" class="headerlink" title="深度学习为什么会崛起"></a>深度学习为什么会崛起</h1><p>随着 数据量的增多，计算能力的提高以及算法的进步，使得深度学习的训练周期变短，可以快速进行迭代更新优化<br>从而用于工业生成</p>
<p>下面以二分类问题为例，复习一下相关的数学知识和概念</p>
<h1 id="损失函数和成本函数"><a href="#损失函数和成本函数" class="headerlink" title="损失函数和成本函数"></a>损失函数和成本函数</h1><p>重新理解一下<br>损失函数是一个训练数据的预测结果和实际值的差<br>成本函数是所有训练数据的损失函数的平均值<br><img src="/image/deepLearning/1.png" alt></p>
<h1 id="常见求导公式"><a href="#常见求导公式" class="headerlink" title="常见求导公式"></a>常见求导公式</h1><p>1.C’=0(C为常数)；<br>2.(Xn)’=nX(n-1) (n∈R)；<br>3.(sinX)’=cosX；<br>4.(cosX)’=-sinX；<br>5.(aX)’=aXIna （ln为自然对数）；<br>6.(logaX)’=1/(Xlna) (a&gt;0，且a≠1)；<br>7.(tanX)’=1/(cosX)2=(secX)2<br>8.(cotX)’=-1/(sinX)2=-(cscX)2<br>9.(secX)’=tanX secX；<br>10.(cscX)’=-cotX cscX；</p>
<h1 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h1><h2 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h2><p>计算成本函数<br><img src="/image/deepLearning/2.png" alt></p>
<h2 id="向后传播"><a href="#向后传播" class="headerlink" title="向后传播"></a>向后传播</h2><p>通过链式求导得到每一轮计算中参数的导数，从而用于进行梯度下降计算<br><img src="/image/deepLearning/3.png" alt></p>
<h1 id="梯度下降计算过程"><a href="#梯度下降计算过程" class="headerlink" title="梯度下降计算过程"></a>梯度下降计算过程</h1><p><img src="/image/deepLearning/4.png" alt><br><img src="/image/deepLearning/5.png" alt><br><img src="/image/deepLearning/6.png" alt><br><img src="/image/deepLearning/7.png" alt><br>Neural network programming guideline<br>Whenever possible, avoid explicit for-loops.<br>避免for-loops循环计算带来的算力损耗，使用向量对上面两次循环(训练数迭代和参数迭代)进行优化，最终只剩训练次数一次loop 循环<br><img src="/image/deepLearning/8.png" alt></p>
<h2 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h2><p>通过使用向量的广播计算，可以大幅度减少for循环的计算成本<br>广播常见的计算过程如下</p>
<p><img src="/image/deepLearning/9.png" alt><br><img src="/image/deepLearning/10.png" alt></p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>使用广播的运算概念，实现下面的softmax 函数<br><img src="/image/deepLearning/36.png" alt><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def softmax(x):</span><br><span class="line">    &quot;&quot;&quot;Calculates the softmax for each row of the input x.</span><br><span class="line"></span><br><span class="line">    Your code should work for a row vector and also for matrices of shape (m,n).</span><br><span class="line"></span><br><span class="line">    Argument:</span><br><span class="line">    x -- A numpy matrix of shape (m,n)</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    s -- A numpy matrix equal to the softmax of x, of shape (m,n)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Apply exp() element-wise to x. Use np.exp(...).</span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line"></span><br><span class="line">    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).</span><br><span class="line">    x_sum = np.sum(x_exp, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line">    </span><br><span class="line">    return s</span><br><span class="line"></span><br><span class="line">x = np.array([</span><br><span class="line">    [9, 2, 5, 0, 0],</span><br><span class="line">    [7, 5, 0, 0 ,0]])</span><br><span class="line">print(&quot;softmax(x) = &quot; + str(softmax(x)))</span><br><span class="line">==&gt;</span><br><span class="line">softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04</span><br><span class="line">  1.21052389e-04]</span><br><span class="line"> [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04</span><br><span class="line">  8.01252314e-04]]</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/GeeeekExplorer/AndrewNg-Deep-Learning/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python%20Basics%20With%20Numpy%20Solution.ipynb" target="_blank" rel="noopener">github 实验练习</a></p>
<p><a href="https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html" target="_blank" rel="noopener">numpy 官网</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/dataWash.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/dataWash.html" itemprop="url">
                  数据清洗和转换
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-10-15T20:14:37+08:00">
                2024-10-15
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="常见数据错误形式"><a href="#常见数据错误形式" class="headerlink" title="常见数据错误形式"></a>常见数据错误形式</h1><ol>
<li>超出正常数据范围，值或者太大或者太小</li>
<li>不符合相关校验规则，比如数值类型只能是整形，货币单位应该是美元，但是出现了英镑</li>
<li>形式错误，比如日期格式错误，电话号码格式错误，不符合相关语法语义规范</li>
</ol>
<p><img src="/image/LLM/236.png" alt></p>
<p>对于这样的数据要进行去除清洗处理</p>
<h1 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h1><p>针对数值，图片，视频，文字不同输入类型，有不同转换方式<br><img src="/image/LLM/237.png" alt><br><img src="/image/LLM/238.png" alt><br><img src="/image/LLM/239.png" alt><br><img src="/image/LLM/240.png" alt><br><img src="/image/LLM/241.png" alt><br>小结</p>
<ol>
<li>数值类型转换就采用各种归一化的计算公式进行转换</li>
<li>图片，可以采用裁剪尺寸，下采样，压缩，白化处理</li>
<li>视频，可以采用截取关键片段，采样关键针降低数据处理陈本</li>
<li>文字，可以进行词干提取，词形还原以及token化处理<br><img src="/image/LLM/242.png" alt></li>
</ol>
<h1 id="mL-常见算法类型"><a href="#mL-常见算法类型" class="headerlink" title="mL 常见算法类型"></a>mL 常见算法类型</h1><p>监督，自监督，半监督，无监督，强化学习<br><img src="/image/LLM/243.png" alt><br><img src="/image/LLM/244.png" alt></p>
<h2 id="自监督相关概念"><a href="#自监督相关概念" class="headerlink" title="自监督相关概念"></a>自监督相关概念</h2><p><img src="/image/LLM/245.png" alt></p>
<h2 id="决策树相关注意点"><a href="#决策树相关注意点" class="headerlink" title="决策树相关注意点"></a>决策树相关注意点</h2><p><img src="/image/LLM/246.png" alt><br><img src="/image/LLM/247.png" alt><br><img src="/image/LLM/248.png" alt></p>
<h2 id="二分类评价指标"><a href="#二分类评价指标" class="headerlink" title="二分类评价指标"></a>二分类评价指标</h2><p><img src="/image/LLM/249.png" alt></p>
<h2 id="复杂度解释or解释"><a href="#复杂度解释or解释" class="headerlink" title="复杂度解释or解释"></a>复杂度解释or解释</h2><p><img src="/image/LLM/250.png" alt><br><img src="/image/LLM/251.png" alt><br><img src="/image/LLM/252.png" alt><br><img src="/image/LLM/253.png" alt><br><img src="/image/LLM/254.png" alt></p>
<p><a href="https://c.d2l.ai/stanford-cs329p/syllabus.html#data-i" target="_blank" rel="noopener">李沐机器学习ppt</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/datalabel.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/datalabel.html" itemprop="url">
                  数据标注
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-10-14T21:05:37+08:00">
                2024-10-14
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>根据标注数据的目的(数据优化还是训练模型)，数据当前的标注情况以及预算情况<br>可以根据下面的流程进行数据标注<br><img src="/image/LLM/226.png" alt></p>
<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><p>如果一开始有一部分数据可以进行监督学习训练，然后用未标记的数据进行测试，拿到测试结果，<br>根据测试结果准确性判断是否将当前未标记数据当做标记数据添加到下一轮的训练中<br><img src="/image/LLM/227.png" alt><br><img src="/image/LLM/228.png" alt><br>由于这里的工作是进行数据标注，所以可以使用较深的神经网络或者较贵的模型进行训练<br>以保证得到更准确的标注结果</p>
<h1 id="众包标注"><a href="#众包标注" class="headerlink" title="众包标注"></a>众包标注</h1><p>如果有足够的资金预算，可以将数据交给第三方进行标注，然后将标注结果进行汇总<br><img src="/image/LLM/229.png" alt><br>但要面临如何降低标注门槛，标注质量，价格昂贵，标注人员不稳定等问题<br><img src="/image/LLM/230.png" alt></p>
<h2 id="主动学习"><a href="#主动学习" class="headerlink" title="主动学习"></a>主动学习</h2><p>只将训练结果最不确定的数据，或者最难标记的数据进行人工标注，然后用多个模型投票保证标记准确度<br><img src="/image/LLM/231.png" alt><br>通常与半监督学习结合使用<br><img src="/image/LLM/232.png" alt></p>
<h2 id="质量控制"><a href="#质量控制" class="headerlink" title="质量控制"></a>质量控制</h2><p>防止标错或者范围有问题，可以选择将一个数据发给多个标注人员进行标注，然后根据标注结果进行投票<br>但这样做会导致标注成本增加<br>降低成本的方法可以是，<br>一是从结果角度思考，先让模型进行推测，如果人工标注与模型推测结果相差较大，则将数据发给多个标注人员进行标注<br>否则停止任务发送，减少成本；或者发送的前几个人标记结果都一样，就停止发送更多人进行标记<br>二是从人的角度思考，先给一些有确定标注的数据给标记人员进行标注，如果标注结果与确定标注相差结果较大，说明标注人员能力有问题，进行人员更换<br><img src="/image/LLM/233.png" alt></p>
<h2 id="弱监督学习"><a href="#弱监督学习" class="headerlink" title="弱监督学习"></a>弱监督学习</h2><p>使用启发式规则通过数据编程得到一些有噪音的标注<br>通过半自动化的方式生成准确度弱于人工标记，但足以进行模型训练的标注<br>通过根据数据特征的一系列判断(启发式规则)进行投票，然后将投票结果进行阈值比较，从而判断属于哪个分类标签<br><img src="/image/LLM/233.png" alt></p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>三种常见数据标注方式</p>
<ol>
<li>半监督学习</li>
<li>众包标注</li>
<li>弱监督学习<br>对于没有标记的数据也可以用无监督或者自监督学习进行训练<br><img src="/image/LLM/233.png" alt></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/ReinforcementLearning.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/ReinforcementLearning.html" itemprop="url">
                  强化学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-10-07T14:34:37+08:00">
                2024-10-07
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>强化学习的主要思想不是告诉算法每个输入的正确输出是什么<br>而是指定一个奖励函数，告诉它什么时候做的好，什么时候做的不好<br>算法的工作是自动找出如何选择好的动作</p>
<p><img src="/image/LLM/202.png" alt></p>
<h1 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h1><p>以火星探测器为例，在其决定路线的过程中产生的几个概念</p>
<ol>
<li>S ： 当前状态</li>
<li>a : 动作</li>
<li>S’ : 下一个状态</li>
<li>R : 奖励函数</li>
<li><p>teminal state : 终止状态<br><img src="/image/LLM/203.png" alt><br>每种路线回报通过计算路上每一步奖励乘以折现系数加和得到<br><img src="/image/LLM/204.png" alt></p>
</li>
<li><p>policy : 策略函数，根据当前的状态选择可以获得最大收益的动作</p>
</li>
</ol>
<p>A policy is a function π（s）= a  mapping from states to actions, that tells you what action a to take in a given state s.</p>
<p>The goal of reinforcement learning ===&gt;<br>Find a policy 5 that tells you what action (a = 5(s)) to take in every state (s) so as to maximize the return.</p>
<p><img src="/image/LLM/205.png" alt></p>
<h1 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h1><p>未来只取决于当前的名称，而不是到达当前状态之前可能发生的任何事情</p>
<p><img src="/image/LLM/206.png" alt></p>
<h1 id="状态动作回报函数-Q"><a href="#状态动作回报函数-Q" class="headerlink" title="状态动作回报函数 Q"></a>状态动作回报函数 Q</h1><p>当前状态下执行一次函数能够得到的最大回报值<br>如果能够找到最大回报值也就能知道接下来应该用什么动作</p>
<p><img src="/image/LLM/207.png" alt></p>
<p><img src="/image/LLM/208.png" alt></p>
<h1 id="贝尔曼公式"><a href="#贝尔曼公式" class="headerlink" title="贝尔曼公式"></a>贝尔曼公式</h1><p><img src="/image/LLM/209.png" alt><br><img src="/image/LLM/210.png" alt><br><img src="/image/LLM/211.png" alt></p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>面对环境随机的情况，动作实际执行过程可能存在多个可能路线，导致每次得到的最大回报值不同，因此计算当前状态最大收益时取所有路线情况的平均值进行计算<br>即使用期望回报值进行计算<br><img src="/image/LLM/212.png" alt><br><img src="/image/LLM/213.png" alt></p>
<h1 id="DQN-算法"><a href="#DQN-算法" class="headerlink" title="DQN 算法"></a>DQN 算法</h1><p>D: deep learning<br>Q: Q function<br>N: Network</p>
<p>对于连续状态值的情况，使用神经网络训练Q函数进行深度强化学习<br><img src="/image/LLM/214.png" alt><br><img src="/image/LLM/215.png" alt><br><img src="/image/LLM/216.png" alt><br>By using experience replay we avoid problematic correlations, oscillations and instabilities. In addition, experience replay also allows the agent to potentially use the same experience in multiple weight updates, which increases data efficiency.<br>通过使用经验重放，我们可以避免有问题的相关性、振荡和不稳定性。此外，经验重放还允许代理在多次权重更新中使用相同的经验，从而提高数据效率。</p>
<h2 id="优化-1"><a href="#优化-1" class="headerlink" title="优化"></a>优化</h2><h3 id="优化神经网络结构"><a href="#优化神经网络结构" class="headerlink" title="优化神经网络结构"></a>优化神经网络结构</h3><p>上面将s和a作为X 同时参与训练，最终只会得到一个动作a最大回报函数值,需要进行多次运算<br>如果仅将s作为输入，输出层产生多个a的回报值，就可以根据回报值大小选择相应的动作</p>
<p><img src="/image/LLM/217.png" alt><br><img src="/image/LLM/218.png" alt></p>
<h3 id="epsilon-greedy-policy"><a href="#epsilon-greedy-policy" class="headerlink" title="epsilon-greedy policy"></a>epsilon-greedy policy</h3><p>选择action 过程中，如果一直按Q值最大原则选择action,万一初始值特别小无法开启我们想要的第一步程序，就会导致无法进行后续的action<br>epsilon-greedy policy 方案就是找一个合适的阈值epsilon，比如说0.05,<br>95%的时间选择最大Q值action (贪婪剥削策略)<br>5%的时间选择随机选择action（探索策略）<br>这样就可以避免选到固定不符合预期的action,开放一定的窗口有可能选到其他的action<br>epsilon 大小 类似于梯度，随训练过程进行会逐渐变小，变小的过程，模型也就学会了如何选择更有可能选择符合预期的action</p>
<p><img src="/image/LLM/219.png" alt></p>
<h3 id="小批量"><a href="#小批量" class="headerlink" title="小批量"></a>小批量</h3><p>训练数据如果非常庞大，在训练过程中可能会造成时间消耗，为了提高训练速度，可以采用小批量的方式进行<br>将训练数据分成多个批次，每次迭代用不同批次数据，虽然梯度会比较嘈杂，但还是会朝着梯度下降的方向进行<br><img src="/image/LLM/220.png" alt><br><img src="/image/LLM/221.png" alt><br><img src="/image/LLM/222.png" alt><br><img src="/image/LLM/223.png" alt></p>
<h3 id="软更新"><a href="#软更新" class="headerlink" title="软更新"></a>软更新</h3><p>在更新参数过程中，每次按比例更新参数，每次仅更新部分比例的参数，可以使强化学习更好的收敛<br><img src="/image/LLM/224.png" alt></p>
<h1 id="强化学习的一些限制"><a href="#强化学习的一些限制" class="headerlink" title="强化学习的一些限制"></a>强化学习的一些限制</h1><p><img src="/image/LLM/225.png" alt></p>
<p><a href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Unsupervised%20learning%20recommenders%20reinforcement%20learning/week3/Practice%20Lab-Reinforcement%20Learning/C3_W3_A1_Assignment.ipynb" target="_blank" rel="noopener">实验练习</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/RecommenderSystem.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/RecommenderSystem.html" itemprop="url">
                  推荐系统
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-10-06T07:10:37+08:00">
                2024-10-06
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>下面以根据电影评分推荐电影为例，介绍推荐系统的开发过程</p>
<h1 id="思路整理"><a href="#思路整理" class="headerlink" title="思路整理"></a>思路整理</h1><p>根据电影的特征，用户对每个电影都会有一个评分(0-5分)，比如电影A的评分是5分，电影B的评分是4分，电影C的评分是3分<br>一般情况下，用户对某电影评分越高，说明后续对该类型电影的青睐度越高，对于系统来说越值得推荐给用户<br>即系统需要预测用户对某个电影X的评分，从而决定是否推荐给用户<br>系统需要依赖的数据是电影的特征数据（X）和以往用户对电影的评分数据(Y)<br>根据二者的关系，计算出相关的算法参数<br>当有需要预测一个电影评分的时候，输入待评分电影特征即可得到评分，然后根据阈值判断是否推荐给用户<br>下面是对一个观众的电影评分预测过程<br><img src="/image/LLM/179.png" alt><br>对应的成本函数<br><img src="/image/LLM/180.png" alt><br>如果训练集有多个用户的评分数据，拿到所有用户的参数后加合，就可以预测大众用户对某个电影的整体评分情况<br><img src="/image/LLM/181.png" alt></p>
<h1 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h1><p>对于电影的评分，一开始不能确定使用电影的哪些特征，协同过滤算法将输入，电影的特征X， 也看做是一个参数参与成本函数的计算<br>从而利用梯度下降过程找到合适的参数和特征值</p>
<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><ol>
<li>已知多个人对一部电影的评分和相关参数，可以反推出X 的 情况<br><img src="/image/LLM/182.png" alt></li>
<li>如果现在已知多个人对一部电影的评分和相关参数，可以对电影特征X进行成本函数计算<br>加和之后可以对多个电影的特征进行预测<br><img src="/image/LLM/183.png" alt></li>
<li>观察成本函数的公式，现在加和参数和特征的成本函数，<br><img src="/image/LLM/185.png" alt><br><img src="/image/LLM/184.png" alt></li>
<li>可以发现， 梯度下降过程，可以同时找出多个人对电影评分参数和对多部电影的特征值推测<br><img src="/image/LLM/186.png" alt></li>
</ol>
<p>协同在这里的体现在于多个用户对一部电影进行了评价，通过合作得到了对电影的整体评价，同时可以预测出能够代表这部电影的特征值<br>反过来，可以预测尚未对同一部电影进行评分的其他用户的评分<br>即从多个用户收集数据，然后预测其他用户的评分</p>
<h1 id="线性回归转向二进制分类"><a href="#线性回归转向二进制分类" class="headerlink" title="线性回归转向二进制分类"></a>线性回归转向二进制分类</h1><p>基于线性回归的推荐系统适合于上面评分有连续值的推测，基于上述思路，将评分结果通过逻辑函数转成二进制结果<br>即可实现二进制分类问题的预测<br><img src="/image/LLM/187.png" alt><br><img src="/image/LLM/188.png" alt><br><img src="/image/LLM/189.png" alt></p>
<h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><h2 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h2><p>对于尚未对电影作为任何评价的用户，如果参数为0，那么预测的评分结果为0，会极大影响推荐的准确性<br>因此，先对多用户评分计算均值，然后对所有用户的评分减去均值，得到新的评分，在新的评分上进行参数获取<br>进行评分预测的时候再把均值加回来，这样即使参数为0，预测的评分值也是之前评分的均值，对整体评分meian值不会有影响<br><img src="/image/LLM/190.png" alt></p>
<h2 id="如何找到相似的推荐"><a href="#如何找到相似的推荐" class="headerlink" title="如何找到相似的推荐"></a>如何找到相似的推荐</h2><p>找到与当前item 距离最近的其他item<br><img src="/image/LLM/191.png" alt></p>
<h2 id="协同过滤的限制"><a href="#协同过滤的限制" class="headerlink" title="协同过滤的限制"></a>协同过滤的限制</h2><ol>
<li>对冷启动问题不友好</li>
<li>不能直接获取到有价值的特征数据，可能是关于观众或者电影的片面的信息，只能从这些信息上推测用户的爱好<br><img src="/image/LLM/192.png" alt></li>
</ol>
<h1 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h1><p>对比基于协同过滤的推荐算法(根据用户对相似item的评分进行推荐)<br>基于内容的推荐算法同时基于用户和item的特征，通过计算item 和用户的匹配度，来判断用户是否对该item感兴趣<br><img src="/image/LLM/193.png" alt></p>
<h2 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h2><p>利用神经网络从用户特征和item 特征中提取n 个特征，计算二者的点积从而判断用户是否对item感兴趣，是否要推荐给用户<br><img src="/image/LLM/194.png" alt><br><img src="/image/LLM/195.png" alt><br><img src="/image/LLM/196.png" alt><br><img src="/image/LLM/197.png" alt><br><img src="/image/LLM/198.png" alt></p>
<h1 id="从大目录中进行推荐"><a href="#从大目录中进行推荐" class="headerlink" title="从大目录中进行推荐"></a>从大目录中进行推荐</h1><ol>
<li><p>进行检索，找出候选列表<br>但是检索过程需要注意，通过对更多的项目进行检索可以得到更好的结果但是检索的速度回变慢，<br>为了分析优化权衡二者，可以实施离线实验观察新增的检索项是否增加了检索结果的相关性</p>
</li>
<li><p>对候选列表进行fine-tune排序找出得分最高的item给用户</p>
</li>
</ol>
<p><img src="/image/LLM/199.png" alt><br><img src="/image/LLM/200.png" alt><br><img src="/image/LLM/201.png" alt></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/abnormalTest.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/abnormalTest.html" itemprop="url">
                  异常检测
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-10-05T18:50:37+08:00">
                2024-10-05
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h1><p>异常检测是机器学习中一个重要的概念，它是指在数据中检测出不符合预期的数据点，以便及时发现和处理异常情况。</p>
<h2 id="检测方法：密度估计"><a href="#检测方法：密度估计" class="headerlink" title="检测方法：密度估计"></a>检测方法：密度估计</h2><p>密度估计是异常检测中的一种方法，它通过计算数据的密度分布来识别异常数据点。<br>通过将特征值的可能性进行乘积计算，得到数据的密度，然后跟阈值进行比较，判断当前数据是否正常。<br><img src="/image/LLM/164.png" alt><br><img src="/image/LLM/169.png" alt><br><img src="/image/LLM/165.png" alt></p>
<h1 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h1><p>高斯分布的位置受数据集的平均值𝜇和方差𝜎^2 决定<br><img src="/image/LLM/166.png" alt><br>𝜇 决定钟形最高点在x轴上的位置<br>𝜎^2 决定钟形的宽度，因为整个钟形面积为1，所以，如果𝜎^2 变小，那个整个钟形会变得很高<br><img src="/image/LLM/167.png" alt></p>
<p><img src="/image/LLM/168.png" alt></p>
<h1 id="异常检测算法步骤"><a href="#异常检测算法步骤" class="headerlink" title="异常检测算法步骤"></a>异常检测算法步骤</h1><ol>
<li>在训练数据中选择你认为可能会引起异常的数据的n个特征值</li>
<li>计算各个特征值的平均值𝜇和方差𝜎^2</li>
<li>计算每个特征值的密度，如果密度小于阈值，则认为是异常数据</li>
</ol>
<p><img src="/image/LLM/170.png" alt></p>
<h1 id="开发过程中如何评估异常检测系统"><a href="#开发过程中如何评估异常检测系统" class="headerlink" title="开发过程中如何评估异常检测系统"></a>开发过程中如何评估异常检测系统</h1><p>通常采用实数评估方案，就是可以用一个具体的数值小小来衡量算法的好坏<br>假设训练数据中的数据都是正常的，存在标签0<br>在交叉验证和测试数据中，存在少量异常数据，标签为1<br>测试算法好坏的标准就是能否将异常数据识别出来<br>如果是特别少量的异常数据，可以仅通过交叉验证来评估测试好坏<br>从而决定合适的阈值大小<br><img src="/image/LLM/171.png" alt><br><img src="/image/LLM/172.png" alt><br>另外的评估方法可参考<a href>二分类错误度量</a><br>相关的衡量指标，真假值，精确率，召回率，F1 score等<br><img src="/image/LLM/173.png" alt></p>
<h1 id="如何选择异常检测算法和监督学习"><a href="#如何选择异常检测算法和监督学习" class="headerlink" title="如何选择异常检测算法和监督学习"></a>如何选择异常检测算法和监督学习</h1><p>异常检测算法适用于</p>
<ol>
<li>正常数据少量但是有大量异常数据</li>
<li>异常的特征值是未知的，且是多样的</li>
</ol>
<p>监督学习算法适用于</p>
<ol>
<li>存在大量异常和正常数据</li>
<li>有足够数据告诉算法什么是正常的数据，什么是异常的数据<br>将来出现的异常数据很可能是之前训练集中出现过的异常数据</li>
</ol>
<p><img src="/image/LLM/174.png" alt></p>
<p><img src="/image/LLM/175.png" alt></p>
<h1 id="如何选择要使用的特征"><a href="#如何选择要使用的特征" class="headerlink" title="如何选择要使用的特征"></a>如何选择要使用的特征</h1><ol>
<li><p>选择符合高斯分布的特征，如果不能拟合高斯分布，可以通过取对数，开方等方式进行特殊处理后进行拟合<br>但要注意，训练数据如果有对特征值进行特殊处理，那么交叉验证集合测试集也要进行相同的处理<br><img src="/image/LLM/176.png" alt></p>
</li>
<li><p>对于误判的异常数据，可以通过增加特征值来进行弥补，保证数据在新加的特征值上存在异常大或者异常小的数据</p>
</li>
</ol>
<p><img src="/image/LLM/177.png" alt></p>
<ol start="3">
<li>可以在已有的原始特征值基础上创造新的特征值参与运算<br><img src="/image/LLM/178.png" alt></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/Kmeans.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/Kmeans.html" itemprop="url">
                  Kmeans 聚类算法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-10-03T00:37:37+08:00">
                2024-10-03
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无监督学习算法一</p>
<h1 id="什么是聚类"><a href="#什么是聚类" class="headerlink" title="什么是聚类"></a>什么是聚类</h1><p>将输入数据分成多个类别，每个类别包含一组相似的数据点的过程就是聚类，分好的类被称为cluster</p>
<h1 id="聚类算法k-means处理过程"><a href="#聚类算法k-means处理过程" class="headerlink" title="聚类算法k-means处理过程"></a>聚类算法k-means处理过程</h1><ol>
<li>随机选择k个点作为初始的中心点, 计算每个点到中心点的距离，将每个点分配到距离最近的中心点所属的类别中</li>
<li>计算每个类别的中心点(平均值)，并更新中心点，重复步骤1</li>
<li>直到中心点不再变化，或者达到最大迭代次数</li>
</ol>
<h2 id="具体实现-伪代码"><a href="#具体实现-伪代码" class="headerlink" title="具体实现(伪代码)"></a>具体实现(伪代码)</h2><p><img src="/image/LLM/157.png" alt><br><img src="/image/LLM/158.png" alt></p>
<h2 id="成本函数"><a href="#成本函数" class="headerlink" title="成本函数"></a>成本函数</h2><p><img src="/image/LLM/159.png" alt><br><img src="/image/LLM/160.png" alt></p>
<h2 id="如何初始化中心点"><a href="#如何初始化中心点" class="headerlink" title="如何初始化中心点"></a>如何初始化中心点</h2><p>随机选择k个点作为初始的中心点（k小于m），计算最终中心点和损失函数<br>重复N次，选择损失函数最小的中心点作为最终的中心点<br><img src="/image/LLM/161.png" alt></p>
<h2 id="如何选择K的数量"><a href="#如何选择K的数量" class="headerlink" title="如何选择K的数量"></a>如何选择K的数量</h2><p>取决后续如何使用分类好的集群数据<br><img src="/image/LLM/163.png" alt><br><img src="/image/LLM/162.png" alt></p>
<p><a href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Unsupervised%20learning%20recommenders%20reinforcement%20learning/week1/2%20Practice%20Lab1/C3_W1_KMeans_Assignment.ipynb" target="_blank" rel="noopener">压缩图像应用</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/machineLearning/decisionTreeModel.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/machineLearning/decisionTreeModel.html" itemprop="url">
                  决策树模型
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-09-30T15:31:37+08:00">
                2024-09-30
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>决策树模型是通过计算特征纯度后，选取最大纯度值的特征作为决策节点，<br>将数据根据是否符合当前特征节点一份为二，再根据特征纯度，继续划分，<br>最后根据停止划分规则进行数据分类或推测的模型</p>
<p><img src="/image/LLM/136.png" alt></p>
<h1 id="创建过程"><a href="#创建过程" class="headerlink" title="创建过程"></a>创建过程</h1><p>决策树创建过程需要考虑两件事</p>
<ol>
<li><p>在每个节点上如果选择根据什么特征进行数据分类<br>Maximize purity<br>如果一个特征在把数据分成两组之后，使分组后的数据能够最大程度的趋于同一类，那么这个特征就是纯度高的特征,<br>即 如果这个特征能够直接决定数据属于哪个分类的程度越高，纯度就越高<br>比如用DNA特征判断猫狗分类比用耳朵是尖的还是软的更直接，DNA特征就是最大纯度的特征<br><img src="/image/LLM/137.png" alt></p>
</li>
<li><p>什么时候停止数据分类<br>a. 当一个节点里面的数据都属于同一类的时候<br>b. 到达树的深度最大值的时候，树越深，过拟合越有可能，计算成本越高<br>c. 当特征纯度(熵)低于某个阈值的时候<br>d. 当节点里的数据个数低于某个阈值的时候</p>
</li>
</ol>
<p><img src="/image/LLM/138.png" alt></p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>可以理解为数据的混乱程度，如果数据特别混乱，则值越大，返回数据如果种类单一，则值越小，趋近0<br>这里用熵来计算特征的非纯度或者较杂质程度，<br>如果根据某个特征分类后的数据的熵 越小，说数据越干净，杂质越少<br>反之，如果得到的熵越大，说数据越混乱，不同类的数据越多<br>如下面判断是否是猫的问题<br>p1 代表是每组数据中猫的比例，都是猫或狗的话熵 是0，5:5 的时候熵 最大值为1，数据最混乱<br><img src="/image/LLM/139.png" alt><br>具体熵 的计算公式如下<br><img src="/image/LLM/140.png" alt></p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>在了解熵的含义后，用下面的计算过程选择节点的判断特征<br>根节点的熵减去分类后两个节点熵的加权平均值，值越大说明分类后数据越纯了<br><img src="/image/LLM/141.png" alt><br>这个计算方式得到的值就叫信息增益<br>即特征信息增益越大，在分类过程中，能够把数据分的越纯<br><img src="/image/LLM/142.png" alt></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>决策树学习过程<br><img src="/image/LLM/143.png" alt><br>对于每个节点，都要像对待根节点一样<br>根据拿到的数据先找到最大信息增益的特征然后进行分类<br>整个过程就是一个递归的过程，直到满足停止分类的规则为止<br><img src="/image/LLM/144.png" alt></p>
<h1 id="多特征值处理办法"><a href="#多特征值处理办法" class="headerlink" title="多特征值处理办法"></a>多特征值处理办法</h1><h2 id="one-hot"><a href="#one-hot" class="headerlink" title="one-hot"></a>one-hot</h2><p>If a categorical feature can take on 𝑘 values, create 𝑘 binary features (0 or 1 valued).<br>如果一个特征有大于2个以上的N可枚举值，那么将当前特征拆分成N个新的代表相应枚举值的特征即可<br><img src="/image/LLM/145.png" alt></p>
<h2 id="连续值"><a href="#连续值" class="headerlink" title="连续值"></a>连续值</h2><p>如果一个特质的值是连续值，不可枚举，那么需要设定一个阈值，大于该阈值一类，小于则是另一类，从而实现对该特征的二分类<br>阈值的选取还是通过计算信息增益，选取能够使信息增益值最大的阈值参与分类<br>一般情况下先对所有数据按这个特征值排序，然后选取排序列表中两个数据间的中点做阈值进行信息增益计算，<br>多轮计算后再从中选取信息增益最大的阈值<br><img src="/image/LLM/146.png" alt></p>
<h1 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h1><p>上面是使用决策树进行分类计算<br>接下来使用方差对连续数据进行推测，就是回归树<br>如下图，根据前三个特征，推测weight 的值，weight 是个连续的值，不能枚举的<br><img src="/image/LLM/147.png" alt></p>
<p><img src="/image/LLM/148.png" alt></p>
<p><a href="https://colab.research.google.com/github/kaieye/2022-Machine-Learning-Specialization/blob/main/Advanced%20Learning%20Algorithms/week4/7.Practice%20lab%20decision%20trees/C2_W4_Decision_Tree_with_Markdown.ipynb#scrollTo=5MIhOlYfSm26" target="_blank" rel="noopener">决策树练习</a></p>
<h1 id="多个决策树"><a href="#多个决策树" class="headerlink" title="多个决策树"></a>多个决策树</h1><p>单个决策树对训练数据非常敏感，只要更改一个训练数据，就有可能更改信息增益排序，<br>从而影响节点特征选择，进而导致整棵树发生变化，使得算法失去健壮性<br>解决办法就是构建多个树，让他们投票最终的预测结果，使整体的算法对任何单个树可能在做什么不那么敏感<br><img src="/image/LLM/149.png" alt></p>
<h2 id="放回抽样"><a href="#放回抽样" class="headerlink" title="放回抽样"></a>放回抽样</h2><p>一共有m个训练数据，每次从m个数据中随机抽取1个数据，直到抽取到m个数据，抽取到的数据可能是重复的<br><img src="/image/LLM/150.png" alt></p>
<h2 id="随机森林算法"><a href="#随机森林算法" class="headerlink" title="随机森林算法"></a>随机森林算法</h2><p>使用放回抽样的数据选取方法，每次拿到m个训练数据，用这个m个训练数据训练决策树，重复 B(小于100)次,<br>得到B棵决策树，从而形成决策树森林对预测结果进行投票，这种算法就是随机森林算法<br><img src="/image/LLM/151.png" alt><br>B 如果大于100 一个是训练效果会下降，推测准确性降低，另外一个就是会增加计算成本，使算法变得复杂，得不偿失</p>
<h3 id="随机特征选取"><a href="#随机特征选取" class="headerlink" title="随机特征选取"></a>随机特征选取</h3><p>对于具备N个特征的数据，通常选择N的K个特征子集进行训练，如果N特别大，K一般等于N的平方根<br><img src="/image/LLM/152.png" alt></p>
<h2 id="XGBoost-随机森林增强算法"><a href="#XGBoost-随机森林增强算法" class="headerlink" title="XGBoost 随机森林增强算法"></a>XGBoost 随机森林增强算法</h2><p>除了第一次等概率的从m个训练数据中抽样new dataSet 外，后续的每一轮抽样，都将前一轮推测失败的训练数据的权重加大，<br>使被抽取到的概率变高，尽可能的将推测失败的数据参与到后续的训练中，从而推动算法更快的学习，并学习的更好，提高算法的准确性<br><img src="/image/LLM/153.png" alt></p>
<p>这种算法又称为XGBoost 算法，是随机森林算法的改进版，优点如下<br><img src="/image/LLM/154.png" alt><br><img src="/image/LLM/155.png" alt></p>
<h1 id="决策树-VS-神经网络"><a href="#决策树-VS-神经网络" class="headerlink" title="决策树 VS 神经网络"></a>决策树 VS 神经网络</h1><p><img src="/image/LLM/156.png" alt></p>
<h2 id="决策树-amp-决策森林"><a href="#决策树-amp-决策森林" class="headerlink" title="决策树&amp; 决策森林"></a>决策树&amp; 决策森林</h2><p>适合结构化数据(可以用表格表示的数据)<br>不适合非结构化数据（音视频，图像）<br>小的决策树可以被人类解释推测过程<br>训练速度快，缩短算法迭代循环周期，更快的提高算法性能</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>对于所有数据类型都很友好，包括结构化和非结构化<br>训练速度比决策树要慢<br>但可以轻松实现迁移学习，但是决策树每次训练只能特定的特征，得到特定的决策树<br>方便构建多模型系统，神经网络间串联方便</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/psb.jpg" alt="YooHannah">
          <p class="site-author-name" itemprop="name">YooHannah</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">264</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YooHannah</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/treedocument/treedocument.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  

  


</body>
</html>
