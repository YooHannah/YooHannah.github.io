<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="My Little World" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta property="og:type" content="website">
<meta property="og:title" content="My Little World">
<meta property="og:url" content="http://yoohannah.github.io/page/4/index.html">
<meta property="og:site_name" content="My Little World">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="My Little World">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoohannah.github.io/page/4/">





  <title> My Little World </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">My Little World</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">learn and share</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/funtuning.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/funtuning.html" itemprop="url">
                  funtuning
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-08-03T15:18:37+08:00">
                2024-08-03
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>跑测试<br>重新听</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/transformer.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/transformer.html" itemprop="url">
                  transformer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-08-03T15:15:37+08:00">
                2024-08-03
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>to do<br>整理书中知识点</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/LangSmith.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/LangSmith.html" itemprop="url">
                  LangSmith
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-28T17:52:37+08:00">
                2024-07-28
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="LangSmith"><a href="#LangSmith" class="headerlink" title="LangSmith"></a>LangSmith</h2><ol>
<li><p>安装 LangSmith</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade langsmith</span><br></pre></td></tr></table></figure>
</li>
<li><p>注册账号，并申请一个 <code>LANGCHAIN_API_KEY</code></p>
</li>
<li><p>在环境变量中设置以下值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LANGCHAIN_TRACING_V2=true</span><br><span class="line">export LANGCHAIN_PROJECT=YOUR_PROJECT_NAME #自定义项目名称（可选）</span><br><span class="line">export LANGCHAIN_API_KEY=LANGCHAIN_API_KEY # LangChain API Key</span><br></pre></td></tr></table></figure>
</li>
<li><p>程序中的调用将自动被记录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from datetime import datetime</span><br><span class="line">os.environ[&quot;LANGCHAIN_TRACING_V2&quot;] = &quot;true&quot;</span><br><span class="line">os.environ[&quot;LANGCHAIN_PROJECT&quot;] = &quot;hello-world-&quot;+datetime.now().strftime(&quot;%d/%m/%Y %H:%M:%S&quot;)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h3><ol>
<li>Traces</li>
<li>LLM Calls</li>
<li>Monitor</li>
<li>Playground</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from langchain.prompts import (</span><br><span class="line">    ChatPromptTemplate,</span><br><span class="line">    HumanMessagePromptTemplate,</span><br><span class="line">)</span><br><span class="line">from langchain_core.output_parsers import StrOutputParser</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain_core.runnables import RunnablePassthrough</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    HumanMessagePromptTemplate.from_template(&quot;Say hello to &#123;input&#125;!&quot;)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 定义输出解析器</span><br><span class="line">parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line">chain = (</span><br><span class="line">    &#123;&quot;input&quot;: RunnablePassthrough()&#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | model</span><br><span class="line">    | parser</span><br><span class="line">)</span><br><span class="line">chain.invoke(&quot;王卓然&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="在线标注-在平台上进行标注"><a href="#在线标注-在平台上进行标注" class="headerlink" title="在线标注(在平台上进行标注)"></a>在线标注(在平台上进行标注)</h3><p>上传已有数据集<br>定义评估函数<br>运行测试  </p>
<h3 id="基于-LLM-的评估函数"><a href="#基于-LLM-的评估函数" class="headerlink" title="基于 LLM 的评估函数"></a>基于 LLM 的评估函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/LangFuse.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/LangFuse.html" itemprop="url">
                  Langfuse
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-28T17:17:37+08:00">
                2024-07-28
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="维护一个生产级的-LLM-应用，我们需要做什么"><a href="#维护一个生产级的-LLM-应用，我们需要做什么" class="headerlink" title="维护一个生产级的 LLM 应用，我们需要做什么"></a>维护一个生产级的 LLM 应用，我们需要做什么</h2><ol>
<li>各种指标监控与统计：访问记录、响应时长、Token 用量、计费等等</li>
<li>调试 Prompt</li>
<li>测试/验证系统的相关评估指标</li>
<li>数据集管理（便于回归测试）</li>
<li>Prompt 版本管理（便于升级/回滚）</li>
</ol>
<h2 id="针对以上需求，目前有两个生产级-LLM-App-维护平台"><a href="#针对以上需求，目前有两个生产级-LLM-App-维护平台" class="headerlink" title="针对以上需求，目前有两个生产级 LLM App 维护平台"></a>针对以上需求，目前有两个生产级 LLM App 维护平台</h2><ol>
<li>LangFuse: 开源 + SaaS（免费/付费），LangSmith 平替，可集成 LangChain 也可直接对接 OpenAI API；</li>
<li>LangSmith: LangChain 的官方平台，SaaS 服务（免费/付费），非开源，企业版支持私有部署；</li>
</ol>
<p>根据自己的技术栈，选择：</p>
<ol>
<li>LangFuse：开源平台，支持 LangChain 和原生 OpenAI API</li>
<li>LangSmith: LangChain 的原始管理平台</li>
<li>Prompt Flow：开源平台，支持 Semantic Kernel</li>
</ol>
<h2 id="LangFuse"><a href="#LangFuse" class="headerlink" title="LangFuse"></a>LangFuse</h2><p>开源，支持 LangChain 集成或原生 OpenAI API 集成</p>
<p>官方网站：<a href="https://langfuse.com/" target="_blank" rel="noopener">https://langfuse.com/</a></p>
<p>项目地址：<a href="https://github.com/langfuse" target="_blank" rel="noopener">https://github.com/langfuse</a></p>
<p>文档地址：<a href="https://langfuse.com/docs" target="_blank" rel="noopener">https://langfuse.com/docs</a></p>
<p>API文档：<a href="https://api.reference.langfuse.com/" target="_blank" rel="noopener">https://api.reference.langfuse.com/</a></p>
<ol>
<li>Python SDK:<br><a href="https://python.reference.langfuse.com/" target="_blank" rel="noopener">https://python.reference.langfuse.com/</a></li>
<li><p>JS SDK:<br><a href="https://js.reference.langfuse.com/" target="_blank" rel="noopener">https://js.reference.langfuse.com/</a></p>
</li>
<li><p>通过官方云服务使用：</p>
<ol>
<li>注册: cloud.langfuse.com</li>
<li>创建 API Key</li>
</ol>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LANGFUSE_SECRET_KEY=&quot;sk-lf-...&quot;</span><br><span class="line">LANGFUSE_PUBLIC_KEY=&quot;pk-lf-...&quot;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>通过 Docker 本地部署</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Clone repository</span><br><span class="line">git clone https://github.com/langfuse/langfuse.git</span><br><span class="line">cd langfuse</span><br><span class="line"></span><br><span class="line"># Run server and db</span><br><span class="line">docker compose up -d</span><br><span class="line"></span><br><span class="line"># 在自己部署的系统中生成上述两个 KEY</span><br><span class="line"># 并在环境变量中指定服务地址</span><br><span class="line"></span><br><span class="line">LANGFUSE_SECRET_KEY=&quot;sk-lf-...&quot;</span><br><span class="line">LANGFUSE_PUBLIC_KEY=&quot;pk-lf-..&quot;</span><br><span class="line">LANGFUSE_HOST=&quot;http://localhost:3000&quot;</span><br></pre></td></tr></table></figure>
<h2 id="几个基本概念"><a href="#几个基本概念" class="headerlink" title="几个基本概念"></a>几个基本概念</h2><ol>
<li>Trace 一般表示用户与系统的一次交互，其中记录输入、输出，也包括自定义的 metadata 比如用户名、session id 等；</li>
<li>一个 trace 内部可以包含多个子过程，这里叫 observarions；</li>
<li>Observation 可以是多个类型：<ol>
<li>Event 是最基本的单元，用于记录一个 trace 中的每个事件；</li>
<li>Span 表一个 trace 中的一个”耗时”的过程；</li>
<li>Generation 是用于记录与 AI 模型交互的 span，例如：调用 embedding 模型、调用 LLM。</li>
</ol>
</li>
<li>Observation 可以嵌套使用。</li>
</ol>
<h2 id="通过装饰器记录（上报）"><a href="#通过装饰器记录（上报）" class="headerlink" title="通过装饰器记录（上报）"></a>通过装饰器记录（上报）</h2><h3 id="observe-装饰器的参数"><a href="#observe-装饰器的参数" class="headerlink" title="observe() 装饰器的参数"></a>observe() 装饰器的参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def observe(</span><br><span class="line">    self,</span><br><span class="line">    *,</span><br><span class="line">    name: Optional[str] = None, # Trace 或 Span 的名称，默认为函数名</span><br><span class="line">    as_type: Optional[Literal[&apos;generation&apos;]] = None, # 将记录定义为 Observation (LLM 调用）</span><br><span class="line">    capture_input: bool = True, # 记录输入</span><br><span class="line">    capture_output: bool = True, # 记录输出</span><br><span class="line">    transform_to_string: Optional[Callable[[Iterable], str]] = None # 将输出转为 string</span><br><span class="line">) -&gt; Callable[[~F], ~F]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from langfuse.decorators import observe</span><br><span class="line">from langfuse.openai import openai # OpenAI integration</span><br><span class="line"></span><br><span class="line">@observe()</span><br><span class="line">def run():</span><br><span class="line">    return openai.chat.completions.create(</span><br><span class="line">        model=&quot;gpt-3.5-turbo&quot;,</span><br><span class="line">        messages=[</span><br><span class="line">          &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;对我说Hello, World!&quot;&#125;</span><br><span class="line">        ],</span><br><span class="line">    ).choices[0].message.content</span><br><span class="line"></span><br><span class="line">print(run())</span><br></pre></td></tr></table></figure>
<h3 id="通过-langfuse-context-记录-User-ID、Metadata-等"><a href="#通过-langfuse-context-记录-User-ID、Metadata-等" class="headerlink" title="通过 langfuse_context 记录 User ID、Metadata 等"></a>通过 langfuse_context 记录 User ID、Metadata 等</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from langfuse.decorators import observe, langfuse_context</span><br><span class="line">from langfuse.openai import openai # OpenAI integration</span><br><span class="line"></span><br><span class="line">@observe()</span><br><span class="line">def run():</span><br><span class="line">    langfuse_context.update_current_trace(</span><br><span class="line">        name=&quot;HelloWorld&quot;,</span><br><span class="line">        user_id=&quot;wzr&quot;,</span><br><span class="line">        metadata=&#123;&quot;test&quot;:&quot;test value&quot;&#125;</span><br><span class="line">    )</span><br><span class="line">    return openai.chat.completions.create(</span><br><span class="line">        model=&quot;gpt-3.5-turbo&quot;,</span><br><span class="line">        messages=[</span><br><span class="line">          &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;对我说Hello, World!&quot;&#125;</span><br><span class="line">        ],</span><br><span class="line">    ).choices[0].message.content</span><br><span class="line"></span><br><span class="line">print(run())</span><br></pre></td></tr></table></figure>
<h3 id="通过-LangChain-的回调集成"><a href="#通过-LangChain-的回调集成" class="headerlink" title="通过 LangChain 的回调集成"></a>通过 LangChain 的回调集成</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from langfuse.decorators import langfuse_context, observe</span><br><span class="line"></span><br><span class="line">@observe()</span><br><span class="line">def run():</span><br><span class="line">    langfuse_context.update_current_trace(</span><br><span class="line">            name=&quot;LangChainDemo&quot;,</span><br><span class="line">            user_id=&quot;wzr&quot;,</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    # 获取当前 LangChain 回调处理器</span><br><span class="line">    langfuse_handler = langfuse_context.get_current_langchain_handler()</span><br><span class="line">    </span><br><span class="line">    return chain.invoke(input=&quot;AGIClass&quot;, config=&#123;&quot;callbacks&quot;: [langfuse_handler]&#125;)</span><br><span class="line"></span><br><span class="line">print(run())</span><br></pre></td></tr></table></figure>
<h3 id="用-Trace-记录一个多次调用-LLM-的过程"><a href="#用-Trace-记录一个多次调用-LLM-的过程" class="headerlink" title="用 Trace 记录一个多次调用 LLM 的过程"></a>用 Trace 记录一个多次调用 LLM 的过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import uuid</span><br><span class="line">from langfuse.decorators import langfuse_context, observe</span><br><span class="line"></span><br><span class="line"># 主流程</span><br><span class="line">@observe()</span><br><span class="line">def verify_question(</span><br><span class="line">    question: str,</span><br><span class="line">    outlines: str,</span><br><span class="line">    question_list: list,</span><br><span class="line">    user_id: str,</span><br><span class="line">) -&gt; bool:</span><br><span class="line">    langfuse_context.update_current_trace(</span><br><span class="line">            name=&quot;AGIClassAssistant&quot;,</span><br><span class="line">            user_id=user_id,</span><br><span class="line">        )</span><br><span class="line">    langfuse_handler = langfuse_context.get_current_langchain_handler()</span><br><span class="line">    # 判断是否需要回答</span><br><span class="line">    if need_answer_chain.invoke(</span><br><span class="line">        &#123;&quot;user_input&quot;: question, &quot;outlines&quot;: outlines&#125;,</span><br><span class="line">        config=&#123;&quot;callbacks&quot;: [langfuse_handler]&#125;</span><br><span class="line">    ) == &apos;Y&apos;:</span><br><span class="line">        # 判断是否为重复问题</span><br><span class="line">        if is_duplicated_chain.invoke(</span><br><span class="line">            &#123;&quot;user_input&quot;: question,</span><br><span class="line">                &quot;question_list&quot;: &quot;\n&quot;.join(question_list)&#125;,</span><br><span class="line">            config=&#123;&quot;callbacks&quot;: [langfuse_handler]&#125;</span><br><span class="line">        ) == &apos;N&apos;:</span><br><span class="line">            question_list.append(question)</span><br><span class="line">            return True</span><br><span class="line">    return False</span><br></pre></td></tr></table></figure>
<h3 id="用-Session-记录一个用户的多轮对话"><a href="#用-Session-记录一个用户的多轮对话" class="headerlink" title="用 Session 记录一个用户的多轮对话"></a>用 Session 记录一个用户的多轮对话</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@observe()</span><br><span class="line">def chat_one_turn(user_input, user_id, turn_id):</span><br><span class="line">    langfuse_context.update_current_trace(</span><br><span class="line">        name=f&quot;ChatTurn&#123;turn_id&#125;&quot;,</span><br><span class="line">        user_id=user_id,</span><br><span class="line">        session_id=&quot;chat-&quot;+now.strftime(&quot;%d/%m/%Y %H:%M:%S&quot;)</span><br><span class="line">    )</span><br><span class="line">    langfuse_handler = langfuse_context.get_current_langchain_handler()</span><br><span class="line">    messages.append(HumanMessage(content=user_input))</span><br><span class="line">    response = llm.invoke(messages, config=&#123;&quot;callbacks&quot;: [langfuse_handler]&#125;)</span><br><span class="line">    messages.append(response)</span><br><span class="line">    return response.content</span><br></pre></td></tr></table></figure>
<h3 id="数据集与测试"><a href="#数据集与测试" class="headerlink" title="数据集与测试"></a>数据集与测试</h3><p>在线标注(在平台上进行标注)</p>
<p>上传已有数据集</p>
<p>定义评估函数</p>
<p>运行测试</p>
<p>Prompt 调优与回归测试</p>
<h3 id="Prompt-版本管理"><a href="#Prompt-版本管理" class="headerlink" title="Prompt 版本管理"></a>Prompt 版本管理</h3><p>目前只支持 Langfuse 自己的 SDK</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 按名称加载</span><br><span class="line">prompt = langfuse.get_prompt(&quot;need_answer_v1&quot;)</span><br><span class="line"></span><br><span class="line"># 按名称和版本号加载</span><br><span class="line">prompt = langfuse.get_prompt(&quot;need_answer_v1&quot;, version=2)</span><br><span class="line"></span><br><span class="line"># 对模板中的变量赋值</span><br><span class="line">compiled_prompt = prompt.compile(input=&quot;老师好&quot;, outlines=&quot;test&quot;)</span><br><span class="line"></span><br><span class="line">print(compiled_prompt)</span><br><span class="line"></span><br><span class="line"># 获取 config</span><br><span class="line"></span><br><span class="line">prompt = langfuse.get_prompt(&quot;need_answer_v1&quot;, version=5)</span><br><span class="line"></span><br><span class="line">print(prompt.config)</span><br></pre></td></tr></table></figure>
<h3 id="如何比较两个句子的相似性：一些经典-NLP-的评测方法（选）"><a href="#如何比较两个句子的相似性：一些经典-NLP-的评测方法（选）" class="headerlink" title="如何比较两个句子的相似性：一些经典 NLP 的评测方法（选）"></a>如何比较两个句子的相似性：一些经典 NLP 的评测方法（选）</h3><p><span style="color: #F33232;">用途：比较llm 返回值和预期值，从而进行打分计算</span></p>
<ol>
<li>编辑距离：也叫莱文斯坦距离(Levenshtein),是针对二个字符串的差异程度的量化量测，量测方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串。<ol>
<li>具体计算过程是一个动态规划算法：<a href="https://zhuanlan.zhihu.com/p/164599274" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/164599274</a></li>
<li>衡量两个句子的相似度时，可以以词为单位计算</li>
</ol>
</li>
<li>BLEU Score:<ol>
<li>计算输出与参照句之间的 n-gram 准确率（n=1…4）</li>
<li>对短输出做惩罚</li>
<li>在整个测试集上平均下述值</li>
<li>函数库：<a href="https://www.nltk.org/_modules/nltk/translate/bleu_score.html" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/bleu_score.html</a></li>
</ol>
</li>
<li>Rouge Score:<ol>
<li>Rouge-N：将模型生成的结果和标准结果按 N-gram 拆分后，只计算召回率；</li>
<li>Rouge-L: 利用了最长公共子序列（Longest Common Sequence）</li>
<li>函数库：<a href="https://pypi.org/project/rouge-score/" target="_blank" rel="noopener">https://pypi.org/project/rouge-score/</a></li>
<li>对比 BLEU 与 ROUGE：<ol>
<li>BLEU 能评估流畅度，但指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强）</li>
<li>ROUGE 不管流畅度，所以只适合深度学习的生成模型：结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）</li>
</ol>
</li>
</ol>
</li>
<li>METEOR: 另一个从机器翻译领域借鉴的指标。与 BLEU 相比，METEOR 考虑了更多的因素，如同义词匹配、词干匹配、词序等，因此它通常被认为是一个更全面的评价指标。<ol>
<li>对语言学和语义词表有依赖，所以对语言依赖强。</li>
</ol>
</li>
</ol>
<p><span style="background-color: #7ABF36;">此类方法常用于对文本生成模型的自动化评估。实际使用中，我们通常更关注相对变化而不是绝对值（调优过程中指标是不是在变好）</span>。</p>
<h3 id="基于-LLM-的测试方法"><a href="#基于-LLM-的测试方法" class="headerlink" title="基于 LLM 的测试方法"></a>基于 LLM 的测试方法</h3><p>LangFuse 集成了一些原生的基于 LLM 的自动测试标准。</p>
<p>具体参考：<a href="https://langfuse.com/docs/scores/model-based-evals" target="_blank" rel="noopener">https://langfuse.com/docs/scores/model-based-evals</a></p>
<p><strong>划重点：</strong>此类方法，对于用于评估的 LLM 自身能力有要求。需根据具体情况选择使用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/LangChain.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/LangChain.html" itemprop="url">
                  LangChain
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-28T12:31:37+08:00">
                2024-07-28
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>LangChain 也是一套面向大模型的开发框架（SDK）</p>
<ol>
<li>LangChain 是 AGI 时代软件工程的一个探索和原型</li>
<li>学习 LangChain 要关注接口变更</li>
</ol>
<h2 id="LangChain-的核心组件"><a href="#LangChain-的核心组件" class="headerlink" title="LangChain 的核心组件"></a>LangChain 的核心组件</h2><ol>
<li>模型 I/O 封装<ul>
<li>LLMs：大语言模型</li>
<li>Chat Models：一般基于 LLMs，但按对话结构重新封装</li>
<li>PromptTemple：提示词模板</li>
<li>OutputParser：解析输出</li>
</ul>
</li>
<li>数据连接封装<ul>
<li>Document Loaders：各种格式文件的加载器</li>
<li>Document Transformers：对文档的常用操作，如：split, filter, translate, extract metadata, etc</li>
<li>Text Embedding Models：文本向量化表示，用于检索等操作</li>
<li>Verctorstores: （面向检索的）向量的存储</li>
<li>Retrievers: 向量的检索</li>
</ul>
</li>
<li>记忆封装<ul>
<li>Memory：这里不是物理内存，从文本的角度，可以理解为”上文”、”历史记录”或者说”记忆力”的管理</li>
</ul>
</li>
<li>架构封装<ul>
<li>Chain：实现一个功能或者一系列顺序功能组合</li>
<li>Agent：根据用户输入，自动规划执行步骤，自动选择每步需要的工具，最终完成用户指定的功能<ul>
<li>Tools：调用外部功能的函数，例如：调 google 搜索、文件 I/O、Linux Shell 等等</li>
<li>Toolkits：操作某软件的一组工具集，例如：操作 DB、操作 Gmail 等等</li>
</ul>
</li>
</ul>
</li>
<li>Callbacks</li>
</ol>
<h2 id="模型-I-O-封装"><a href="#模型-I-O-封装" class="headerlink" title="模型 I/O 封装"></a>模型 I/O 封装</h2><p><span style="color: rgb(27, 94, 32); background-color: rgb(200, 230, 201);">通过模型封装，实现不同模型的统一接口调用</span></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain.schema import (</span><br><span class="line">    AIMessage,  # 等价于OpenAI接口中的assistant role</span><br><span class="line">    HumanMessage,  # 等价于OpenAI接口中的user role</span><br><span class="line">    SystemMessage  # 等价于OpenAI接口中的system role</span><br><span class="line">)</span><br><span class="line">from langchain_community.chat_models import QianfanChatEndpoint</span><br><span class="line">from langchain_core.messages import HumanMessage</span><br><span class="line"></span><br><span class="line">#  OpenAI 模型封装</span><br><span class="line">llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)  # gpt-4o 默认是gpt-3.5-turbo</span><br><span class="line">response = llm.invoke(&quot;你是谁&quot;)</span><br><span class="line">print(response.content)</span><br><span class="line"></span><br><span class="line"># 多轮对话 Session 封装</span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=&quot;你是AGIClass的课程助理。&quot;),</span><br><span class="line">    HumanMessage(content=&quot;我是学员，我叫王卓然。&quot;),</span><br><span class="line">    AIMessage(content=&quot;欢迎！&quot;),</span><br><span class="line">    HumanMessage(content=&quot;我是谁&quot;)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">ret = llm.invoke(messages)</span><br><span class="line"></span><br><span class="line">print(ret.content)</span><br><span class="line"></span><br><span class="line"># 国产模型 其它模型分装在 langchain_community 底包中</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">llm = QianfanChatEndpoint(</span><br><span class="line">    qianfan_ak=os.getenv(&apos;ERNIE_CLIENT_ID&apos;),</span><br><span class="line">    qianfan_sk=os.getenv(&apos;ERNIE_CLIENT_SECRET&apos;)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    HumanMessage(content=&quot;你是谁&quot;)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">ret = llm.invoke(messages)</span><br><span class="line"></span><br><span class="line">print(ret.content)</span><br></pre></td></tr></table></figure>
<h3 id="Prompt-模板封装"><a href="#Prompt-模板封装" class="headerlink" title="Prompt 模板封装"></a>Prompt 模板封装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain.prompts import (</span><br><span class="line">    ChatPromptTemplate,</span><br><span class="line">    HumanMessagePromptTemplate,</span><br><span class="line">    SystemMessagePromptTemplate,</span><br><span class="line">    MessagesPlaceholder,</span><br><span class="line">    PromptTemplate</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ol>
<li>PromptTemplate 可以在模板中自定义变量</li>
<li>ChatPromptTemplate 用模板表示的对话上下文</li>
<li>MessagesPlaceholder 把多轮对话变成模板</li>
<li>从文件加载 Prompt 模板: PromptTemplate.from_file</li>
</ol>
<p><span style="color: rgb(27, 94, 32); background-color: rgb(200, 230, 201);">把Prompt模板看作带有参数的函数</span></p>
<h3 id="输出封装-OutputParser"><a href="#输出封装-OutputParser" class="headerlink" title="输出封装 OutputParser"></a>输出封装 OutputParser</h3><p>自动把 LLM 输出的字符串按指定格式加载。<br>LangChain 内置的 OutputParser 包括:</p>
<ol>
<li>ListParser</li>
<li>DatetimeParser</li>
<li>EnumParser</li>
<li>JsonOutputParser</li>
<li>PydanticParser</li>
<li>XMLParser</li>
</ol>
<p>等等</p>
<h3 id="Pydantic-JSON-Parser"><a href="#Pydantic-JSON-Parser" class="headerlink" title="Pydantic (JSON) Parser"></a>Pydantic (JSON) Parser</h3><p>自动根据 Pydantic 类的定义，生成输出的格式说明</p>
<h3 id="Auto-Fixing-Parser"><a href="#Auto-Fixing-Parser" class="headerlink" title="Auto-Fixing Parser"></a>Auto-Fixing Parser</h3><p>利用 LLM 自动根据解析异常修复并重新解析</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ol>
<li>LangChain 统一封装了各种模型的调用接口，包括补全型和对话型两种</li>
<li>LangChain 提供了 PromptTemplate 类，可以自定义带变量的模板</li>
<li>LangChain 提供了一些列输出解析器，用于将大模型的输出解析成结构化对象；额外带有自动修复功能。</li>
<li>上述模型属于 LangChain 中较为优秀的部分；美中不足的是 OutputParser 自身的 Prompt 维护在代码中，耦合度较高。</li>
</ol>
<h2 id="数据连接封装"><a href="#数据连接封装" class="headerlink" title="数据连接封装"></a>数据连接封装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 文档加载器：Document Loaders</span><br><span class="line">from langchain_community.document_loaders import PyMuPDFLoader</span><br><span class="line"># 文档处理器 TextSplitter</span><br><span class="line">from langchain_text_splitters import RecursiveCharacterTextSplitter</span><br><span class="line"># 向量数据库与向量检索</span><br><span class="line">from langchain_openai import OpenAIEmbeddings</span><br><span class="line">from langchain_text_splitters import RecursiveCharacterTextSplitter</span><br><span class="line">from langchain_community.vectorstores import FAISS</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain_community.document_loaders import PyMuPDFLoader</span><br></pre></td></tr></table></figure>
<p><span style="color: rgb(0, 96, 100);">类似 LlamaIndex，LangChain 也提供了丰富的 <a href="https://python.langchain.com/v0.2/docs/how_to/#document-loaders" target="_blank" rel="noopener">Document Loaders</a> 和 <a href="https://python.langchain.com/v0.2/docs/how_to/#text-splitters" target="_blank" rel="noopener">Text Splitters</a>﻿</span></p>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><ol>
<li>文档处理部分，建议在实际应用中详细测试后使用</li>
<li>与向量数据库的链接部分本质是接口封装，向量数据库需要自己选型</li>
</ol>
<h2 id="记忆封装：Memory"><a href="#记忆封装：Memory" class="headerlink" title="记忆封装：Memory"></a>记忆封装：Memory</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 对话上下文：ConversationBufferMemory</span><br><span class="line">from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory</span><br><span class="line"></span><br><span class="line"># 只保留一个窗口的上下文：ConversationBufferWindowMemory</span><br><span class="line">from langchain.memory import ConversationBufferWindowMemory</span><br><span class="line"></span><br><span class="line"># 通过 Token 数控制上下文长度：ConversationTokenBufferMemory</span><br><span class="line">from langchain.memory import ConversationTokenBufferMemory</span><br></pre></td></tr></table></figure>
<h3 id="更多类型"><a href="#更多类型" class="headerlink" title="更多类型"></a>更多类型</h3><ol>
<li>ConversationSummaryMemory: 对上下文做摘要<ul>
<li><a href="https://python.langchain.com/docs/modules/memory/types/summary" target="_blank" rel="noopener">https://python.langchain.com/docs/modules/memory/types/summary</a>﻿</li>
</ul>
</li>
<li>ConversationSummaryBufferMemory: 保存 Token 数限制内的上下文，对更早的做摘要<ul>
<li><a href="https://python.langchain.com/docs/modules/memory/types/summary_buffer" target="_blank" rel="noopener">https://python.langchain.com/docs/modules/memory/types/summary_buffer</a>﻿</li>
</ul>
</li>
<li>VectorStoreRetrieverMemory: 将 Memory 存储在向量数据库中，根据用户输入检索回最相关的部分<ul>
<li><a href="https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory" target="_blank" rel="noopener">https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory</a>﻿</li>
</ul>
</li>
</ol>
<h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><ol>
<li>LangChain 的 Memory 管理机制属于可用的部分，尤其是简单情况如按轮数或按 Token 数管理；</li>
<li>对于复杂情况，它不一定是最优的实现，例如检索向量库方式，建议根据实际情况和效果评估；</li>
<li>但是<strong>它对内存的各种维护方法的思路在实际生产中可以借鉴</strong>。</li>
</ol>
<p>LangChain Expression Language（LCEL）是一种声明式语言，可轻松组合不同的调用顺序构成 Chain。LCEL 自创立之初就被设计为能够支持将原型投入生产环境，<strong>无需代码更改</strong>，从最简单的”提示+LLM”链到最复杂的链（已有用户成功在生产环境中运行包含数百个步骤的 LCEL Chain）。</p>
<p>LCEL 的一些亮点包括：</p>
<ol>
<li>流支持</li>
<li>异步支持</li>
<li>优化的并行执行</li>
<li>重试和回退</li>
<li>访问中间结果</li>
<li>输入和输出模式</li>
<li>无缝 LangSmith 跟踪集成</li>
<li>无缝 LangServe 部署集成</li>
</ol>
<p>原文：<a href="https://python.langchain.com/docs/expression_language/" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/</a>﻿</p>
<h3 id="Pipeline-式调用-PromptTemplate-LLM-和-OutputParser"><a href="#Pipeline-式调用-PromptTemplate-LLM-和-OutputParser" class="headerlink" title="Pipeline 式调用 PromptTemplate, LLM 和 OutputParser"></a>Pipeline 式调用 PromptTemplate, LLM 和 OutputParser</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain.prompts import ChatPromptTemplate</span><br><span class="line">from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser</span><br><span class="line">from langchain_core.runnables import RunnablePassthrough</span><br><span class="line">from langchain_core.pydantic_v1 import BaseModel, Field, validator</span><br><span class="line">from typing import List, Dict, Optional</span><br><span class="line">from enum import Enum</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"># LCEL 表达式</span><br><span class="line">runnable = (</span><br><span class="line">    &#123;&quot;text&quot;: RunnablePassthrough()&#125; | prompt | model | parser</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 直接运行</span><br><span class="line">ret = runnable.invoke(&quot;不超过100元的流量大的套餐有哪些&quot;)</span><br><span class="line"></span><br><span class="line"># 流式输出</span><br><span class="line">for s in runnable.stream(&quot;不超过100元的流量大的套餐有哪些&quot;):</span><br><span class="line">    print(s, end=&quot;&quot;)</span><br></pre></td></tr></table></figure>
<p><strong><span style="color: rgb(27, 94, 32);">使用 LCEL 的价值，也就是 LangChain 的核心价值。</span></strong></p>
<p><span style="color: rgb(27, 94, 32); background-color: rgb(200, 230, 201);">官方从不同角度给出了举例说明：</span><a href="https://python.langchain.com/docs/expression_language/why" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/why</a>﻿</p>
<h3 id="通过-LCEL，还可以实现"><a href="#通过-LCEL，还可以实现" class="headerlink" title="通过 LCEL，还可以实现"></a>通过 LCEL，还可以实现</h3><ol>
<li>配置运行时变量：<a href="https://python.langchain.com/docs/expression_language/how_to/configure" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/how_to/configure</a></li>
<li>故障回退：<a href="https://python.langchain.com/docs/expression_language/how_to/fallbacks" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/how_to/fallbacks</a></li>
<li>并行调用：<a href="https://python.langchain.com/docs/expression_language/how_to/map" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/how_to/map</a></li>
<li>逻辑分支：<a href="https://python.langchain.com/docs/expression_language/how_to/routing" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/how_to/routing</a></li>
<li>调用自定义流式函数：<a href="https://python.langchain.com/docs/expression_language/how_to/generators" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/how_to/generators</a></li>
<li>链接外部 Memory：<a href="https://python.langchain.com/docs/expression_language/how_to/message_history" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/how_to/message_history</a></li>
</ol>
<p>更多例子：<a href="https://python.langchain.com/docs/expression_language/cookbook/" target="_blank" rel="noopener">https://python.langchain.com/docs/expression_language/cookbook/</a>﻿</p>
<h3 id="什么是智能体（Agent）"><a href="#什么是智能体（Agent）" class="headerlink" title="什么是智能体（Agent）"></a>什么是智能体（Agent）</h3><p>将大语言模型作为一个推理引擎, 给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。</p>
<h3 id="先定义一些工具：Tools"><a href="#先定义一些工具：Tools" class="headerlink" title="先定义一些工具：Tools"></a>先定义一些工具：Tools</h3><ol>
<li>可以是一个函数或三方 API</li>
<li>也可以把一个 Chain 或者 Agent 的 run()作为一个 Tool</li>
</ol>
<h2 id="下载一个现有的-Prompt-模板"><a href="#下载一个现有的-Prompt-模板" class="headerlink" title="下载一个现有的 Prompt 模板"></a>下载一个现有的 Prompt 模板</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">react_prompt = hub.pull(&quot;hwchase17/react&quot;)</span><br><span class="line">print(react_prompt.template)</span><br></pre></td></tr></table></figure>
<h2 id="直接定义执行执行调用"><a href="#直接定义执行执行调用" class="headerlink" title="直接定义执行执行调用"></a>直接定义执行执行调用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain.agents import AgentExecutor, create_react_agent</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(model_name=&apos;gpt-4o&apos;, temperature=0, model_kwargs=&#123;&quot;seed&quot;:23&#125;)</span><br><span class="line"></span><br><span class="line"># 定义一个 agent: 需要大模型、工具集、和 Prompt 模板</span><br><span class="line">agent = create_react_agent(llm, tools, react_prompt)</span><br><span class="line"># 定义一个执行器：需要 agent 对象 和 工具集</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)</span><br><span class="line"></span><br><span class="line"># 执行</span><br><span class="line">agent_executor.invoke(&#123;&quot;input&quot;: &quot;2024年周杰伦的演唱会星期几&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="LangServe"><a href="#LangServe" class="headerlink" title="LangServe"></a>LangServe</h2><p>LangServe 用于将 Chain 或者 Runnable 部署成一个 REST API 服务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装 LangServe</span><br><span class="line"># !pip install --upgrade &quot;langserve[all]&quot;</span><br><span class="line"></span><br><span class="line"># 也可以只安装一端</span><br><span class="line"># !pip install &quot;langserve[client]&quot;</span><br><span class="line"># !pip install &quot;langserve[server]&quot;</span><br></pre></td></tr></table></figure>
<h2 id="LangChain-js"><a href="#LangChain-js" class="headerlink" title="LangChain.js"></a>LangChain.js</h2><p>Python 版 LangChain 的姊妹项目，都是由 Harrison Chase 主理。</p>
<p>项目地址：<a href="https://github.com/langchain-ai/langchainjs" target="_blank" rel="noopener">https://github.com/langchain-ai/langchainjs</a></p>
<p>文档地址：<a href="https://js.langchain.com/docs/" target="_blank" rel="noopener">https://js.langchain.com/docs/</a></p>
<p>特色：</p>
<ol>
<li>可以和 Python 版 LangChain 无缝对接</li>
<li>抽象设计完全相同，概念一一对应</li>
<li>所有对象序列化后都能跨语言使用，但 API 差别挺大，不过在努力对齐</li>
</ol>
<p>支持环境：</p>
<ol>
<li>Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x</li>
<li>Cloudflare Workers</li>
<li>Vercel / Next.js (Browser, Serverless and Edge functions)</li>
<li>Supabase Edge Functions</li>
<li>Browser</li>
<li>Deno</li>
</ol>
<p>安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install langchain</span><br></pre></td></tr></table></figure>
<p>当前重点：</p>
<ol>
<li>追上 Python 版的能力（甚至为此做了一个基于 gpt-3.5-turbo 的代码翻译器）</li>
<li>保持兼容尽可能多的环境</li>
<li>对质量关注不多，随时间自然能解决</li>
</ol>
<h3 id="LangChain-与-LlamaIndex-的错位竞争"><a href="#LangChain-与-LlamaIndex-的错位竞争" class="headerlink" title="LangChain 与 LlamaIndex 的错位竞争"></a>LangChain 与 LlamaIndex 的错位竞争</h3><ol>
<li>LangChain 侧重与 LLM 本身交互的封装<ul>
<li>Prompt、LLM、Memory、OutputParser 等工具丰富</li>
<li>在数据处理和 RAG 方面提供的工具相对粗糙</li>
<li>主打 LCEL 流程封装</li>
<li>配套 Agent、LangGraph 等智能体与工作流工具</li>
<li>另有 LangServe 部署工具和 LangSmith 监控调试工具</li>
</ul>
</li>
<li>LlamaIndex 侧重与数据交互的封装<ul>
<li>数据加载、切割、索引、检索、排序等相关工具丰富</li>
<li>Prompt、LLM 等底层封装相对单薄</li>
<li>配套实现 RAG 相关工具</li>
<li>有 Agent 相关工具，不突出</li>
</ul>
</li>
<li>LlamaIndex 为 LangChain 提供了集成<ul>
<li>在 LlamaIndex 中调用 LangChain 封装的 LLM 接口：<a href="https://docs.llamaindex.ai/en/stable/api_reference/llms/langchain/" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/stable/api_reference/llms/langchain/</a></li>
<li>将 LlamaIndex 的 Query Engine 作为 LangChain Agent 的工具：<a href="https://docs.llamaindex.ai/en/v0.10.17/community/integrations/using_with_langchain.html" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/v0.10.17/community/integrations/using_with_langchain.html</a></li>
<li>LangChain 也<em>曾经</em>集成过 LlamaIndex，目前相关接口仍在：<a href="https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.llama_index.LlamaIndexRetriever.html" target="_blank" rel="noopener">https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.llama_index.LlamaIndexRetriever.html</a></li>
</ul>
</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/LlamaIndex.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/LlamaIndex.html" itemprop="url">
                  llamaindex
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-28T10:57:37+08:00">
                2024-07-28
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="LlamaIndex-简介"><a href="#LlamaIndex-简介" class="headerlink" title="LlamaIndex 简介"></a>LlamaIndex 简介</h1><p>LlamaIndex 是一个为开发「<strong>上下文增强</strong>」的大语言模型应用的框架（也就是 SDK）。<strong>上下文增强</strong>，泛指任何在私有或特定领域数据基础上应用大语言模型的情况。例如：</p>
<ol>
<li>Question-Answering Chatbots (也就是 RAG)</li>
<li>Document Understanding and Extraction （文档理解与信息抽取）</li>
<li>Autonomous Agents that can perform research and take actions （智能体应用）</li>
</ol>
<p>LlamaIndex 有 Python 和 Typescript 两个版本，Python 版的文档相对更完善。</p>
<ol>
<li>Python 文档地址：<br><a href="https://docs.llamaindex.ai/en/stable/" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/stable/</a></li>
<li>Python API 接口文档：<br><a href="https://docs.llamaindex.ai/en/stable/api_reference/" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/stable/api_reference/</a></li>
<li>TS 文档地址：<br><a href="https://ts.llamaindex.ai/" target="_blank" rel="noopener">https://ts.llamaindex.ai/</a></li>
<li>TS API 接口文档：<br><a href="https://ts.llamaindex.ai/api/" target="_blank" rel="noopener">https://ts.llamaindex.ai/api/</a></li>
</ol>
<p>LlamaIndex 是一个开源框架，Github 链接：<br><a href="https://github.com/run-llama" target="_blank" rel="noopener">https://github.com/run-llama</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-index</span><br></pre></td></tr></table></figure>
<h1 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h1><h2 id="加载本地数据"><a href="#加载本地数据" class="headerlink" title="加载本地数据"></a>加载本地数据</h2><p><code>SimpleDirectoryReader</code> 是一个简单的本地文件加载器。它会遍历指定目录，并根据文件扩展名自动加载文件（<strong>文本内容</strong>）。</p>
<p>默认的 <code>PDFReader</code> 效果并不理想，我们可以更换文件加载器：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pymupdf</span><br></pre></td></tr></table></figure>
<p>更多的 PDF 加载器还有 <a href="https://llamahub.ai/l/readers/llama-index-readers-smart-pdf-loader?from=readers" target="_blank" rel="noopener">SmartPDFLoader</a> 和 <a href="https://llamahub.ai/l/readers/llama-index-readers-llama-parse?from=readers" target="_blank" rel="noopener">LlamaParse</a>, 二者都提供了更丰富的解析能力，包括解析章节与段落结构等。但不是 100%准确，偶有文字丢失或错位情况，建议根据自身需求详细测试评估。</p>
<h2 id="Data-Connectors"><a href="#Data-Connectors" class="headerlink" title="Data Connectors"></a>Data Connectors</h2><p>对图像、视频、语音类文件，默认不会自动提取其中文字。如需提取, 需要对应读取器。</p>
<p>处理更丰富的数据类型，并将其读取为 <code>Document</code> 的形式（text + metadata）。</p>
<ol>
<li>比如 加载飞书文档 pip install llama-index-readers-feishu-docs</li>
<li>内置的<a href="https://llamahub.ai/l/readers/llama-index-readers-file" target="_blank" rel="noopener">文件加载器</a></li>
<li>连接三方服务的<a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/" target="_blank" rel="noopener">数据加载器</a></li>
<li>更多加载器可以在 <a href="https://llamahub.ai/" target="_blank" rel="noopener">LlamaHub</a> 上找到</li>
</ol>
<h1 id="文本切分与解析（Chunking）"><a href="#文本切分与解析（Chunking）" class="headerlink" title="文本切分与解析（Chunking）"></a>文本切分与解析（Chunking）</h1><p>LlamaIndex 中，<code>Node</code> 被定义为一个文本的「chunk」。</p>
<h2 id="使用-TextSplitters-对文本做切分"><a href="#使用-TextSplitters-对文本做切分" class="headerlink" title="使用 TextSplitters 对文本做切分"></a>使用 TextSplitters 对文本做切分</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from llama_index.core.node_parser import TokenTextSplitter</span><br><span class="line">from llama_index.core import Document</span><br><span class="line">from llama_index.core.node_parser import TokenTextSplitter</span><br><span class="line"></span><br><span class="line">node_parser = TokenTextSplitter(</span><br><span class="line">    chunk_size=100,  # 每个 chunk 的最大长度</span><br><span class="line">    chunk_overlap=50  # chunk 之间重叠长度 </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">nodes = node_parser.get_nodes_from_documents(</span><br><span class="line">    documents, show_progress=False</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">show_json(nodes[0])</span><br></pre></td></tr></table></figure>
<p>LlamaIndex 提供了丰富的 <code>TextSplitter</code>，例如：</p>
<ol>
<li>SentenceSplitter: 在切分指定长度的 chunk 同时尽量保证句子边界不被切断；</li>
<li>CodeSplitter: 根据 AST（编译器的抽象句法树）切分代码，保证代码功能片段完整；</li>
<li>SemanticSplitterNodeParser: 根据语义相关性对将文本切分为片段</li>
</ol>
<h2 id="使用-NodeParsers-对有结构的文档做解析"><a href="#使用-NodeParsers-对有结构的文档做解析" class="headerlink" title="使用 NodeParsers 对有结构的文档做解析"></a>使用 NodeParsers 对有结构的文档做解析</h2><p>更多的 <code>NodeParser</code> 包括 <a href="https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/html/" target="_blank" rel="noopener">HTMLNodeParser</a>，<a href="https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/json/" target="_blank" rel="noopener">JSONNodeParser</a>等等。</p>
<h1 id="索引（Indexing）与检索（Retrieval）"><a href="#索引（Indexing）与检索（Retrieval）" class="headerlink" title="索引（Indexing）与检索（Retrieval）"></a>索引（Indexing）与检索（Retrieval）</h1><p><strong>基础概念</strong>：在「检索」相关的上下文中，「索引」即 <code>index</code>， 通常是指为了实现快速检索而设计的特定「数据结构」。</p>
<p><a href="https://en.wikipedia.org/wiki/Search_engine_indexing" target="_blank" rel="noopener">传统索引</a>、<a href="https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5" target="_blank" rel="noopener">向量索引</a></p>
<h2 id="向量检索"><a href="#向量检索" class="headerlink" title="向量检索"></a>向量检索</h2><ol>
<li>SimpleVectorStore 直接在内存中构建一个 Vector Store 并建索引</li>
</ol>
<p>LlamaIndex 默认的 Embedding 模型是 <code>OpenAIEmbedding(model=&quot;text-embedding-ada-002&quot;)</code>。</p>
<ol start="2">
<li>使用自定义的 Vector Store，以 <code>Chroma</code> 为例：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-index-vector-stores-chroma</span><br></pre></td></tr></table></figure>
<h3 id="更多索引与检索方式"><a href="#更多索引与检索方式" class="headerlink" title="更多索引与检索方式"></a>更多索引与检索方式</h3><p>LlamaIndex 内置了丰富的检索机制，例如：</p>
<p>关键字检索</p>
<ol>
<li><a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/" target="_blank" rel="noopener">BM25Retriever</a>：基于 tokenizer 实现的 BM25 经典检索算</li>
<li><a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableGPTRetriever" target="_blank" rel="noopener">KeywordTableGPTRetriever</a>：使用 GPT 提取检索关键字</li>
<li><a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableSimpleRetriever" target="_blank" rel="noopener">KeywordTableSimpleRetriever</a>：使用正则表达式提取检索关键字</li>
<li><a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableRAKERetriever" target="_blank" rel="noopener">KeywordTableRAKERetriever</a>：使用<a href="https://pypi.org/project/rake-nltk/" target="_blank" rel="noopener">RAKE</a>算法提取检索关键字（有语言限制）</li>
<li>RAG-Fusion <a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/query_fusion/" target="_blank" rel="noopener">QueryFusionRetriever</a>﻿</li>
</ol>
<p>还支持 <a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/" target="_blank" rel="noopener">KnowledgeGraph</a>、<a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever" target="_blank" rel="noopener">SQL</a>、<a href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever" target="_blank" rel="noopener">Text-to-SQL</a> 等等</p>
<h3 id="Ingestion-Pipeline-自定义数据处理流程"><a href="#Ingestion-Pipeline-自定义数据处理流程" class="headerlink" title="Ingestion Pipeline 自定义数据处理流程"></a>Ingestion Pipeline 自定义数据处理流程</h3><p>LlamaIndex 通过 <code>Transformations</code> 定义一个数据（<code>Documents</code>）的多步处理的流程（Pipeline）。 </p>
<p>这个 Pipeline 的一个显著特点是，<strong>它的每个子步骤是可以缓存（cache）的</strong>，即如果该子步骤的输入与处理方法不变，重复调用时会直接从缓存中获取结果，而无需重新执行该子步骤，这样即节省时间也会节省 token （如果子步骤涉及大模型调用）。</p>
<p>此外，也可以用远程的 Redis 或 MongoDB 等存储 <code>IngestionPipeline</code> 的缓存，具体参考官方文档：<a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#remote-cache-management" target="_blank" rel="noopener">Remote Cache Management</a>。</p>
<p><code>IngestionPipeline</code> 也支持异步和并发调用，请参考官方文档：<a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#async-support" target="_blank" rel="noopener">Async Support</a>、<a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#parallel-processing" target="_blank" rel="noopener">Parallel Processing</a>。</p>
<h2 id="检索后处理"><a href="#检索后处理" class="headerlink" title="检索后处理"></a>检索后处理</h2><p>LlamaIndex 的 <code>Node Postprocessors</code> 提供了一系列检索后处理模块。</p>
<p>更多的 Rerank 及其它后处理方法，参考官方文档：<a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/" target="_blank" rel="noopener">Node Postprocessor Modules</a>﻿</p>
<h1 id="生成回复（QA-amp-Chat）"><a href="#生成回复（QA-amp-Chat）" class="headerlink" title="生成回复（QA &amp; Chat）"></a>生成回复（QA &amp; Chat）</h1><h2 id="单轮问答（Query-Engine）"><a href="#单轮问答（Query-Engine）" class="headerlink" title="单轮问答（Query Engine）"></a>单轮问答（Query Engine）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qa_engine = index.as_query_engine()</span><br><span class="line">response = qa_engine.query(&quot;Llama2 有多少参数?&quot;)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>
<h3 id="流式输出"><a href="#流式输出" class="headerlink" title="流式输出"></a>流式输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qa_engine = index.as_query_engine(streaming=True)</span><br><span class="line">response = qa_engine.query(&quot;Llama2 有多少参数?&quot;)</span><br><span class="line">response.print_response_stream()</span><br></pre></td></tr></table></figure>
<h2 id="多轮对话（Chat-Engine）"><a href="#多轮对话（Chat-Engine）" class="headerlink" title="多轮对话（Chat Engine）"></a>多轮对话（Chat Engine）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">chat_engine = index.as_chat_engine()</span><br><span class="line">response = chat_engine.chat(&quot;Llama2 有多少参数?&quot;)</span><br><span class="line">print(response)</span><br><span class="line"></span><br><span class="line">response = chat_engine.chat(&quot;How many at most?&quot;)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>
<h3 id="流式输出-1"><a href="#流式输出-1" class="headerlink" title="流式输出"></a>流式输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chat_engine = index.as_chat_engine()</span><br><span class="line">streaming_response = chat_engine.stream_chat(&quot;Llama 2有多少参数?&quot;)</span><br><span class="line">for token in streaming_response.response_gen:</span><br><span class="line">    print(token, end=&quot;&quot;)</span><br></pre></td></tr></table></figure>
<h1 id="底层接口：Prompt、LLM-与-Embedding"><a href="#底层接口：Prompt、LLM-与-Embedding" class="headerlink" title="底层接口：Prompt、LLM 与 Embedding"></a>底层接口：Prompt、LLM 与 Embedding</h1><h2 id="Prompt-模板"><a href="#Prompt-模板" class="headerlink" title="Prompt 模板"></a>Prompt 模板</h2><p><code>PromptTemplate</code> 定义提示词模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prompt = PromptTemplate(&quot;写一个关于&#123;topic&#125;的笑话&quot;)</span><br><span class="line"></span><br><span class="line">prompt.format(topic=&quot;小明&quot;)</span><br></pre></td></tr></table></figure>
<p><code>ChatPromptTemplate</code> 定义多轮消息模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from llama_index.core.llms import ChatMessage, MessageRole</span><br><span class="line">from llama_index.core import ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">chat_text_qa_msgs = [</span><br><span class="line">    ChatMessage(</span><br><span class="line">        role=MessageRole.SYSTEM,</span><br><span class="line">        content=&quot;你叫&#123;name&#125;，你必须根据用户提供的上下文回答问题。&quot;,</span><br><span class="line">    ),</span><br><span class="line">    ChatMessage(</span><br><span class="line">        role=MessageRole.USER, </span><br><span class="line">        content=(</span><br><span class="line">            &quot;已知上下文：\n&quot; \</span><br><span class="line">            &quot;&#123;context&#125;\n\n&quot; \</span><br><span class="line">            &quot;问题：&#123;question&#125;&quot;</span><br><span class="line">        )</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)</span><br><span class="line"></span><br><span class="line">print(</span><br><span class="line">    text_qa_template.format(</span><br><span class="line">        name=&quot;瓜瓜&quot;,</span><br><span class="line">        context=&quot;这是一个测试&quot;,</span><br><span class="line">        question=&quot;这是什么&quot;</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from llama_index.llms.openai import OpenAI</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=0, model=&quot;gpt-4o&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="设置全局使用的语言模型"><a href="#设置全局使用的语言模型" class="headerlink" title="设置全局使用的语言模型"></a>设置全局使用的语言模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from llama_index.core import Settings</span><br><span class="line">Settings.llm = OpenAI(temperature=0, model=&quot;gpt-4o&quot;)</span><br></pre></td></tr></table></figure>
<p>除 OpenAI 外，LlamaIndex 已集成多个大语言模型，包括云服务 API 和本地部署 API，详见官方文档：<a href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/" target="_blank" rel="noopener">Available LLM integrations</a>﻿</p>
<h2 id="Embedding-模型"><a href="#Embedding-模型" class="headerlink" title="Embedding 模型"></a>Embedding 模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from llama_index.embeddings.openai import OpenAIEmbedding</span><br><span class="line">from llama_index.core import Settings</span><br></pre></td></tr></table></figure>
<p><em>全局设定</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Settings.embed_model = OpenAIEmbedding(model=&quot;text-embedding-3-small&quot;, dimensions=512)</span><br></pre></td></tr></table></figure>
<p>LlamaIndex 同样集成了多种 Embedding 模型，包括云服务 API 和开源模型（HuggingFace）等，详见<a href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/" target="_blank" rel="noopener">官方文档</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于 LlamaIndex 实现一个功能较完整的 RAG 系统</span><br></pre></td></tr></table></figure>
<h1 id="LlamaIndex-的更多功能"><a href="#LlamaIndex-的更多功能" class="headerlink" title="LlamaIndex 的更多功能"></a>LlamaIndex 的更多功能</h1><ol>
<li>智能体（Agent）开发框架：<br><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/</a></li>
<li>RAG 的评测：<br><a href="https://docs.llamaindex.ai/en/stable/module_guides/evaluating/" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/stable/module_guides/evaluating/</a></li>
<li>过程监控：<br><a href="https://docs.llamaindex.ai/en/stable/module_guides/observability/" target="_blank" rel="noopener">https://docs.llamaindex.ai/en/stable/module_guides/observability/</a></li>
</ol>
<p>以上内容涉及较多背景知识，暂时不在本课展开，相关知识会在后面课程中逐一详细讲解。</p>
<p>此外，LlamaIndex 针对生产级的 RAG 系统中遇到的各个方面的细节问题，总结了很多高端技巧（<a href="https://docs.llamaindex.ai/en/stable/optimizing/production_rag/" target="_blank" rel="noopener">Advanced Topics</a>），对实战很有参考价值，非常推荐有能力的同学阅读。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/AssistantsAPI.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/AssistantsAPI.html" itemprop="url">
                  AssistantsAPI
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-13T10:43:37+08:00">
                2024-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Assistants-API"><a href="#Assistants-API" class="headerlink" title="Assistants API"></a>Assistants API</h2><h3 id="https-platform-openai-com-docs-assistants-overview"><a href="#https-platform-openai-com-docs-assistants-overview" class="headerlink" title="https://platform.openai.com/docs/assistants/overview"></a><a href="https://platform.openai.com/docs/assistants/overview" target="_blank" rel="noopener">https://platform.openai.com/docs/assistants/overview</a></h3><p><img src="/image/llmCourse/assis1.png" alt></p>
<h3 id="Assistants-API-的主要能力"><a href="#Assistants-API-的主要能力" class="headerlink" title="Assistants API 的主要能力"></a>Assistants API 的主要能力</h3><p>已有能力：</p>
<ol>
<li>创建和管理 assistant，每个 assistant 有独立的配置</li>
<li>支持无限长的多轮对话，对话历史保存在 OpenAI 的服务器上</li>
<li>通过自有向量数据库支持基于文件的 RAG</li>
<li>支持 Code Interpreter<br>a. 在沙箱里编写并运行 Python 代码<br>b. 自我修正代码<br>c. 可传文件给 Code Interpreter</li>
<li>支持 Function Calling</li>
<li>支持在线调试的 Playground</li>
</ol>
<p>承诺未来会有的能力：</p>
<ol>
<li>支持 DALL·E</li>
<li>支持图片消息</li>
<li>支持自定义调整 RAG 的配置项</li>
</ol>
<p>收费：</p>
<ol>
<li>按 token 收费。无论多轮对话，还是 RAG，所有都按实际消耗的 token 收费</li>
<li>如果对话历史过多超过大模型上下文窗口，会自动放弃最老的对话消息</li>
<li>文件按数据大小和存放时长收费。1 GB <span style="font-weight:bold">向量存储</span> 一天收费 0.10 美元</li>
<li>Code interpreter 跑一次 $0.03</li>
</ol>
<p><span style="color:#1b5e20">划重点：</span>使用 assistant 的意义之一，是可以隔离不同角色的 instruction 和 function 能力。<br>可以为每个应用，甚至应用中的每个有对话历史的使用场景，创建一个 assistant。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">assistant = client.beta.assistants.create(</span><br><span class="line">    name=&quot;Demo test&quot;,</span><br><span class="line">    instructions=&quot;你叫xxxx, 你负责xxxx&quot;,</span><br><span class="line">    model=&quot;gpt-4o&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="管理-thread"><a href="#管理-thread" class="headerlink" title="管理 thread"></a>管理 thread</h3><p>Threads：</p>
<ol>
<li>Threads 里保存的是对话历史，即 messages</li>
<li>一个 assistant 可以有多个 thread</li>
<li>一个 thread 可以有无限条 message</li>
<li>一个用户与 assistant 的多轮对话历史可以维护在一个 thread 里</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 可以根据需要，自定义 `metadata`，比如创建 thread 时，把 thread 归属的用户信息存入。也可以不传</span><br><span class="line">thread = client.beta.threads.create(</span><br><span class="line">    metadata=&#123;&quot;fullname&quot;: &quot;xxx&quot;, &quot;username&quot;: &quot;hhh&quot;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">===&gt;&#123;</span><br><span class="line">    &quot;id&quot;: &quot;thread_ZO9PbLBA4sHJ1xrnKwhprusi&quot;,</span><br><span class="line">    &quot;created_at&quot;: 1718986823,</span><br><span class="line">    &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;fullname&quot;: &quot;xxx&quot;,</span><br><span class="line">        &quot;username&quot;: &quot;hhh&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;object&quot;: &quot;thread&quot;,</span><br><span class="line">    &quot;tool_resources&quot;: &#123;</span><br><span class="line">        &quot;code_interpreter&quot;: &#123;</span><br><span class="line">            &quot;file_ids&quot;: []</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;file_search&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Thread ID 如果保存下来，是可以在下次运行时继续对话的。<br>从 thread ID 获取 thread 对象的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thread = client.beta.threads.retrieve(thread.id)</span><br></pre></td></tr></table></figure>
<p>此外，还有：</p>
<ol>
<li>threads.modify() 修改 thread 的 metadata和tool_resources</li>
<li>threads.retrieve() 获取 thread</li>
<li>threads.delete() 删除 thread。</li>
</ol>
<p>具体文档参考：<a href="https://platform.openai.com/docs/api-reference/threads" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/threads</a></p>
<h3 id="给-Threads-添加-Messages"><a href="#给-Threads-添加-Messages" class="headerlink" title="给 Threads 添加 Messages"></a>给 Threads 添加 Messages</h3><p>这里的 messages 结构要复杂一些：</p>
<ol>
<li>不仅有文本，还可以有图片和文件</li>
<li>也有metadata</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">message = client.beta.threads.messages.create(</span><br><span class="line">    thread_id=thread.id,  # message 必须归属于一个 thread</span><br><span class="line">    role=&quot;user&quot;,          # 取值是 user 或者 assistant。但 assistant 消息会被自动加入，我们一般不需要自己构造</span><br><span class="line">    content=&quot;你都能做什么？&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">===&gt;&#123;</span><br><span class="line">    &quot;id&quot;: &quot;msg_P2lX3QL5arxOO8tg4JoAIrhb&quot;,</span><br><span class="line">    &quot;assistant_id&quot;: null,</span><br><span class="line">    &quot;attachments&quot;: [],</span><br><span class="line">    &quot;completed_at&quot;: null,</span><br><span class="line">    &quot;content&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;text&quot;: &#123;</span><br><span class="line">                &quot;annotations&quot;: [],</span><br><span class="line">                &quot;value&quot;: &quot;你都能做什么？&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;type&quot;: &quot;text&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;created_at&quot;: 1718887069,</span><br><span class="line">    &quot;incomplete_at&quot;: null,</span><br><span class="line">    &quot;incomplete_details&quot;: null,</span><br><span class="line">    &quot;metadata&quot;: &#123;&#125;,</span><br><span class="line">    &quot;object&quot;: &quot;thread.message&quot;,</span><br><span class="line">    &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">    &quot;run_id&quot;: null,</span><br><span class="line">    &quot;status&quot;: null,</span><br><span class="line">    &quot;thread_id&quot;: &quot;thread_ZO9PbLBA4sHJ1xrnKwhprusi&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>还有如下函数：</p>
<ol>
<li>threads.messages.retrieve() 获取 message</li>
<li>threads.messages.update() 更新 message 的 metadata</li>
<li>threads.messages.list() 列出给定 thread 下的所有 messages</li>
</ol>
<p>具体文档参考：<a href="https://platform.openai.com/docs/api-reference/messages" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/messages</a></p>
<p>也可以在创建 thread 同时初始化一个 message 列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">thread = client.beta.threads.create(</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">            &quot;content&quot;: &quot;你好&quot;,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;role&quot;: &quot;assistant&quot;,</span><br><span class="line">            &quot;content&quot;: &quot;有什么可以帮您？&quot;,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">            &quot;content&quot;: &quot;你是谁？&quot;,</span><br><span class="line">        &#125;,</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="开始-Run"><a href="#开始-Run" class="headerlink" title="开始 Run"></a>开始 Run</h3><ol>
<li>用 run 把 assistant 和 thread 关联，进行对话</li>
<li>一个 prompt 就是一次 run</li>
</ol>
<p>（执行一次run, 如果run 入参的thread 里面携带(有绑定)message 就相当于向LLM 进行一次提问）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">assistant_id = &quot;asst_ahXpE6toS71zFyq9h4iMIDj2&quot;  # 从 Playground 中拷贝</span><br><span class="line"></span><br><span class="line">run = client.beta.threads.runs.create_and_poll(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    assistant_id=assistant_id,</span><br><span class="line">)</span><br><span class="line">if run.status == &apos;completed&apos;:</span><br><span class="line">    messages = client.beta.threads.messages.list(</span><br><span class="line">        thread_id=thread.id</span><br><span class="line">    )</span><br><span class="line">    show_json(messages)</span><br><span class="line">else:</span><br><span class="line">    print(run.status)</span><br></pre></td></tr></table></figure>
<p>Run 的底层是个异步调用，意味着它不等大模型处理完，就返回。我们通过 <span style="color: rgb(0, 0, 0);">run.status</span>了解大模型的工作进展情况，来判断下一步该干什么。</p>
<p><span style="color: rgb(0, 0, 0);">run.status</span> 有的状态，和状态之间的转移关系如图。</p>
<p><img src="/image/llmCourse/assis2.png" alt></p>
<h3 id="流式运行"><a href="#流式运行" class="headerlink" title="流式运行"></a>流式运行</h3><ol>
<li>创建回调函数</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from typing_extensions import override</span><br><span class="line">from openai import AssistantEventHandler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class EventHandler(AssistantEventHandler):</span><br><span class="line">    @override</span><br><span class="line">    def on_text_created(self, text) -&gt; None:</span><br><span class="line">        &quot;&quot;&quot;响应输出创建事件&quot;&quot;&quot;</span><br><span class="line">        print(f&quot;\nassistant &gt; &quot;, end=&quot;&quot;, flush=True)</span><br><span class="line"></span><br><span class="line">    @override</span><br><span class="line">    def on_text_delta(self, delta, snapshot):</span><br><span class="line">        &quot;&quot;&quot;响应输出生成的流片段&quot;&quot;&quot;</span><br><span class="line">        print(delta.value, end=&quot;&quot;, flush=True)</span><br></pre></td></tr></table></figure>
<p><span style="color: rgb(0, 96, 100);">更多流中的 Event：</span> <a href="https://platform.openai.com/docs/api-reference/assistants-streaming/events" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/assistants-streaming/events</a></p>
<ol start="2">
<li>运行 run</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 添加新一轮的 user message</span><br><span class="line">message = client.beta.threads.messages.create(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    role=&quot;user&quot;,</span><br><span class="line">    content=&quot;你说什么？&quot;,</span><br><span class="line">)</span><br><span class="line"># 使用 stream 接口并传入 EventHandler</span><br><span class="line">with client.beta.threads.runs.stream(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    assistant_id=assistant_id,</span><br><span class="line">    event_handler=EventHandler(),</span><br><span class="line">) as stream:</span><br><span class="line">    stream.until_done()</span><br></pre></td></tr></table></figure>
<p>还有如下函数：</p>
<ol>
<li>threads.runs.list() 列出 thread 归属的 run</li>
<li>threads.runs.retrieve() 获取 run</li>
<li>threads.runs.update() 修改 run 的 metadata</li>
<li>threads.runs.cancel() 取消 <span style="color: rgb(0, 0, 0);">in_progress</span> 状态的 run</li>
</ol>
<p>具体文档参考：<a href="https://platform.openai.com/docs/api-reference/runs" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/runs</a></p>
<h2 id="使用-Tools"><a href="#使用-Tools" class="headerlink" title="使用 Tools"></a>使用 Tools</h2><h3 id="创建-Assistant-时声明-Code-Interpreter"><a href="#创建-Assistant-时声明-Code-Interpreter" class="headerlink" title="创建 Assistant 时声明 Code_Interpreter"></a>创建 Assistant 时声明 Code_Interpreter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">如果用代码创建：</span><br><span class="line">assistant = client.beta.assistants.create(</span><br><span class="line">    name=&quot;Demo Assistant&quot;,</span><br><span class="line">    instructions=&quot;你是人工智能助手。你可以通过代码回答很多数学问题。&quot;,</span><br><span class="line">    tools=[&#123;&quot;type&quot;: &quot;code_interpreter&quot;&#125;],</span><br><span class="line">    model=&quot;gpt-4o&quot;</span><br><span class="line">)</span><br><span class="line">在回调中加入 code_interpreter 的事件响应</span><br><span class="line">class EventHandler(AssistantEventHandler):</span><br><span class="line">    @override</span><br><span class="line">    def on_text_created(self, text) -&gt; None:</span><br><span class="line">        &quot;&quot;&quot;响应输出创建事件&quot;&quot;&quot;</span><br><span class="line">        print(f&quot;\nassistant &gt; &quot;, end=&quot;&quot;, flush=True)</span><br><span class="line"></span><br><span class="line">    @override</span><br><span class="line">    def on_text_delta(self, delta, snapshot):</span><br><span class="line">        &quot;&quot;&quot;响应输出生成的流片段&quot;&quot;&quot;</span><br><span class="line">        print(delta.value, end=&quot;&quot;, flush=True)</span><br><span class="line"></span><br><span class="line">    @override</span><br><span class="line">    def on_tool_call_created(self, tool_call):</span><br><span class="line">        &quot;&quot;&quot;响应工具调用&quot;&quot;&quot;</span><br><span class="line">        print(f&quot;\nassistant &gt; &#123;tool_call.type&#125;\n&quot;, flush=True)</span><br><span class="line"></span><br><span class="line">    @override</span><br><span class="line">    def on_tool_call_delta(self, delta, snapshot):</span><br><span class="line">        &quot;&quot;&quot;响应工具调用的流片段&quot;&quot;&quot;</span><br><span class="line">        if delta.type == &apos;code_interpreter&apos;:</span><br><span class="line">            if delta.code_interpreter.input:</span><br><span class="line">                print(delta.code_interpreter.input, end=&quot;&quot;, flush=True)</span><br><span class="line">        if delta.code_interpreter.outputs:</span><br><span class="line">            print(f&quot;\n\noutput &gt;&quot;, flush=True)</span><br><span class="line">            for output in delta.code_interpreter.outputs:</span><br><span class="line">                if output.type == &quot;logs&quot;:</span><br><span class="line">                    print(f&quot;\n&#123;output.logs&#125;&quot;, flush=True)</span><br></pre></td></tr></table></figure>
<p>发个 Code Interpreter 请求</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 创建 thread</span><br><span class="line">thread = client.beta.threads.create()</span><br><span class="line"></span><br><span class="line"># 添加新一轮的 user message</span><br><span class="line">message = client.beta.threads.messages.create(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    role=&quot;user&quot;,</span><br><span class="line">    content=&quot;用代码计算 1234567 的平方根&quot;,</span><br><span class="line">)</span><br><span class="line"># 使用 stream 接口并传入 EventHandler</span><br><span class="line">with client.beta.threads.runs.stream(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    assistant_id=assistant_id,</span><br><span class="line">    event_handler=EventHandler(),</span><br><span class="line">) as stream:</span><br><span class="line">    stream.until_done()</span><br></pre></td></tr></table></figure>
<h3 id="Code-Interpreter-操作文件"><a href="#Code-Interpreter-操作文件" class="headerlink" title="Code_Interpreter 操作文件"></a>Code_Interpreter 操作文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 上传文件到 OpenAI</span><br><span class="line">file = client.files.create(</span><br><span class="line">    file=open(&quot;mydata.csv&quot;, &quot;rb&quot;),</span><br><span class="line">    purpose=&apos;assistants&apos;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建 assistant</span><br><span class="line">my_assistant = client.beta.assistants.create(</span><br><span class="line">    name=&quot;CodeInterpreterWithFileDemo&quot;,</span><br><span class="line">    instructions=&quot;你是数据分析师，按要求分析数据。&quot;,</span><br><span class="line">    model=&quot;gpt-4o&quot;,</span><br><span class="line">    tools=[&#123;&quot;type&quot;: &quot;code_interpreter&quot;&#125;],</span><br><span class="line">    tool_resources=&#123;</span><br><span class="line">        &quot;code_interpreter&quot;: &#123;</span><br><span class="line">          &quot;file_ids&quot;: [file.id]  # 为 code_interpreter 关联文件</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>关于文件操作，还有如下函数：</p>
<ol>
<li>client.files.list() 列出所有文件</li>
<li>client.files.retrieve() 获取文件对象</li>
<li>client.files.delete() 删除文件</li>
<li>client.files.content() 读取文件内容</li>
</ol>
<p>具体文档参考：<a href="https://platform.openai.com/docs/api-reference/files" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/files</a></p>
<h3 id="创建-Assistant-时声明-Function-calling"><a href="#创建-Assistant-时声明-Function-calling" class="headerlink" title="创建 Assistant 时声明 Function calling"></a>创建 Assistant 时声明 Function calling</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">assistant = client.beta.assistants.create(</span><br><span class="line">  instructions=&quot;你叫瓜瓜。你是AGI课堂的助手。你只回答跟AI大模型有关的问题。不要跟学生闲聊。每次回答问题前，你要拆解问题并输出一步一步的思考过程。&quot;,</span><br><span class="line">  model=&quot;gpt-4o&quot;,</span><br><span class="line">  tools=[&#123;</span><br><span class="line">    &quot;type&quot;: &quot;function&quot;,</span><br><span class="line">    &quot;function&quot;: &#123;</span><br><span class="line">      &quot;name&quot;: &quot;course_info&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;用于查看具体课程信息，包括时间表，题目，讲师，等等。Function输入必须是一个合法的SQL表达式。&quot;,</span><br><span class="line">      &quot;parameters&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">          &quot;query&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;description&quot;: &quot;SQL query extracting info to answer the user&apos;s question.\nSQL should be written using this database schema:\n\nCREATE TABLE Courses (\n\tid INT AUTO_INCREMENT PRIMARY KEY,\n\tcourse_date DATE NOT NULL,\n\tstart_time TIME NOT NULL,\n\tend_time TIME NOT NULL,\n\tcourse_name VARCHAR(255) NOT NULL,\n\tinstructor VARCHAR(255) NOT NULL\n);\n\nThe query should be returned in plain text, not in JSON.\nThe query should only contain grammars supported by SQLite.&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;required&quot;: [</span><br><span class="line">          &quot;query&quot;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="两个无依赖的-function-会在一次请求中一起被调用"><a href="#两个无依赖的-function-会在一次请求中一起被调用" class="headerlink" title="两个无依赖的 function 会在一次请求中一起被调用"></a>两个无依赖的 function 会在一次请求中一起被调用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 创建 thread</span><br><span class="line">thread = client.beta.threads.create()</span><br><span class="line"></span><br><span class="line"># 添加 user message</span><br><span class="line">message = client.beta.threads.messages.create(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    role=&quot;user&quot;,</span><br><span class="line">    content=&quot;Q1，Q2&quot;, ===&gt; 两个问题对应两个函数，一起被调用</span><br><span class="line">)</span><br><span class="line"># 使用 stream 接口并传入 EventHandler</span><br><span class="line">with client.beta.threads.runs.stream(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    assistant_id=assistant.id,</span><br><span class="line">    event_handler=EventHandler(),</span><br><span class="line">) as stream:</span><br><span class="line">    stream.until_done()</span><br></pre></td></tr></table></figure>
<h2 id="创建-Assistant-时声明file-search"><a href="#创建-Assistant-时声明file-search" class="headerlink" title="创建 Assistant 时声明file_search"></a>创建 Assistant 时声明file_search</h2><p>tool.type 为file_search时相当于<strong>内置的 RAG 功能</strong></p>
<h3 id="创建-Vector-Store，上传文件"><a href="#创建-Vector-Store，上传文件" class="headerlink" title="创建 Vector Store，上传文件"></a>创建 Vector Store，上传文件</h3><ol>
<li>通过代码创建 Vector Store</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">vector_store = client.beta.vector_stores.create(</span><br><span class="line">  name=&quot;MyVectorStore&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">通过代码上传文件到 OpenAI 的存储空间</span><br><span class="line">file = client.files.create(</span><br><span class="line">  file=open(&quot;agiclass_intro.pdf&quot;, &quot;rb&quot;),</span><br><span class="line">  purpose=&quot;assistants&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">通过代码将文件添加到 Vector Store</span><br><span class="line">vector_store_file = client.beta.vector_stores.files.create(</span><br><span class="line">  vector_store_id=vector_store.id,</span><br><span class="line">  file_id=file.id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">批量上传文件到 Vector Store</span><br><span class="line">files = [&apos;file1.pdf&apos;,&apos;file2.pdf&apos;]</span><br><span class="line"></span><br><span class="line">file_batch = client.beta.vector_stores.file_batches.upload_and_poll(</span><br><span class="line">    vector_store_id=vector_store.id,</span><br><span class="line">    files=[open(filename, &quot;rb&quot;) for filename in files]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Vector store 和 vector store file 也有对应的 list,retrieve 和 delete等操作。</p>
<p>具体文档参考：</p>
<ol>
<li>Vector store: <a href="https://platform.openai.com/docs/api-reference/vector-stores" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/vector-stores</a></li>
<li>Vector store file: <a href="https://platform.openai.com/docs/api-reference/vector-stores-files" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/vector-stores-files</a></li>
<li>Vector store file 批量操作: <a href="https://platform.openai.com/docs/api-reference/vector-stores-file-batches" target="_blank" rel="noopener">https://platform.openai.com/docs/api-reference/vector-stores-file-batches</a></li>
</ol>
<h3 id="创建-Assistant-时声明-RAG-能力"><a href="#创建-Assistant-时声明-RAG-能力" class="headerlink" title="创建 Assistant 时声明 RAG 能力"></a>创建 Assistant 时声明 RAG 能力</h3><p>RAG 实际被当作一种 tool</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">assistant = client.beta.assistants.create(</span><br><span class="line">  instructions=&quot;你是个问答机器人，你根据给定的知识回答用户问题。&quot;,</span><br><span class="line">  model=&quot;gpt-4o&quot;,</span><br><span class="line">  tools=[&#123;&quot;type&quot;: &quot;file_search&quot;&#125;],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>指定检索源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">assistant = client.beta.assistants.update(</span><br><span class="line">  assistant_id=assistant.id,</span><br><span class="line">  tool_resources=&#123;&quot;file_search&quot;: &#123;&quot;vector_store_ids&quot;: [vector_store.id]&#125;&#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>RAG 请求</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 创建 thread</span><br><span class="line">thread = client.beta.threads.create()</span><br><span class="line"></span><br><span class="line"># 添加 user message</span><br><span class="line">message = client.beta.threads.messages.create(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    role=&quot;user&quot;,</span><br><span class="line">    content=&quot;AI⼤模型全栈⼯程师适合哪些人&quot;,</span><br><span class="line">)</span><br><span class="line"># 使用 stream 接口并传入 EventHandler</span><br><span class="line">with client.beta.threads.runs.stream(</span><br><span class="line">    thread_id=thread.id,</span><br><span class="line">    assistant_id=assistant_id,</span><br><span class="line">    event_handler=EventHandler(),</span><br><span class="line">) as stream:</span><br><span class="line">    stream.until_done()</span><br></pre></td></tr></table></figure>
<h2 id="多个-Assistants-协作"><a href="#多个-Assistants-协作" class="headerlink" title="多个 Assistants 协作"></a>多个 Assistants 协作</h2><p><strong>划重点：</strong> 使用 assistant 的意义之一，是可以隔离不同角色的 instruction 和 function 能力。</p>
<p>6顶思维帽实验</p>
<h2 id="技术选型参考"><a href="#技术选型参考" class="headerlink" title="技术选型参考"></a>技术选型参考</h2><p><strong>GPTs 现状：</strong></p>
<ol>
<li>界面不可定制，不能集成进自己的产品</li>
<li>只有 ChatGPT Plus/Team/Enterprise 用户才能访问</li>
<li>未来开发者可以根据使用量获得报酬，北美先开始</li>
<li>承诺会推出 Team/Enterprise 版的组织内部专属 GPTs</li>
</ol>
<p><strong>适合使用 Assistants API 的场景：</strong></p>
<ol>
<li>定制界面，或和自己的产品集成</li>
<li>需要传大量文件</li>
<li>服务国外用户，或国内 B 端客户</li>
<li>数据保密性要求不高</li>
<li>不差钱</li>
</ol>
<p><strong>适合使用原生 API 的场景：</strong></p>
<ol>
<li>需要极致调优</li>
<li>追求性价比</li>
<li>服务国外用户，或国内 B 端客户</li>
<li>数据保密性要求不高</li>
</ol>
<p><strong>适合使用国产或开源大模型的场景：</strong></p>
<ol>
<li>服务国内用户</li>
<li>数据保密性要求高</li>
<li>压缩长期成本</li>
<li>需要极致调优</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/RAG.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/RAG.html" itemprop="url">
                  RAG
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-10T19:43:37+08:00">
                2024-07-10
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="LLM-固有的局限性-🔗"><a href="#LLM-固有的局限性-🔗" class="headerlink" title="LLM 固有的局限性 🔗"></a><span style="color: black; font-weight: bold; font-size: 20px;">LLM 固有的局限性</span> <a href="https://colab.research.google.com/github/YooHannah/algorithm/blob/master/llmCourese/05-rag-embeddings/index.ipynb#scrollTo=d5936819-e086-439c-822e-72ba4478c814" target="_blank" rel="noopener">🔗</a></h3><ol>
<li><span style="font-size: 14px;">LLM 的知识不是实时的</span></li>
<li><span style="font-size: 14px;">LLM 可能不知道你私有的领域/业务知识</span></li>
</ol>
<p>RAG 通过给LLM 增加额外/专有知识文档，提高LLM 回答问题的准确性</p>
<p><img src="/image/llmCourse/rag.svg" alt="流程图"></p>
<h3 id="搭建过程："><a href="#搭建过程：" class="headerlink" title="搭建过程："></a><span style="color: black; font-weight: bold; font-size: 20px;">搭建过程：</span></h3><ol>
<li><span style="font-size: 14px;">文档加载，并按一定条件<strong>切割</strong>成片段</span></li>
<li><span style="font-size: 14px;">将切割的文本片段灌入<strong>检索引擎</strong></span></li>
<li><span style="font-size: 14px;">封装<strong>检索接口</strong></span></li>
<li><span style="font-size: 14px;">构建<strong>调用流程</strong>：Query -&gt; 检索 -&gt; Prompt -&gt; LLM -&gt; 回复</span></li>
</ol>
<h3 id="关键字检索的局限性"><a href="#关键字检索的局限性" class="headerlink" title="关键字检索的局限性"></a><span style="color: black; font-weight: bold; font-size: 20px;">关键字检索的局限性</span></h3><p><span style="color: black; background-color: white; font-size: 14px;">同一个语义，用词不同，可能导致检索不到有效的结果</span></p>
<p><span style="color: black; background-color: white; font-size: 14px;">解决办法===&gt; 向量检索</span></p>
<h2 id="向量检索"><a href="#向量检索" class="headerlink" title="向量检索"></a><span style="color: black; font-weight: bold; font-size: 22px;">向量检索</span></h2><p><span style="color: black; background-color: white; font-size: 14px;">二维空间中的向量可以表示为(x,y) 表示从原点(0,0) 到点 (x,y) 的有向线段。</span></p>
<p><span style="color: black; background-color: white; font-size: 14px;">以此类推，我可以用一组坐标 (x0，x1,…..xN) 表示一个𝑁 维空间中的向量，𝑁 叫向量的维度。</span></p>
<h3 id="文本向量（Text-Embeddings）"><a href="#文本向量（Text-Embeddings）" class="headerlink" title="文本向量（Text Embeddings）"></a><span style="color: black; font-weight: bold; font-size: 20px;">文本向量（Text Embeddings）</span></h3><ol>
<li><span style="font-size: 14px;">将文本转成一组 𝑁 维浮点数，即<strong>文本向量</strong>又叫 Embeddings</span></li>
<li><span style="font-size: 14px;">向量之间可以计算距离，距离远近对应<strong>语义相似度</strong>大小</span></li>
</ol>
<h3 id="文本向量是怎么得到的-🔗"><a href="#文本向量是怎么得到的-🔗" class="headerlink" title="文本向量是怎么得到的 🔗"></a><span style="color: black; font-weight: bold; font-size: 20px;">文本向量是怎么得到的</span> <a href="https://colab.research.google.com/github/YooHannah/algorithm/blob/master/llmCourese/05-rag-embeddings/index.ipynb#scrollTo=de84fb43-b02f-4089-9eda-d02f90359915" target="_blank" rel="noopener">🔗</a></h3><ol>
<li><span style="font-size: 14px;">构建相关（正立）与不相关（负例）的句子对儿样本</span></li>
<li><span style="font-size: 14px;">训练双塔式模型，让正例间的距离小，负例间的距离大</span></li>
</ol>
<h3 id="向量间的相似度计算"><a href="#向量间的相似度计算" class="headerlink" title="向量间的相似度计算"></a><span style="color: black; font-weight: bold; font-size: 20px;">向量间的相似度计算</span></h3><p><span style="color: black; background-color: white; font-size: 14px;">余弦距离 – 越大越相似</span></p>
<p><span style="color: black; background-color: white; font-size: 14px;">欧氏距离 – 越小越相似</span></p>
<p><span style="color: black; background-color: white; font-size: 14px;">向量数据库，是专门为向量检索设计的中间件</span></p>
<p><span style="color: #1b5e20; font-weight: bold; font-size: 14px;">澄清几个关键概念：</span></p>
<ol>
<li><span style="font-size: 14px;">向量数据库的意义是快速的检索；</span></li>
<li><span style="font-size: 14px;">向量数据库本身不生成向量，向量是由 Embedding 模型产生的；</span></li>
<li><span style="font-size: 14px;">向量数据库与传统的关系型数据库是互补的，不是替代关系，在实际应用中根据实际需求经常同时使用。</span></li>
</ol>
<p><span style="color: #1b5e20; font-weight: bold; font-size: 14px;">划重点：</span></p>
<ol>
<li><span style="font-size: 14px;">不是每个 Embedding 模型都对余弦距离和欧氏距离同时有效</span></li>
<li><span style="font-size: 14px;">哪种相似度计算有效要阅读模型的说明（通常都支持余弦距离计算）</span></li>
</ol>
<h3 id="优化方向"><a href="#优化方向" class="headerlink" title="优化方向"></a><span style="color: black; font-weight: bold; font-size: 20px;">优化方向</span></h3><h3 id="文本分割的粒度-🔗"><a href="#文本分割的粒度-🔗" class="headerlink" title="文本分割的粒度 🔗"></a><span style="color: black; font-weight: bold; font-size: 20px;">文本分割的粒度</span> <a href="https://colab.research.google.com/github/YooHannah/algorithm/blob/master/llmCourese/05-rag-embeddings/index.ipynb#scrollTo=99b07741-a60e-4927-b2f1-f037d6fce69c" target="_blank" rel="noopener">🔗</a></h3><p><strong>缺陷</strong></p>
<ol>
<li><span style="font-size: 14px;">粒度太大可能导致检索不精准，粒度太小可能导致信息不全面</span></li>
<li><span style="font-size: 14px;">问题的答案可能跨越两个片段</span></li>
</ol>
<p><span style="color: black; font-weight: bold; background-color: white; font-size: 14px;">改进</span><span style="color: black; background-color: white; font-size: 14px;">: 按一定粒度，部分重叠式的切割文本，使上下文更完整</span></p>
<h3 id="检索后排序-🔗"><a href="#检索后排序-🔗" class="headerlink" title="检索后排序 🔗"></a><span style="color: black; font-weight: bold; font-size: 20px;">检索后排序</span> <a href="https://colab.research.google.com/github/YooHannah/algorithm/blob/master/llmCourese/05-rag-embeddings/index.ipynb#scrollTo=a5b4d28b-392f-4bec-8134-1f52659048be" target="_blank" rel="noopener">🔗</a></h3><p><strong>问题</strong>: 有时，最合适的答案不一定排在检索的最前面</p>
<p><strong>方案</strong>:</p>
<ol>
<li><span style="font-size: 14px;">检索时过招回一部分文本</span></li>
<li><span style="font-size: 14px;">通过一个排序模型对 query 和 document 重新打分排序</span></li>
</ol>
<h3 id="混合检索（Hybrid-Search）"><a href="#混合检索（Hybrid-Search）" class="headerlink" title="混合检索（Hybrid Search）"></a><span style="color: black; font-weight: bold; font-size: 20px;">混合检索（Hybrid Search）</span></h3><p>在实际生产中，传统的关键字检索（稀疏表示）与向量检索（稠密表示）各有优劣。</p>
<p>举个具体例子，比如文档中包含很长的专有名词，关键字检索往往更精准而向量检索容易引入概念混淆。</p>
<p>有时候我们需要结合不同的检索算法，来达到比单一检索算法更优的效果。这就是<strong>混合检索</strong>。</p>
<p>混合检索的核心是，综合文档 𝑑 在不同检索算法下的排序名次（rank），为其生成最终排序。</p>
<p><span style="color: black; background-color: white; font-size: 14px;">一个最常用的算法叫 </span><span style="color: black; font-weight: bold; background-color: white; font-size: 14px;">Reciprocal Rank Fusion（RRF）</span></p>
<h3 id="RAG-Fusion"><a href="#RAG-Fusion" class="headerlink" title="RAG-Fusion"></a><span style="color: black; font-weight: bold; font-size: 20px;">RAG-Fusion</span></h3><p><span style="color: black; background-color: white; font-size: 14px;">RAG-Fusion 就是利用了 RRF 的原理来提升检索的准确性。</span></p>
<p><a href="https://github.com/Raudaschl/rag-fusion" target="_blank" rel="noopener">https://github.com/Raudaschl/rag-fusion</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/aiCoding.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/aiCoding.html" itemprop="url">
                  从AI编程认知AI
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-09T20:51:37+08:00">
                2024-07-09
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一些产品设计的思想"><a href="#一些产品设计的思想" class="headerlink" title="一些产品设计的思想"></a>一些产品设计的思想</h2><p><span style="color:#1b5e20;font-weight:bold">划重点：</span></p>
<ol>
<li>凡是重复脑力劳动都可以考虑 AI 化</li>
<li>凡是「输入和输出都是文本」的场景，都值得尝试用大模型提效</li>
</ol>
<h2 id="如何理解-AI-能编写程序"><a href="#如何理解-AI-能编写程序" class="headerlink" title="如何理解 AI 能编写程序"></a>如何理解 AI 能编写程序</h2><p><a href="https://github.com/YooHannah/algorithm/blob/master/llmCourese/04-ai-programming/index.ipynb" target="_blank" rel="noopener">如何理解 AI 能编写程序</a></p>
<p><strong>编程能力是大模型各项能力的天花板</strong></p>
<ul>
<li>「编程」是目前大模型能力最强的垂直领域，甚至超越了对「自然语言」本身的处理能力。因为：<ul>
<li>训练数据质量高</li>
<li>结果可衡量</li>
<li>编程语言无二义性</li>
<li>有<a href="https://arxiv.org/pdf/2211.09110.pdf" target="_blank" rel="noopener">论文</a><ul>
<li>“The first model that OpenAI gave us was a Python-only model,” Ziegler remembers. “Next we were delivered a JavaScript model and a multilingual model, and it turned out that the Javascript model had particular problems that the multilingual model did not. It actually came as a surprise to us that the multilingual model could perform so well. But each time, the models were just getting better and better, which was really exciting for GitHub Copilot’s progress.” –<a href="https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/" target="_blank" rel="noopener">Inside GitHub: Working with the LLMs behind GitHub Copilot</a></li>
</ul>
</li>
</ul>
</li>
<li>知道怎么用好 AI 编程，了解它的能力边界、使用场景，就能类比出在其他领域 AI 怎么落地，能力上限在哪<ul>
<li><a href="https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/" target="_blank" rel="noopener">How to build an enterprise LLM application: Lessons from GitHub Copilot</a></li>
</ul>
</li>
</ul>
<p><span style="color:#1b5e20;font-weight:bold">划重点：</span></p>
<ol>
<li>使用 AI 编程，除了解决编程问题以外，更重要是形成对 AI 的正确认知。</li>
<li>数据质量决定 AI 的质量。</li>
</ol>
<h3 id="一些技巧"><a href="#一些技巧" class="headerlink" title="一些技巧"></a>一些技巧</h3><ol>
<li>代码有了，再写注释，更省力</li>
<li>改写当前代码，可另起一块新写，AI 补全得更准，完成后再删旧代码</li>
<li>Cmd/Ctrl + → 只接受一个 token</li>
<li>如果有旧代码希望被参考，就把代码文件在新 tab 页里打开</li>
</ol>
<p><span style="color:#1b5e20;font-weight:bold">产品设计经验：</span><span style="color:#1b5e20;background-color:#c8e6c9">在 chat 界面里用 @ 串联多个 agent 是一个常见的 AI 产品设计范式。</span></p>
<p><span style="color:#1b5e20;font-weight:bold">产品设计经验：</span><span style="color:#1b5e20;background-color:#c8e6c9">让 AI 在不影响用户原有工作习惯的情况下切入使用场景，接受度最高。</span></p>
<p><span style="color:#1b5e20;font-weight:bold">产品设计经验：流程化操作</span><span style="color:#1b5e20;background-color:#c8e6c9">步步都需要人工调整、确认</span></p>
<p><span style="color:#1b5e20;font-weight:bold">落地经验：</span><span style="color:#1b5e20;background-color:#c8e6c9">只有可量化的结果，才能说服老板买单</span></p>
<h2 id="GitHub-Copilot-基本原理"><a href="#GitHub-Copilot-基本原理" class="headerlink" title="GitHub Copilot 基本原理"></a>GitHub Copilot 基本原理</h2><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><ul>
<li>模型层：最初使用 OpenAI Codex 模型，它也是 GPT-3.5、GPT-4 的「一部分」。<br><a href="https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/" target="_blank" rel="noopener">现在已经完全升级，模型细节未知</a>。</li>
<li>应用层： prompt engineering。Prompt 中包含：<br>a. 组织上下文：光标前和光标后的代码片段<br>b. 获取代码片段：其它相关代码片段。当前文件和其它打开的同语言文件 tab 里的代码被切成每个 60 行的片段，用<a href="https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8D%A1%E5%B0%94%E6%8C%87%E6%95%B0" target="_blank" rel="noopener">Jaccard 相似度</a><ul>
<li>为什么是打开的 tabs？</li>
<li>多少个 tabs 是有效的呢？经验选择：20 个<br>c. 修饰相关上下文：被取用的代码片段的路径。<br>d. 优先级：根据一些代码常识判断补全输入内容的优先级<br>e. 补全格式：在函数定义、类定义、if-else 等之后，会补全整段代码，其它时候只补全当前行</li>
</ul>
</li>
</ul>
<p>有效性：</p>
<ul>
<li>Telemetry(远程遥测<a href="https://docs.github.com/en/site-policy/privacy-policies/github-general-privacy-statement" target="_blank" rel="noopener">如何取消</a>)</li>
<li>A/B Test</li>
<li>智谱的度量方式</li>
</ul>
<h3 id="AI-能力定律："><a href="#AI-能力定律：" class="headerlink" title="AI 能力定律："></a>AI 能力定律：</h3><p>AI 能力的上限，是使用者的判断力</p>
<p><span style="font-size:16px">AI 能力=min(AI 能力,使用者判断力)</span></p>
<h3 id="AI-提效定律："><a href="#AI-提效定律：" class="headerlink" title="AI 提效定律："></a>AI 提效定律：</h3><p>AI 提升的效率，与使用者的判断力成正比，与生产力成反比</p>
<p><span style="font-size:16px">效率提升幅度 = 使用者判断力/使用者生产力</span></p>
<p>解读：</p>
<ol>
<li>使用者的判断力，是最重要的</li>
<li>提升判断力，比提升实操能力更重要。所谓「眼高手低」者的福音</li>
<li>广阔的视野是判断力的养料</li>
</ol>
<p><span style="color:#1b5e20;font-weight:bold">要点总结：</span></p>
<ol>
<li>通过天天使用，总结使用大模型的规律，认知：<strong>凡是「输入和输出都是文本」的场景，都值得尝试用大模型提效。</strong></li>
<li>通过体验 GitHub Copilot，认知：<strong>AI 产品的打磨过程、落地和目前盈利产品如何打造</strong></li>
<li>通过介绍原理，认知：<strong>AI 目前的上限，以及 AI 组织数据和达到上限的条件</strong></li>
<li>对于 AI 产品如何反馈有效性，认知：<strong>AI 产品落地的有效性管理方法</strong></li>
<li>通过介绍两大定律，认知：<strong>AI 幻觉不可消灭； AI 的能效；</strong></li>
</ol>
<p><span style="color:#1b5e20;font-weight:bold">以成功案例为例，理解基本原理，避免拍脑袋</span></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoohannah.github.io/post/llm/functionCalling.html">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YooHannah">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/psb.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="My Little World">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="My Little World" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/post/llm/functionCalling.html" itemprop="url">
                  Function Calling
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2024-07-09T17:35:37+08:00">
                2024-07-09
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="为什么要大模型连接外部世界？"><a href="#为什么要大模型连接外部世界？" class="headerlink" title="为什么要大模型连接外部世界？"></a>为什么要大模型连接外部世界？</h2><h3 id="大模型两大缺陷："><a href="#大模型两大缺陷：" class="headerlink" title="大模型两大缺陷："></a>大模型两大缺陷：</h3><ol>
<li><strong>并非知晓一切</strong><br>a. 训练数据不可能什么都有。垂直、非公开数据必有欠缺<br>b. 不知道最新信息。大模型的训练周期很长，且更新一次耗资巨大，还有越训越傻的风险。所以 ta 不可能实时训练。<br> ⅰ. GPT-3.5 知识截至 2021 年 9 月<br> ⅱ. GPT-4-turbo 知识截至 2023 年 12 月<br> ⅲ. GPT-4o 知识截至 2023 年 10 月  </li>
<li><strong>没有「真逻辑」</strong>。它表现出的逻辑、推理，是训练文本的统计规律，而不是真正的逻辑，所以有幻觉。</li>
</ol>
<p>所以：大模型需要连接真实世界，并对接真逻辑系统执行确定性任务。</p>
<h2 id="ChatGPT-用-Actions-连接外部世界"><a href="#ChatGPT-用-Actions-连接外部世界" class="headerlink" title="ChatGPT 用 Actions 连接外部世界"></a>ChatGPT 用 Actions 连接外部世界</h2><p><span style="color: #1b5e20; font-weight: bold;">划重点：</span></p>
<ol>
<li>通过 Actions 的 schema，GPT 能读懂各个 API 能做什么、怎么调用（相当于人读 API 文档）</li>
<li>拿到 prompt，GPT 分析出是否要调用 API 才能解决问题（相当于人读需求）</li>
<li>如果要调用 API，生成调用参数（相当于人编写调用代码）</li>
<li>ChatGPT（注意，不是 GPT）调用 API（相当于人运行程序）</li>
<li>API 返回结果，GPT 读懂结果，整合到回答中（相当于人整理结果，输出结论）</li>
</ol>
<p>把 AI 当人看！</p>
<h2 id="Function-Calling-的机制"><a href="#Function-Calling-的机制" class="headerlink" title="Function Calling 的机制"></a>Function Calling 的机制</h2><p>原理和 Actions 一样，只是使用方式有区别。</p>
<p>Function Calling 完整的官方接口文档：<br><a href="https://platform.openai.com/docs/guides/function-calling" target="_blank" rel="noopener">https://platform.openai.com/docs/guides/function-calling</a></p>
<p><img src="/image/llmCourse/fc.svg" alt="Function Calling 机制图示"></p>
<p><span style="color: #1b5e20; font-weight: bold;">划重点：</span></p>
<ol>
<li>Function Calling 中的函数与参数的描述也是一种 Prompt</li>
<li>这种 Prompt 也需要调优，否则会影响函数的召回、参数的准确性，甚至让 GPT 产生幻觉</li>
<li>函数声明是消耗 token 的。要在功能覆盖、省钱、节约上下文窗口之间找到最佳平衡</li>
<li>Function Calling 不仅可以调用读函数，也能调用写函数。但<a href="https://platform.openai.com/docs/guides/function-calling/introduction" target="_blank" rel="noopener">官方强烈建议，在写之前，一定要有真人做确认</a></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_completion</span><span class="params">(messages, model=<span class="string">"gpt-3.5-turbo"</span>)</span>:</span></span><br><span class="line">    response = client.chat.completions.create(</span><br><span class="line">        model=model,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">0.7</span>,</span><br><span class="line">        tools=[&#123;  <span class="comment"># 用 JSON 描述函数。可以定义多个。由大模型决定调用谁。也可能都不调用</span></span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"function"</span>,</span><br><span class="line">            <span class="string">"function"</span>: &#123;</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"sum"</span>,</span><br><span class="line">                <span class="string">"description"</span>: <span class="string">"加法器，计算一组数的和"</span>,</span><br><span class="line">                <span class="string">"parameters"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">                    <span class="string">"properties"</span>: &#123;</span><br><span class="line">                        <span class="string">"numbers"</span>: &#123;</span><br><span class="line">                            <span class="string">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">                            <span class="string">"items"</span>: &#123;</span><br><span class="line">                                <span class="string">"type"</span>: <span class="string">"number"</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;],</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response.choices[<span class="number">0</span>].message</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">"Tell me the sum of 1, 2, 3, 4, 5, 6, 7, 8, 9, 10."</span></span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">"role"</span>: <span class="string">"system"</span>, <span class="string">"content"</span>: <span class="string">"你是一个数学家"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: prompt&#125;</span><br><span class="line">]</span><br><span class="line">response = get_completion(messages)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把大模型的回复加入到对话历史中。必须有</span></span><br><span class="line">messages.append(response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果返回的是函数调用结果，则打印出来</span></span><br><span class="line"><span class="keyword">if</span> (response.tool_calls <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">    <span class="comment"># 是否要调用 sum</span></span><br><span class="line">    tool_call = response.tool_calls[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> (tool_call.function.name == <span class="string">"sum"</span>):</span><br><span class="line">        <span class="comment"># 调用 sum</span></span><br><span class="line">        args = json.loads(tool_call.function.arguments)</span><br><span class="line">        result = sum(args[<span class="string">"numbers"</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把函数调用结果加入到对话历史中</span></span><br><span class="line">        messages.append(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"tool_call_id"</span>: tool_call.id,  <span class="comment"># 用于标识函数调用的 ID</span></span><br><span class="line">                <span class="string">"role"</span>: <span class="string">"tool"</span>,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"sum"</span>,</span><br><span class="line">                <span class="string">"content"</span>: str(result)  <span class="comment"># 数值 result 必须转成字符串</span></span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 再次调用大模型</span></span><br><span class="line">        print(<span class="string">"=====最终 GPT 回复====="</span>)</span><br><span class="line">        print(get_completion(messages).content)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"=====对话历史====="</span>)</span><br><span class="line">print_json(messages)</span><br></pre></td></tr></table></figure>
<p>更多练习<br>本地单函数调用<br>本地多Function 调用(根据name 识别调用不同函数)<br>通过 Function Calling 查询单数据库<br>用 Function Calling 实现多表查询(把多表的描述给进去就好了)<br>Stream 模式（流式（stream）输出不会一次返回完整 JSON 结构，所以需要拼接后再使用，拿到什么就输出什么可减少用户等待时间，调用时client.chat.completions.create设置stream=True即可</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/psb.jpg" alt="YooHannah">
          <p class="site-author-name" itemprop="name">YooHannah</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">264</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YooHannah</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/treedocument/treedocument.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  

  

  

  


</body>
</html>
